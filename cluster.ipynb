{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cluster",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpH+YSEh9rvyj/tgxY0cCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JetteKA/BigData/blob/master/cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZbioMpUQWdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "40e79226-878a-479c-dca4-90522d9c75af"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOtyVBro7w78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 10\n",
        "num_centroids = 5\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 50\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3diY8rIQeK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c550124f-6e3b-455e-a0af-0407eb67cdc0"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "foNhuQk6e91n",
        "colab": {}
      },
      "source": [
        "x_cluster = x_train.reshape((x_train.shape[0], -1))\n",
        "x_cluster = np.divide(x_cluster, 255.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdKQmlLUQ-FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=n_classes, init='k-means++')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZysWVqtSt0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_distance = kmeans.fit_transform(x_cluster)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHwPSglD12wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "closest_to_centroid = cluster_distance.argsort(axis=0)[:num_centroids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq-F8-1S2r7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctc = closest_to_centroid.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikNEXFIAwCa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_label = {0:\"T-shirt/top\",\n",
        "              1:\"Trouser\",\n",
        "              2:\"Pullover\",\n",
        "              3:\"Dress\",\n",
        "              4:\"Coat\",\n",
        "              5:\"Sandal\",\n",
        "              6:\"Shirt\",\n",
        "              7:\"Sneaker\",\n",
        "              8:\"Bag\",\n",
        "              9:\"Ankle boot\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saZ3YHzmvSeZ",
        "colab_type": "code",
        "outputId": "e8dbccc2-efc3-4669-c818-904a5ae10bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(figsize = (15,25))\n",
        "bottom = 0.35\n",
        "\n",
        "x_labelled = []\n",
        "y_labelled = np.empty(int(n_classes*num_centroids))\n",
        "for i, index in enumerate(ctc.flatten()):\n",
        "\n",
        " x_labelled.append(x_train[index])\n",
        " y_labelled[i] = y_train[index]\n",
        "\n",
        " plt.subplots_adjust(bottom)\n",
        " \n",
        " plt.subplot(n_classes,num_centroids,i+1, xticks=[], yticks=[])\n",
        " plt.title('Label: {} {}'.format(y_train[index], word_label[y_train[index]]),fontsize = 12)\n",
        " plt.imshow(x_train[index], cmap='gray')\n",
        " plt.savefig(\"5centroids_cluster.png\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABMEAAAUwCAYAAABKbUIFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHoIBAAAAAACgeP8AaM1A8Cy3bcYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 2160x1728 with 100 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdJL7RBoEE7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_labelled = np.array(x_labelled)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dtd5LnyFaqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_unlabelled = np.delete(x_train, ctc.flatten(), axis=0)\n",
        "y_unlabelled = np.delete(y_train, ctc.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb-iKM4ONDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_x_for_cnn(data, img_rows=img_rows, img_cols=img_cols):\n",
        "\n",
        "\n",
        "  data = data.reshape(data.shape[0], img_rows, img_cols, 1)\n",
        "  data = data.astype('float32')\n",
        "  data /=255\n",
        "\n",
        "  return data\n",
        "\n",
        "def reshape_y_for_cnn(data, n_classes=n_classes):\n",
        "  return keras.utils.to_categorical(data, n_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIWV95D3qRdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = reshape_x_for_cnn(x_test)\n",
        "\n",
        "y_test = reshape_y_for_cnn(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCq94kh1oltR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4R4DEu_oyi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "num_labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvrIrIjN4zCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pseudo_label = []\n",
        "actual_label = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC77p5ynASl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_label(model, x_labelled, y_labelled, x_unlabelled, y_unlabelled):\n",
        "\n",
        "\n",
        "  x_label_cnn = reshape_x_for_cnn(x_labelled)\n",
        "\n",
        "\n",
        "  y_label_cnn = reshape_y_for_cnn(y_labelled)\n",
        "\n",
        "  model.fit(x_label_cnn, y_label_cnn,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "  test_score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Test loss:', test_score[0])\n",
        "  print('Test accuracy:', test_score[1])\n",
        "  test_loss.append(test_score[0])\n",
        "  test_acc.append(test_score[1])\n",
        "\n",
        "  train_score = model.evaluate(x_label_cnn, y_label_cnn, verbose=0)\n",
        "  print('Train loss:', train_score[0])\n",
        "  print('Train accuracy:', train_score[1])\n",
        "  train_loss.append(train_score[0])\n",
        "  train_acc.append(train_score[1])\n",
        "\n",
        "  # print(x_labelled)\n",
        "  num_labels.append(x_labelled.shape[0])\n",
        "\n",
        "  x_unlabel_cnn = reshape_x_for_cnn(x_unlabelled)\n",
        "\n",
        "  predictions = model.predict(x_unlabel_cnn)\n",
        "\n",
        "  certainty = np.amax(predictions, axis=1)\n",
        "\n",
        "  most_certain = certainty.argsort()[-25:]\n",
        "\n",
        "  x_pseudo_label = []\n",
        "  y_pseudo_label = []\n",
        "\n",
        "  for i, index in enumerate(most_certain):\n",
        "\n",
        "    x_pseudo_label.append(x_unlabelled[index])\n",
        "    y_pseudo_label.append(np.argmax(predictions[index]))\n",
        "    pseudo_label.append(np.argmax(predictions[index]))\n",
        "    actual_label.append(y_unlabelled[index])\n",
        "\n",
        "  most_uncertain = certainty.argsort()[:25]\n",
        "\n",
        "  x_label_new = []\n",
        "  y_label_new = []\n",
        "  for i, index in enumerate(most_uncertain):\n",
        "\n",
        "    x_label_new.append(x_unlabelled[index])\n",
        "    y_label_new.append(y_unlabelled[index])\n",
        "\n",
        "  y_labelled = np.array(y_labelled.tolist() + y_pseudo_label + y_label_new)\n",
        "  x_labelled = np.array(x_labelled.tolist() + x_pseudo_label + x_label_new)\n",
        "\n",
        "  x_unlabelled = np.delete(x_unlabelled, np.concatenate([most_certain, most_uncertain]), axis=0)\n",
        "  y_unlabelled = np.delete(y_unlabelled, np.concatenate([most_certain, most_uncertain]))\n",
        "\n",
        "  return model, x_labelled, y_labelled, x_unlabelled, y_unlabelled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0u3tT5vFb5z",
        "colab_type": "code",
        "outputId": "6422274e-ad41-4d41-bad0-9ec848c626d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "  print('x label:', x_labelled.shape)\n",
        "  print('y label:', y_labelled.shape)  \n",
        "  print('x unlabel:', x_unlabelled.shape)\n",
        "  print('y unlabel:', y_unlabelled.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x label: (100, 28, 28)\n",
            "y label: (100,)\n",
            "x unlabel: (59900, 28, 28)\n",
            "y unlabel: (59900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FpvfJtDugCB",
        "colab_type": "code",
        "outputId": "0898a718-46c4-4807-d3bf-5ef6ea6bd8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(400):\n",
        "  # print(i)\n",
        "  model, x_labelled, y_labelled, x_unlabelled, y_unlabelled = train_and_label(model, x_labelled, y_labelled, x_unlabelled, y_unlabelled)\n",
        "  # print('x label:', x_labelled.shape)\n",
        "  # print('y label:', y_labelled.shape)  \n",
        "  # print('x unlabel:', x_unlabelled.shape)\n",
        "  # print('y unlabel:', y_unlabelled.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train on 100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 2.1676 - acc: 0.1900 - val_loss: 2.2790 - val_acc: 0.1834\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.9132 - acc: 0.3500 - val_loss: 2.2599 - val_acc: 0.3135\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.3951 - acc: 0.5800 - val_loss: 2.2215 - val_acc: 0.3475\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.5058 - acc: 0.4500 - val_loss: 1.8279 - val_acc: 0.4989\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.1138 - acc: 0.7700 - val_loss: 2.1252 - val_acc: 0.4163\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.0258 - acc: 0.7300 - val_loss: 1.9740 - val_acc: 0.4813\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.8999 - acc: 0.7600 - val_loss: 2.2368 - val_acc: 0.4354\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.0064 - acc: 0.7300 - val_loss: 2.0857 - val_acc: 0.5074\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.8544 - acc: 0.7400 - val_loss: 1.6932 - val_acc: 0.5203\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.6577 - acc: 0.8400 - val_loss: 2.1765 - val_acc: 0.5362\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.8454 - acc: 0.7500 - val_loss: 1.9484 - val_acc: 0.4904\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.7510 - acc: 0.7900 - val_loss: 2.1268 - val_acc: 0.5377\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5223 - acc: 0.8400 - val_loss: 2.2235 - val_acc: 0.5375\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.4969 - acc: 0.8300 - val_loss: 2.5619 - val_acc: 0.5186\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.4768 - acc: 0.8500 - val_loss: 2.1267 - val_acc: 0.5301\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5574 - acc: 0.8200 - val_loss: 2.1293 - val_acc: 0.5531\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.4310 - acc: 0.8500 - val_loss: 2.2723 - val_acc: 0.5447\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.3846 - acc: 0.8500 - val_loss: 2.6149 - val_acc: 0.5370\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.4249 - acc: 0.8600 - val_loss: 2.3931 - val_acc: 0.5658\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.3479 - acc: 0.8600 - val_loss: 2.6342 - val_acc: 0.5387\n",
            "Test loss: 2.634182205581665\n",
            "Test accuracy: 0.5387\n",
            "Train loss: 0.2287938778102398\n",
            "Train accuracy: 0.88\n",
            "x label: (200, 28, 28)\n",
            "y label: (200,)\n",
            "x unlabel: (59800, 28, 28)\n",
            "y unlabel: (59800,)\n",
            "1\n",
            "Train on 200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.7679 - acc: 0.7950 - val_loss: 1.9455 - val_acc: 0.5181\n",
            "Epoch 2/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.6587 - acc: 0.7850 - val_loss: 2.0361 - val_acc: 0.5457\n",
            "Epoch 3/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4839 - acc: 0.8300 - val_loss: 2.2538 - val_acc: 0.5492\n",
            "Epoch 4/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4946 - acc: 0.8500 - val_loss: 2.0977 - val_acc: 0.5567\n",
            "Epoch 5/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4621 - acc: 0.8400 - val_loss: 1.9667 - val_acc: 0.5480\n",
            "Epoch 6/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4130 - acc: 0.8650 - val_loss: 2.1705 - val_acc: 0.5179\n",
            "Epoch 7/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4577 - acc: 0.8600 - val_loss: 1.9715 - val_acc: 0.5473\n",
            "Epoch 8/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.3361 - acc: 0.8950 - val_loss: 2.0634 - val_acc: 0.5570\n",
            "Epoch 9/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.3048 - acc: 0.9050 - val_loss: 2.4807 - val_acc: 0.5462\n",
            "Epoch 10/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.2576 - acc: 0.9050 - val_loss: 2.2555 - val_acc: 0.5491\n",
            "Epoch 11/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.2080 - acc: 0.9050 - val_loss: 2.2214 - val_acc: 0.5666\n",
            "Epoch 12/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.2902 - acc: 0.9100 - val_loss: 2.2889 - val_acc: 0.5738\n",
            "Epoch 13/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1767 - acc: 0.9550 - val_loss: 2.2895 - val_acc: 0.5668\n",
            "Epoch 14/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1684 - acc: 0.9550 - val_loss: 2.6993 - val_acc: 0.5494\n",
            "Epoch 15/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.2047 - acc: 0.9250 - val_loss: 2.3686 - val_acc: 0.5489\n",
            "Epoch 16/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1199 - acc: 0.9750 - val_loss: 2.5182 - val_acc: 0.5709\n",
            "Epoch 17/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1466 - acc: 0.9500 - val_loss: 2.3878 - val_acc: 0.5689\n",
            "Epoch 18/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0973 - acc: 0.9850 - val_loss: 2.5628 - val_acc: 0.5692\n",
            "Epoch 19/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1187 - acc: 0.9600 - val_loss: 2.6497 - val_acc: 0.5560\n",
            "Epoch 20/20\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1272 - acc: 0.9550 - val_loss: 3.0604 - val_acc: 0.5479\n",
            "Test loss: 3.0603819622039796\n",
            "Test accuracy: 0.5479\n",
            "Train loss: 0.1361816708224069\n",
            "Train accuracy: 0.965\n",
            "x label: (300, 28, 28)\n",
            "y label: (300,)\n",
            "x unlabel: (59700, 28, 28)\n",
            "y unlabel: (59700,)\n",
            "2\n",
            "Train on 300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.4903 - acc: 0.8533 - val_loss: 1.7019 - val_acc: 0.5797\n",
            "Epoch 2/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.3176 - acc: 0.8933 - val_loss: 1.5757 - val_acc: 0.6141\n",
            "Epoch 3/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.2209 - acc: 0.9100 - val_loss: 1.3271 - val_acc: 0.6448\n",
            "Epoch 4/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.1556 - acc: 0.9600 - val_loss: 1.5830 - val_acc: 0.6484\n",
            "Epoch 5/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.1896 - acc: 0.9433 - val_loss: 1.4660 - val_acc: 0.6567\n",
            "Epoch 6/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.1469 - acc: 0.9467 - val_loss: 1.5031 - val_acc: 0.6715\n",
            "Epoch 7/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0994 - acc: 0.9700 - val_loss: 1.6413 - val_acc: 0.6545\n",
            "Epoch 8/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.1100 - acc: 0.9533 - val_loss: 1.6890 - val_acc: 0.6365\n",
            "Epoch 9/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0944 - acc: 0.9633 - val_loss: 1.5286 - val_acc: 0.6657\n",
            "Epoch 10/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.1060 - acc: 0.9700 - val_loss: 1.9470 - val_acc: 0.6256\n",
            "Epoch 11/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0768 - acc: 0.9833 - val_loss: 1.5827 - val_acc: 0.6783\n",
            "Epoch 12/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0935 - acc: 0.9633 - val_loss: 1.6405 - val_acc: 0.6676\n",
            "Epoch 13/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0808 - acc: 0.9767 - val_loss: 1.6243 - val_acc: 0.6695\n",
            "Epoch 14/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0574 - acc: 0.9867 - val_loss: 1.6544 - val_acc: 0.6592\n",
            "Epoch 15/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0444 - acc: 0.9933 - val_loss: 1.9387 - val_acc: 0.6550\n",
            "Epoch 16/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0360 - acc: 0.9933 - val_loss: 1.9151 - val_acc: 0.6658\n",
            "Epoch 17/20\n",
            "300/300 [==============================] - 0s 2ms/step - loss: 0.0510 - acc: 0.9800 - val_loss: 2.0305 - val_acc: 0.6477\n",
            "Epoch 18/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.9767 - val_loss: 1.7899 - val_acc: 0.6696\n",
            "Epoch 19/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0372 - acc: 0.9900 - val_loss: 1.9035 - val_acc: 0.6608\n",
            "Epoch 20/20\n",
            "300/300 [==============================] - 0s 1ms/step - loss: 0.0346 - acc: 0.9933 - val_loss: 1.8743 - val_acc: 0.6687\n",
            "Test loss: 1.874326233482361\n",
            "Test accuracy: 0.6687\n",
            "Train loss: 0.010995498634883916\n",
            "Train accuracy: 0.9966666666666667\n",
            "x label: (400, 28, 28)\n",
            "y label: (400,)\n",
            "x unlabel: (59600, 28, 28)\n",
            "y unlabel: (59600,)\n",
            "3\n",
            "Train on 400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.4540 - acc: 0.8850 - val_loss: 1.2991 - val_acc: 0.6496\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.3121 - acc: 0.9175 - val_loss: 1.3044 - val_acc: 0.6706\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.2654 - acc: 0.9225 - val_loss: 1.3257 - val_acc: 0.6990\n",
            "Epoch 4/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.2143 - acc: 0.9450 - val_loss: 1.3969 - val_acc: 0.6846\n",
            "Epoch 5/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.1661 - acc: 0.9425 - val_loss: 1.4471 - val_acc: 0.6980\n",
            "Epoch 6/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.1350 - acc: 0.9525 - val_loss: 1.5024 - val_acc: 0.7017\n",
            "Epoch 7/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0859 - acc: 0.9700 - val_loss: 1.5886 - val_acc: 0.7037\n",
            "Epoch 8/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.1144 - acc: 0.9600 - val_loss: 1.6313 - val_acc: 0.6980\n",
            "Epoch 9/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.1054 - acc: 0.9650 - val_loss: 1.7102 - val_acc: 0.6896\n",
            "Epoch 10/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0883 - acc: 0.9675 - val_loss: 1.5926 - val_acc: 0.7009\n",
            "Epoch 11/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0572 - acc: 0.9825 - val_loss: 1.7439 - val_acc: 0.6924\n",
            "Epoch 12/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0694 - acc: 0.9775 - val_loss: 1.6993 - val_acc: 0.7117\n",
            "Epoch 13/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0639 - acc: 0.9825 - val_loss: 1.6897 - val_acc: 0.7054\n",
            "Epoch 14/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0498 - acc: 0.9825 - val_loss: 1.9602 - val_acc: 0.6805\n",
            "Epoch 15/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0596 - acc: 0.9800 - val_loss: 1.7434 - val_acc: 0.7032\n",
            "Epoch 16/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0431 - acc: 0.9850 - val_loss: 2.0032 - val_acc: 0.6836\n",
            "Epoch 17/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0511 - acc: 0.9800 - val_loss: 1.8321 - val_acc: 0.7066\n",
            "Epoch 18/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0557 - acc: 0.9800 - val_loss: 1.7477 - val_acc: 0.7051\n",
            "Epoch 19/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0304 - acc: 0.9900 - val_loss: 1.8243 - val_acc: 0.7021\n",
            "Epoch 20/20\n",
            "400/400 [==============================] - 0s 1ms/step - loss: 0.0398 - acc: 0.9900 - val_loss: 1.9036 - val_acc: 0.6899\n",
            "Test loss: 1.90364468126297\n",
            "Test accuracy: 0.6899\n",
            "Train loss: 0.010736765873155037\n",
            "Train accuracy: 1.0\n",
            "x label: (500, 28, 28)\n",
            "y label: (500,)\n",
            "x unlabel: (59500, 28, 28)\n",
            "y unlabel: (59500,)\n",
            "4\n",
            "Train on 500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "500/500 [==============================] - 0s 985us/step - loss: 0.3750 - acc: 0.9180 - val_loss: 1.2949 - val_acc: 0.7008\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 0s 903us/step - loss: 0.2582 - acc: 0.9300 - val_loss: 1.2116 - val_acc: 0.7102\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 0s 877us/step - loss: 0.1678 - acc: 0.9440 - val_loss: 1.2947 - val_acc: 0.7107\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 0s 837us/step - loss: 0.1257 - acc: 0.9580 - val_loss: 1.3865 - val_acc: 0.7200\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 0s 951us/step - loss: 0.1053 - acc: 0.9740 - val_loss: 1.3469 - val_acc: 0.7254\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 0s 850us/step - loss: 0.0711 - acc: 0.9700 - val_loss: 1.4342 - val_acc: 0.7173\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 0s 866us/step - loss: 0.0875 - acc: 0.9720 - val_loss: 1.3550 - val_acc: 0.7161\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 0s 893us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 1.3372 - val_acc: 0.7214\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 0s 932us/step - loss: 0.0480 - acc: 0.9880 - val_loss: 1.4088 - val_acc: 0.7242\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 0s 887us/step - loss: 0.0553 - acc: 0.9820 - val_loss: 1.4699 - val_acc: 0.7223\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 0s 840us/step - loss: 0.0369 - acc: 0.9920 - val_loss: 1.4820 - val_acc: 0.7267\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 0s 906us/step - loss: 0.0360 - acc: 0.9860 - val_loss: 1.6866 - val_acc: 0.7146\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 0s 928us/step - loss: 0.0286 - acc: 0.9900 - val_loss: 1.6196 - val_acc: 0.7225\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 0s 947us/step - loss: 0.0341 - acc: 0.9900 - val_loss: 1.6552 - val_acc: 0.7187\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 0s 887us/step - loss: 0.0310 - acc: 0.9920 - val_loss: 1.8040 - val_acc: 0.7063\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 0s 888us/step - loss: 0.0190 - acc: 0.9960 - val_loss: 1.6813 - val_acc: 0.7184\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 0s 896us/step - loss: 0.0305 - acc: 0.9920 - val_loss: 1.7356 - val_acc: 0.7215\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 0s 912us/step - loss: 0.0172 - acc: 0.9960 - val_loss: 1.7664 - val_acc: 0.7192\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 0s 882us/step - loss: 0.0160 - acc: 0.9980 - val_loss: 1.7211 - val_acc: 0.7217\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 0s 835us/step - loss: 0.0259 - acc: 0.9960 - val_loss: 1.7340 - val_acc: 0.7238\n",
            "Test loss: 1.7340067504882812\n",
            "Test accuracy: 0.7238\n",
            "Train loss: 0.0037293282942418957\n",
            "Train accuracy: 1.0\n",
            "x label: (600, 28, 28)\n",
            "y label: (600,)\n",
            "x unlabel: (59400, 28, 28)\n",
            "y unlabel: (59400,)\n",
            "5\n",
            "Train on 600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "600/600 [==============================] - 0s 748us/step - loss: 0.2541 - acc: 0.9333 - val_loss: 1.3096 - val_acc: 0.7256\n",
            "Epoch 2/20\n",
            "600/600 [==============================] - 0s 753us/step - loss: 0.1557 - acc: 0.9550 - val_loss: 1.3689 - val_acc: 0.7351\n",
            "Epoch 3/20\n",
            "600/600 [==============================] - 0s 786us/step - loss: 0.1528 - acc: 0.9567 - val_loss: 1.3937 - val_acc: 0.7347\n",
            "Epoch 4/20\n",
            "600/600 [==============================] - 0s 761us/step - loss: 0.1302 - acc: 0.9533 - val_loss: 1.3383 - val_acc: 0.7330\n",
            "Epoch 5/20\n",
            "600/600 [==============================] - 0s 768us/step - loss: 0.1078 - acc: 0.9733 - val_loss: 1.4986 - val_acc: 0.7276\n",
            "Epoch 6/20\n",
            "600/600 [==============================] - 0s 772us/step - loss: 0.0978 - acc: 0.9700 - val_loss: 1.5272 - val_acc: 0.7312\n",
            "Epoch 7/20\n",
            "600/600 [==============================] - 0s 744us/step - loss: 0.0557 - acc: 0.9817 - val_loss: 1.5658 - val_acc: 0.7353\n",
            "Epoch 8/20\n",
            "600/600 [==============================] - 0s 744us/step - loss: 0.0381 - acc: 0.9933 - val_loss: 1.7022 - val_acc: 0.7366\n",
            "Epoch 9/20\n",
            "600/600 [==============================] - 0s 736us/step - loss: 0.0413 - acc: 0.9850 - val_loss: 1.7516 - val_acc: 0.7339\n",
            "Epoch 10/20\n",
            "600/600 [==============================] - 0s 788us/step - loss: 0.0421 - acc: 0.9850 - val_loss: 1.6804 - val_acc: 0.7384\n",
            "Epoch 11/20\n",
            "600/600 [==============================] - 0s 810us/step - loss: 0.0558 - acc: 0.9850 - val_loss: 1.7089 - val_acc: 0.7389\n",
            "Epoch 12/20\n",
            "600/600 [==============================] - 0s 757us/step - loss: 0.0392 - acc: 0.9883 - val_loss: 1.7018 - val_acc: 0.7383\n",
            "Epoch 13/20\n",
            "600/600 [==============================] - 0s 770us/step - loss: 0.0254 - acc: 0.9950 - val_loss: 1.7160 - val_acc: 0.7373\n",
            "Epoch 14/20\n",
            "600/600 [==============================] - 0s 762us/step - loss: 0.0216 - acc: 0.9950 - val_loss: 1.8151 - val_acc: 0.7360\n",
            "Epoch 15/20\n",
            "600/600 [==============================] - 0s 757us/step - loss: 0.0298 - acc: 0.9900 - val_loss: 1.8140 - val_acc: 0.7356\n",
            "Epoch 16/20\n",
            "600/600 [==============================] - 0s 752us/step - loss: 0.0198 - acc: 0.9950 - val_loss: 1.8504 - val_acc: 0.7343\n",
            "Epoch 17/20\n",
            "600/600 [==============================] - 0s 770us/step - loss: 0.0334 - acc: 0.9867 - val_loss: 1.6979 - val_acc: 0.7354\n",
            "Epoch 18/20\n",
            "600/600 [==============================] - 0s 747us/step - loss: 0.0257 - acc: 0.9933 - val_loss: 1.8694 - val_acc: 0.7292\n",
            "Epoch 19/20\n",
            "600/600 [==============================] - 0s 778us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 1.8743 - val_acc: 0.7332\n",
            "Epoch 20/20\n",
            "600/600 [==============================] - 0s 742us/step - loss: 0.0143 - acc: 0.9967 - val_loss: 1.9775 - val_acc: 0.7262\n",
            "Test loss: 1.9775246072292327\n",
            "Test accuracy: 0.7262\n",
            "Train loss: 0.0018050213773221913\n",
            "Train accuracy: 1.0\n",
            "x label: (700, 28, 28)\n",
            "y label: (700,)\n",
            "x unlabel: (59300, 28, 28)\n",
            "y unlabel: (59300,)\n",
            "6\n",
            "Train on 700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "700/700 [==============================] - 0s 669us/step - loss: 0.2230 - acc: 0.9386 - val_loss: 1.4648 - val_acc: 0.7370\n",
            "Epoch 2/20\n",
            "700/700 [==============================] - 0s 656us/step - loss: 0.1369 - acc: 0.9614 - val_loss: 1.4889 - val_acc: 0.7387\n",
            "Epoch 3/20\n",
            "700/700 [==============================] - 0s 680us/step - loss: 0.1163 - acc: 0.9586 - val_loss: 1.5869 - val_acc: 0.7353\n",
            "Epoch 4/20\n",
            "700/700 [==============================] - 0s 690us/step - loss: 0.0662 - acc: 0.9814 - val_loss: 1.5908 - val_acc: 0.7462\n",
            "Epoch 5/20\n",
            "700/700 [==============================] - 0s 670us/step - loss: 0.0822 - acc: 0.9714 - val_loss: 1.6680 - val_acc: 0.7390\n",
            "Epoch 6/20\n",
            "700/700 [==============================] - 0s 708us/step - loss: 0.0738 - acc: 0.9729 - val_loss: 1.6922 - val_acc: 0.7416\n",
            "Epoch 7/20\n",
            "700/700 [==============================] - 0s 681us/step - loss: 0.0587 - acc: 0.9843 - val_loss: 1.7534 - val_acc: 0.7444\n",
            "Epoch 8/20\n",
            "700/700 [==============================] - 0s 676us/step - loss: 0.0397 - acc: 0.9871 - val_loss: 1.7324 - val_acc: 0.7439\n",
            "Epoch 9/20\n",
            "700/700 [==============================] - 0s 660us/step - loss: 0.0340 - acc: 0.9871 - val_loss: 1.8024 - val_acc: 0.7432\n",
            "Epoch 10/20\n",
            "700/700 [==============================] - 0s 675us/step - loss: 0.0534 - acc: 0.9800 - val_loss: 1.7685 - val_acc: 0.7449\n",
            "Epoch 11/20\n",
            "700/700 [==============================] - 0s 654us/step - loss: 0.0307 - acc: 0.9886 - val_loss: 1.8208 - val_acc: 0.7471\n",
            "Epoch 12/20\n",
            "700/700 [==============================] - 0s 642us/step - loss: 0.0354 - acc: 0.9900 - val_loss: 1.7909 - val_acc: 0.7490\n",
            "Epoch 13/20\n",
            "700/700 [==============================] - 0s 690us/step - loss: 0.0383 - acc: 0.9886 - val_loss: 1.8388 - val_acc: 0.7431\n",
            "Epoch 14/20\n",
            "700/700 [==============================] - 0s 647us/step - loss: 0.0361 - acc: 0.9900 - val_loss: 1.9107 - val_acc: 0.7421\n",
            "Epoch 15/20\n",
            "700/700 [==============================] - 0s 685us/step - loss: 0.0226 - acc: 0.9914 - val_loss: 1.9054 - val_acc: 0.7438\n",
            "Epoch 16/20\n",
            "700/700 [==============================] - 0s 640us/step - loss: 0.0154 - acc: 0.9957 - val_loss: 1.9375 - val_acc: 0.7441\n",
            "Epoch 17/20\n",
            "700/700 [==============================] - 0s 677us/step - loss: 0.0237 - acc: 0.9943 - val_loss: 1.9663 - val_acc: 0.7411\n",
            "Epoch 18/20\n",
            "700/700 [==============================] - 0s 636us/step - loss: 0.0299 - acc: 0.9914 - val_loss: 1.7940 - val_acc: 0.7472\n",
            "Epoch 19/20\n",
            "700/700 [==============================] - 0s 656us/step - loss: 0.0159 - acc: 0.9957 - val_loss: 1.9344 - val_acc: 0.7440\n",
            "Epoch 20/20\n",
            "700/700 [==============================] - 0s 667us/step - loss: 0.0170 - acc: 0.9986 - val_loss: 1.9288 - val_acc: 0.7455\n",
            "Test loss: 1.928786107826233\n",
            "Test accuracy: 0.7455\n",
            "Train loss: 0.002914215138925361\n",
            "Train accuracy: 1.0\n",
            "x label: (800, 28, 28)\n",
            "y label: (800,)\n",
            "x unlabel: (59200, 28, 28)\n",
            "y unlabel: (59200,)\n",
            "7\n",
            "Train on 800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "800/800 [==============================] - 0s 573us/step - loss: 0.2631 - acc: 0.9325 - val_loss: 1.3961 - val_acc: 0.7444\n",
            "Epoch 2/20\n",
            "800/800 [==============================] - 1s 632us/step - loss: 0.1794 - acc: 0.9500 - val_loss: 1.3659 - val_acc: 0.7525\n",
            "Epoch 3/20\n",
            "800/800 [==============================] - 0s 580us/step - loss: 0.1260 - acc: 0.9638 - val_loss: 1.4692 - val_acc: 0.7473\n",
            "Epoch 4/20\n",
            "800/800 [==============================] - 0s 606us/step - loss: 0.1107 - acc: 0.9638 - val_loss: 1.3655 - val_acc: 0.7531\n",
            "Epoch 5/20\n",
            "800/800 [==============================] - 0s 578us/step - loss: 0.0798 - acc: 0.9738 - val_loss: 1.5188 - val_acc: 0.7516\n",
            "Epoch 6/20\n",
            "800/800 [==============================] - 0s 600us/step - loss: 0.0626 - acc: 0.9825 - val_loss: 1.5466 - val_acc: 0.7525\n",
            "Epoch 7/20\n",
            "800/800 [==============================] - 0s 581us/step - loss: 0.0575 - acc: 0.9825 - val_loss: 1.5787 - val_acc: 0.7554\n",
            "Epoch 8/20\n",
            "800/800 [==============================] - 0s 556us/step - loss: 0.0566 - acc: 0.9863 - val_loss: 1.5659 - val_acc: 0.7529\n",
            "Epoch 9/20\n",
            "800/800 [==============================] - 0s 597us/step - loss: 0.0485 - acc: 0.9875 - val_loss: 1.6676 - val_acc: 0.7478\n",
            "Epoch 10/20\n",
            "800/800 [==============================] - 0s 563us/step - loss: 0.0484 - acc: 0.9863 - val_loss: 1.6835 - val_acc: 0.7512\n",
            "Epoch 11/20\n",
            "800/800 [==============================] - 0s 571us/step - loss: 0.0453 - acc: 0.9888 - val_loss: 1.7344 - val_acc: 0.7506\n",
            "Epoch 12/20\n",
            "800/800 [==============================] - 0s 570us/step - loss: 0.0427 - acc: 0.9862 - val_loss: 1.7549 - val_acc: 0.7526\n",
            "Epoch 13/20\n",
            "800/800 [==============================] - 0s 575us/step - loss: 0.0359 - acc: 0.9900 - val_loss: 1.7442 - val_acc: 0.7531\n",
            "Epoch 14/20\n",
            "800/800 [==============================] - 0s 591us/step - loss: 0.0364 - acc: 0.9900 - val_loss: 1.7367 - val_acc: 0.7546\n",
            "Epoch 15/20\n",
            "800/800 [==============================] - 0s 607us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 1.7518 - val_acc: 0.7567\n",
            "Epoch 16/20\n",
            "800/800 [==============================] - 0s 588us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 1.8254 - val_acc: 0.7558\n",
            "Epoch 17/20\n",
            "800/800 [==============================] - 0s 599us/step - loss: 0.0283 - acc: 0.9888 - val_loss: 1.7811 - val_acc: 0.7569\n",
            "Epoch 18/20\n",
            "800/800 [==============================] - 0s 566us/step - loss: 0.0255 - acc: 0.9925 - val_loss: 1.8462 - val_acc: 0.7534\n",
            "Epoch 19/20\n",
            "800/800 [==============================] - 0s 570us/step - loss: 0.0219 - acc: 0.9938 - val_loss: 1.9584 - val_acc: 0.7417\n",
            "Epoch 20/20\n",
            "800/800 [==============================] - 0s 610us/step - loss: 0.0156 - acc: 0.9975 - val_loss: 1.9027 - val_acc: 0.7566\n",
            "Test loss: 1.902685654616356\n",
            "Test accuracy: 0.7566\n",
            "Train loss: 0.0024866116175923025\n",
            "Train accuracy: 0.99875\n",
            "x label: (900, 28, 28)\n",
            "y label: (900,)\n",
            "x unlabel: (59100, 28, 28)\n",
            "y unlabel: (59100,)\n",
            "8\n",
            "Train on 900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "900/900 [==============================] - 1s 588us/step - loss: 0.1819 - acc: 0.9489 - val_loss: 1.6358 - val_acc: 0.7421\n",
            "Epoch 2/20\n",
            "900/900 [==============================] - 1s 599us/step - loss: 0.1347 - acc: 0.9567 - val_loss: 1.4742 - val_acc: 0.7563\n",
            "Epoch 3/20\n",
            "900/900 [==============================] - 1s 590us/step - loss: 0.0884 - acc: 0.9722 - val_loss: 1.4631 - val_acc: 0.7569\n",
            "Epoch 4/20\n",
            "900/900 [==============================] - 1s 568us/step - loss: 0.0564 - acc: 0.9800 - val_loss: 1.6074 - val_acc: 0.7580\n",
            "Epoch 5/20\n",
            "900/900 [==============================] - 1s 591us/step - loss: 0.0401 - acc: 0.9889 - val_loss: 1.6294 - val_acc: 0.7604\n",
            "Epoch 6/20\n",
            "900/900 [==============================] - 1s 561us/step - loss: 0.0394 - acc: 0.9878 - val_loss: 1.6897 - val_acc: 0.7634\n",
            "Epoch 7/20\n",
            "900/900 [==============================] - 1s 563us/step - loss: 0.0616 - acc: 0.9733 - val_loss: 1.6792 - val_acc: 0.7578\n",
            "Epoch 8/20\n",
            "900/900 [==============================] - 1s 564us/step - loss: 0.0376 - acc: 0.9856 - val_loss: 1.6509 - val_acc: 0.7623\n",
            "Epoch 9/20\n",
            "900/900 [==============================] - 1s 560us/step - loss: 0.0410 - acc: 0.9889 - val_loss: 1.6000 - val_acc: 0.7616\n",
            "Epoch 10/20\n",
            "900/900 [==============================] - 0s 546us/step - loss: 0.0383 - acc: 0.9889 - val_loss: 1.6633 - val_acc: 0.7582\n",
            "Epoch 11/20\n",
            "900/900 [==============================] - 1s 562us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 1.8083 - val_acc: 0.7582\n",
            "Epoch 12/20\n",
            "900/900 [==============================] - 0s 548us/step - loss: 0.0442 - acc: 0.9822 - val_loss: 1.7991 - val_acc: 0.7589\n",
            "Epoch 13/20\n",
            "900/900 [==============================] - 0s 546us/step - loss: 0.0181 - acc: 0.9967 - val_loss: 1.8614 - val_acc: 0.7534\n",
            "Epoch 14/20\n",
            "900/900 [==============================] - 0s 543us/step - loss: 0.0336 - acc: 0.9922 - val_loss: 1.8665 - val_acc: 0.7551\n",
            "Epoch 15/20\n",
            "900/900 [==============================] - 0s 530us/step - loss: 0.0261 - acc: 0.9911 - val_loss: 1.7650 - val_acc: 0.7604\n",
            "Epoch 16/20\n",
            "900/900 [==============================] - 0s 524us/step - loss: 0.0211 - acc: 0.9933 - val_loss: 1.7553 - val_acc: 0.7580\n",
            "Epoch 17/20\n",
            "900/900 [==============================] - 1s 556us/step - loss: 0.0268 - acc: 0.9911 - val_loss: 1.8404 - val_acc: 0.7527\n",
            "Epoch 18/20\n",
            "900/900 [==============================] - 1s 607us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 1.8431 - val_acc: 0.7597\n",
            "Epoch 19/20\n",
            "900/900 [==============================] - 1s 595us/step - loss: 0.0131 - acc: 0.9967 - val_loss: 1.8382 - val_acc: 0.7584\n",
            "Epoch 20/20\n",
            "900/900 [==============================] - 1s 587us/step - loss: 0.0150 - acc: 0.9989 - val_loss: 1.8736 - val_acc: 0.7606\n",
            "Test loss: 1.8735555830955506\n",
            "Test accuracy: 0.7606\n",
            "Train loss: 0.0006712294324956423\n",
            "Train accuracy: 1.0\n",
            "x label: (1000, 28, 28)\n",
            "y label: (1000,)\n",
            "x unlabel: (59000, 28, 28)\n",
            "y unlabel: (59000,)\n",
            "9\n",
            "Train on 1000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 1s 518us/step - loss: 0.1903 - acc: 0.9560 - val_loss: 1.5433 - val_acc: 0.7589\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 1s 516us/step - loss: 0.1020 - acc: 0.9740 - val_loss: 1.5004 - val_acc: 0.7650\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 1s 507us/step - loss: 0.1358 - acc: 0.9610 - val_loss: 1.5039 - val_acc: 0.7640\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 0s 496us/step - loss: 0.0978 - acc: 0.9730 - val_loss: 1.4185 - val_acc: 0.7638\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 1s 514us/step - loss: 0.0582 - acc: 0.9860 - val_loss: 1.6105 - val_acc: 0.7603\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 1s 511us/step - loss: 0.0643 - acc: 0.9770 - val_loss: 1.5597 - val_acc: 0.7586\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 1s 503us/step - loss: 0.0452 - acc: 0.9860 - val_loss: 1.6027 - val_acc: 0.7659\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 1s 534us/step - loss: 0.0368 - acc: 0.9870 - val_loss: 1.6265 - val_acc: 0.7695\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 1s 542us/step - loss: 0.0398 - acc: 0.9870 - val_loss: 1.6705 - val_acc: 0.7655\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 1s 534us/step - loss: 0.0435 - acc: 0.9830 - val_loss: 1.6727 - val_acc: 0.7647\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 1s 519us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 1.6241 - val_acc: 0.7694\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 1s 538us/step - loss: 0.0302 - acc: 0.9890 - val_loss: 1.6168 - val_acc: 0.7630\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 1s 502us/step - loss: 0.0385 - acc: 0.9920 - val_loss: 1.6377 - val_acc: 0.7634\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 1s 565us/step - loss: 0.0339 - acc: 0.9890 - val_loss: 1.5732 - val_acc: 0.7673\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 1s 524us/step - loss: 0.0294 - acc: 0.9890 - val_loss: 1.6473 - val_acc: 0.7700\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 1s 511us/step - loss: 0.0184 - acc: 0.9940 - val_loss: 1.7041 - val_acc: 0.7697\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 1s 541us/step - loss: 0.0151 - acc: 0.9950 - val_loss: 1.7724 - val_acc: 0.7698\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 1s 504us/step - loss: 0.0200 - acc: 0.9930 - val_loss: 1.6594 - val_acc: 0.7699\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 1s 537us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 1.6842 - val_acc: 0.7681\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 0s 481us/step - loss: 0.0146 - acc: 0.9950 - val_loss: 1.7230 - val_acc: 0.7753\n",
            "Test loss: 1.7229786752223968\n",
            "Test accuracy: 0.7753\n",
            "Train loss: 0.00044964006095824514\n",
            "Train accuracy: 1.0\n",
            "x label: (1100, 28, 28)\n",
            "y label: (1100,)\n",
            "x unlabel: (58900, 28, 28)\n",
            "y unlabel: (58900,)\n",
            "10\n",
            "Train on 1100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1100/1100 [==============================] - 0s 450us/step - loss: 0.1572 - acc: 0.9600 - val_loss: 1.5766 - val_acc: 0.7586\n",
            "Epoch 2/20\n",
            "1100/1100 [==============================] - 1s 469us/step - loss: 0.1195 - acc: 0.9673 - val_loss: 1.3782 - val_acc: 0.7746\n",
            "Epoch 3/20\n",
            "1100/1100 [==============================] - 1s 469us/step - loss: 0.0706 - acc: 0.9809 - val_loss: 1.5290 - val_acc: 0.7702\n",
            "Epoch 4/20\n",
            "1100/1100 [==============================] - 0s 453us/step - loss: 0.0874 - acc: 0.9709 - val_loss: 1.5096 - val_acc: 0.7697\n",
            "Epoch 5/20\n",
            "1100/1100 [==============================] - 0s 454us/step - loss: 0.0543 - acc: 0.9818 - val_loss: 1.5156 - val_acc: 0.7751\n",
            "Epoch 6/20\n",
            "1100/1100 [==============================] - 1s 474us/step - loss: 0.0504 - acc: 0.9845 - val_loss: 1.4328 - val_acc: 0.7810\n",
            "Epoch 7/20\n",
            "1100/1100 [==============================] - 0s 446us/step - loss: 0.0528 - acc: 0.9827 - val_loss: 1.5152 - val_acc: 0.7815\n",
            "Epoch 8/20\n",
            "1100/1100 [==============================] - 1s 477us/step - loss: 0.0362 - acc: 0.9873 - val_loss: 1.4228 - val_acc: 0.7826\n",
            "Epoch 9/20\n",
            "1100/1100 [==============================] - 1s 455us/step - loss: 0.0562 - acc: 0.9827 - val_loss: 1.5482 - val_acc: 0.7693\n",
            "Epoch 10/20\n",
            "1100/1100 [==============================] - 0s 440us/step - loss: 0.0306 - acc: 0.9882 - val_loss: 1.5266 - val_acc: 0.7804\n",
            "Epoch 11/20\n",
            "1100/1100 [==============================] - 0s 454us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 1.5517 - val_acc: 0.7800\n",
            "Epoch 12/20\n",
            "1100/1100 [==============================] - 1s 466us/step - loss: 0.0224 - acc: 0.9936 - val_loss: 1.5870 - val_acc: 0.7776\n",
            "Epoch 13/20\n",
            "1100/1100 [==============================] - 1s 461us/step - loss: 0.0200 - acc: 0.9945 - val_loss: 1.6266 - val_acc: 0.7795\n",
            "Epoch 14/20\n",
            "1100/1100 [==============================] - 1s 469us/step - loss: 0.0256 - acc: 0.9918 - val_loss: 1.6685 - val_acc: 0.7758\n",
            "Epoch 15/20\n",
            "1100/1100 [==============================] - 1s 464us/step - loss: 0.0320 - acc: 0.9918 - val_loss: 1.6083 - val_acc: 0.7774\n",
            "Epoch 16/20\n",
            "1100/1100 [==============================] - 0s 453us/step - loss: 0.0284 - acc: 0.9918 - val_loss: 1.6513 - val_acc: 0.7723\n",
            "Epoch 17/20\n",
            "1100/1100 [==============================] - 1s 469us/step - loss: 0.0248 - acc: 0.9909 - val_loss: 1.6313 - val_acc: 0.7815\n",
            "Epoch 18/20\n",
            "1100/1100 [==============================] - 1s 457us/step - loss: 0.0164 - acc: 0.9927 - val_loss: 1.7366 - val_acc: 0.7753\n",
            "Epoch 19/20\n",
            "1100/1100 [==============================] - 1s 477us/step - loss: 0.0267 - acc: 0.9927 - val_loss: 1.5399 - val_acc: 0.7829\n",
            "Epoch 20/20\n",
            "1100/1100 [==============================] - 1s 469us/step - loss: 0.0186 - acc: 0.9964 - val_loss: 1.5671 - val_acc: 0.7858\n",
            "Test loss: 1.567111701130867\n",
            "Test accuracy: 0.7858\n",
            "Train loss: 0.0007357377020190887\n",
            "Train accuracy: 1.0\n",
            "x label: (1200, 28, 28)\n",
            "y label: (1200,)\n",
            "x unlabel: (58800, 28, 28)\n",
            "y unlabel: (58800,)\n",
            "11\n",
            "Train on 1200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1200/1200 [==============================] - 1s 431us/step - loss: 0.1661 - acc: 0.9633 - val_loss: 1.4754 - val_acc: 0.7785\n",
            "Epoch 2/20\n",
            "1200/1200 [==============================] - 1s 449us/step - loss: 0.1176 - acc: 0.9650 - val_loss: 1.3571 - val_acc: 0.7759\n",
            "Epoch 3/20\n",
            "1200/1200 [==============================] - 1s 454us/step - loss: 0.0890 - acc: 0.9725 - val_loss: 1.3666 - val_acc: 0.7863\n",
            "Epoch 4/20\n",
            "1200/1200 [==============================] - 1s 454us/step - loss: 0.0496 - acc: 0.9883 - val_loss: 1.4194 - val_acc: 0.7825\n",
            "Epoch 5/20\n",
            "1200/1200 [==============================] - 1s 447us/step - loss: 0.0327 - acc: 0.9908 - val_loss: 1.5070 - val_acc: 0.7827\n",
            "Epoch 6/20\n",
            "1200/1200 [==============================] - 1s 434us/step - loss: 0.0444 - acc: 0.9858 - val_loss: 1.3801 - val_acc: 0.7839\n",
            "Epoch 7/20\n",
            "1200/1200 [==============================] - 1s 427us/step - loss: 0.0355 - acc: 0.9892 - val_loss: 1.3762 - val_acc: 0.7860\n",
            "Epoch 8/20\n",
            "1200/1200 [==============================] - 1s 417us/step - loss: 0.0342 - acc: 0.9925 - val_loss: 1.4539 - val_acc: 0.7868\n",
            "Epoch 9/20\n",
            "1200/1200 [==============================] - 1s 424us/step - loss: 0.0242 - acc: 0.9917 - val_loss: 1.5055 - val_acc: 0.7875\n",
            "Epoch 10/20\n",
            "1200/1200 [==============================] - 1s 427us/step - loss: 0.0359 - acc: 0.9883 - val_loss: 1.4849 - val_acc: 0.7890\n",
            "Epoch 11/20\n",
            "1200/1200 [==============================] - 1s 420us/step - loss: 0.0295 - acc: 0.9933 - val_loss: 1.5205 - val_acc: 0.7804\n",
            "Epoch 12/20\n",
            "1200/1200 [==============================] - 1s 422us/step - loss: 0.0291 - acc: 0.9908 - val_loss: 1.6364 - val_acc: 0.7749\n",
            "Epoch 13/20\n",
            "1200/1200 [==============================] - 1s 419us/step - loss: 0.0289 - acc: 0.9933 - val_loss: 1.5473 - val_acc: 0.7854\n",
            "Epoch 14/20\n",
            "1200/1200 [==============================] - 1s 425us/step - loss: 0.0204 - acc: 0.9958 - val_loss: 1.5356 - val_acc: 0.7853\n",
            "Epoch 15/20\n",
            "1200/1200 [==============================] - 0s 412us/step - loss: 0.0265 - acc: 0.9892 - val_loss: 1.5007 - val_acc: 0.7853\n",
            "Epoch 16/20\n",
            "1200/1200 [==============================] - 1s 426us/step - loss: 0.0287 - acc: 0.9908 - val_loss: 1.5201 - val_acc: 0.7830\n",
            "Epoch 17/20\n",
            "1200/1200 [==============================] - 0s 409us/step - loss: 0.0206 - acc: 0.9933 - val_loss: 1.5979 - val_acc: 0.7848\n",
            "Epoch 18/20\n",
            "1200/1200 [==============================] - 1s 425us/step - loss: 0.0190 - acc: 0.9925 - val_loss: 1.6122 - val_acc: 0.7816\n",
            "Epoch 19/20\n",
            "1200/1200 [==============================] - 0s 406us/step - loss: 0.0164 - acc: 0.9942 - val_loss: 1.6004 - val_acc: 0.7861\n",
            "Epoch 20/20\n",
            "1200/1200 [==============================] - 1s 430us/step - loss: 0.0118 - acc: 0.9975 - val_loss: 1.6408 - val_acc: 0.7862\n",
            "Test loss: 1.6407544478535652\n",
            "Test accuracy: 0.7862\n",
            "Train loss: 0.00018991827879176525\n",
            "Train accuracy: 1.0\n",
            "x label: (1300, 28, 28)\n",
            "y label: (1300,)\n",
            "x unlabel: (58700, 28, 28)\n",
            "y unlabel: (58700,)\n",
            "12\n",
            "Train on 1300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1300/1300 [==============================] - 1s 400us/step - loss: 0.1705 - acc: 0.9608 - val_loss: 1.3466 - val_acc: 0.7831\n",
            "Epoch 2/20\n",
            "1300/1300 [==============================] - 1s 409us/step - loss: 0.0984 - acc: 0.9700 - val_loss: 1.4240 - val_acc: 0.7846\n",
            "Epoch 3/20\n",
            "1300/1300 [==============================] - 1s 400us/step - loss: 0.0772 - acc: 0.9700 - val_loss: 1.5118 - val_acc: 0.7829\n",
            "Epoch 4/20\n",
            "1300/1300 [==============================] - 1s 405us/step - loss: 0.0520 - acc: 0.9808 - val_loss: 1.4189 - val_acc: 0.7899\n",
            "Epoch 5/20\n",
            "1300/1300 [==============================] - 1s 404us/step - loss: 0.0537 - acc: 0.9823 - val_loss: 1.4273 - val_acc: 0.7879\n",
            "Epoch 6/20\n",
            "1300/1300 [==============================] - 1s 400us/step - loss: 0.0267 - acc: 0.9908 - val_loss: 1.5798 - val_acc: 0.7877\n",
            "Epoch 7/20\n",
            "1300/1300 [==============================] - 1s 397us/step - loss: 0.0389 - acc: 0.9900 - val_loss: 1.5014 - val_acc: 0.7903\n",
            "Epoch 8/20\n",
            "1300/1300 [==============================] - 1s 418us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 1.6201 - val_acc: 0.7838\n",
            "Epoch 9/20\n",
            "1300/1300 [==============================] - 1s 392us/step - loss: 0.0387 - acc: 0.9885 - val_loss: 1.4813 - val_acc: 0.7904\n",
            "Epoch 10/20\n",
            "1300/1300 [==============================] - 1s 419us/step - loss: 0.0332 - acc: 0.9892 - val_loss: 1.5771 - val_acc: 0.7872\n",
            "Epoch 11/20\n",
            "1300/1300 [==============================] - 1s 394us/step - loss: 0.0328 - acc: 0.9892 - val_loss: 1.6778 - val_acc: 0.7890\n",
            "Epoch 12/20\n",
            "1300/1300 [==============================] - 1s 401us/step - loss: 0.0217 - acc: 0.9946 - val_loss: 1.6213 - val_acc: 0.7917\n",
            "Epoch 13/20\n",
            "1300/1300 [==============================] - 1s 398us/step - loss: 0.0228 - acc: 0.9923 - val_loss: 1.6656 - val_acc: 0.7880\n",
            "Epoch 14/20\n",
            "1300/1300 [==============================] - 1s 404us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 1.6188 - val_acc: 0.7868\n",
            "Epoch 15/20\n",
            "1300/1300 [==============================] - 1s 389us/step - loss: 0.0170 - acc: 0.9969 - val_loss: 1.6605 - val_acc: 0.7892\n",
            "Epoch 16/20\n",
            "1300/1300 [==============================] - 1s 413us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 1.6844 - val_acc: 0.7875\n",
            "Epoch 17/20\n",
            "1300/1300 [==============================] - 1s 399us/step - loss: 0.0147 - acc: 0.9969 - val_loss: 1.7824 - val_acc: 0.7836\n",
            "Epoch 18/20\n",
            "1300/1300 [==============================] - 1s 404us/step - loss: 0.0140 - acc: 0.9962 - val_loss: 1.7320 - val_acc: 0.7919\n",
            "Epoch 19/20\n",
            "1300/1300 [==============================] - 0s 384us/step - loss: 0.0153 - acc: 0.9962 - val_loss: 1.7548 - val_acc: 0.7894\n",
            "Epoch 20/20\n",
            "1300/1300 [==============================] - 1s 405us/step - loss: 0.0280 - acc: 0.9908 - val_loss: 1.7248 - val_acc: 0.7916\n",
            "Test loss: 1.7248160555124283\n",
            "Test accuracy: 0.7916\n",
            "Train loss: 0.000433623258980403\n",
            "Train accuracy: 1.0\n",
            "x label: (1400, 28, 28)\n",
            "y label: (1400,)\n",
            "x unlabel: (58600, 28, 28)\n",
            "y unlabel: (58600,)\n",
            "13\n",
            "Train on 1400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1400/1400 [==============================] - 1s 385us/step - loss: 0.1377 - acc: 0.9643 - val_loss: 1.5435 - val_acc: 0.7826\n",
            "Epoch 2/20\n",
            "1400/1400 [==============================] - 1s 384us/step - loss: 0.1105 - acc: 0.9721 - val_loss: 1.4752 - val_acc: 0.7853\n",
            "Epoch 3/20\n",
            "1400/1400 [==============================] - 1s 362us/step - loss: 0.0975 - acc: 0.9671 - val_loss: 1.3075 - val_acc: 0.7928\n",
            "Epoch 4/20\n",
            "1400/1400 [==============================] - 1s 388us/step - loss: 0.0634 - acc: 0.9800 - val_loss: 1.3708 - val_acc: 0.7894\n",
            "Epoch 5/20\n",
            "1400/1400 [==============================] - 1s 390us/step - loss: 0.0495 - acc: 0.9893 - val_loss: 1.5131 - val_acc: 0.7917\n",
            "Epoch 6/20\n",
            "1400/1400 [==============================] - 1s 376us/step - loss: 0.0535 - acc: 0.9850 - val_loss: 1.4872 - val_acc: 0.7893\n",
            "Epoch 7/20\n",
            "1400/1400 [==============================] - 1s 366us/step - loss: 0.0470 - acc: 0.9893 - val_loss: 1.5840 - val_acc: 0.7919\n",
            "Epoch 8/20\n",
            "1400/1400 [==============================] - 1s 380us/step - loss: 0.0377 - acc: 0.9914 - val_loss: 1.4730 - val_acc: 0.7929\n",
            "Epoch 9/20\n",
            "1400/1400 [==============================] - 1s 364us/step - loss: 0.0284 - acc: 0.9936 - val_loss: 1.5804 - val_acc: 0.7952\n",
            "Epoch 10/20\n",
            "1400/1400 [==============================] - 1s 396us/step - loss: 0.0426 - acc: 0.9864 - val_loss: 1.6358 - val_acc: 0.7890\n",
            "Epoch 11/20\n",
            "1400/1400 [==============================] - 1s 393us/step - loss: 0.0321 - acc: 0.9929 - val_loss: 1.6504 - val_acc: 0.7870\n",
            "Epoch 12/20\n",
            "1400/1400 [==============================] - 1s 385us/step - loss: 0.0313 - acc: 0.9914 - val_loss: 1.6431 - val_acc: 0.7931\n",
            "Epoch 13/20\n",
            "1400/1400 [==============================] - 1s 369us/step - loss: 0.0459 - acc: 0.9886 - val_loss: 1.6371 - val_acc: 0.7852\n",
            "Epoch 14/20\n",
            "1400/1400 [==============================] - 1s 392us/step - loss: 0.0282 - acc: 0.9957 - val_loss: 1.5879 - val_acc: 0.7925\n",
            "Epoch 15/20\n",
            "1400/1400 [==============================] - 1s 383us/step - loss: 0.0315 - acc: 0.9950 - val_loss: 1.6499 - val_acc: 0.7963\n",
            "Epoch 16/20\n",
            "1400/1400 [==============================] - 1s 371us/step - loss: 0.0228 - acc: 0.9957 - val_loss: 1.6723 - val_acc: 0.7919\n",
            "Epoch 17/20\n",
            "1400/1400 [==============================] - 1s 386us/step - loss: 0.0278 - acc: 0.9943 - val_loss: 1.5437 - val_acc: 0.7950\n",
            "Epoch 18/20\n",
            "1400/1400 [==============================] - 1s 365us/step - loss: 0.0314 - acc: 0.9914 - val_loss: 1.6006 - val_acc: 0.7955\n",
            "Epoch 19/20\n",
            "1400/1400 [==============================] - 1s 389us/step - loss: 0.0309 - acc: 0.9929 - val_loss: 1.7167 - val_acc: 0.7895\n",
            "Epoch 20/20\n",
            "1400/1400 [==============================] - 1s 372us/step - loss: 0.0246 - acc: 0.9957 - val_loss: 1.6798 - val_acc: 0.7965\n",
            "Test loss: 1.6798249463796615\n",
            "Test accuracy: 0.7965\n",
            "Train loss: 0.011685530733790464\n",
            "Train accuracy: 0.9992857142857143\n",
            "x label: (1500, 28, 28)\n",
            "y label: (1500,)\n",
            "x unlabel: (58500, 28, 28)\n",
            "y unlabel: (58500,)\n",
            "14\n",
            "Train on 1500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 1s 374us/step - loss: 0.1474 - acc: 0.9647 - val_loss: 1.4986 - val_acc: 0.7909\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 1s 354us/step - loss: 0.0724 - acc: 0.9780 - val_loss: 1.4170 - val_acc: 0.7902\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 1s 359us/step - loss: 0.0677 - acc: 0.9807 - val_loss: 1.5033 - val_acc: 0.7917\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 1s 350us/step - loss: 0.0652 - acc: 0.9800 - val_loss: 1.4013 - val_acc: 0.7938\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 1s 361us/step - loss: 0.0480 - acc: 0.9833 - val_loss: 1.4095 - val_acc: 0.7943\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 1s 368us/step - loss: 0.0526 - acc: 0.9847 - val_loss: 1.4885 - val_acc: 0.7959\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 1s 365us/step - loss: 0.0484 - acc: 0.9900 - val_loss: 1.4092 - val_acc: 0.7947\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 1s 349us/step - loss: 0.0360 - acc: 0.9907 - val_loss: 1.4652 - val_acc: 0.7967\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 1s 366us/step - loss: 0.0353 - acc: 0.9893 - val_loss: 1.4687 - val_acc: 0.7928\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 1s 359us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 1.5289 - val_acc: 0.7942\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 1s 360us/step - loss: 0.0235 - acc: 0.9920 - val_loss: 1.4860 - val_acc: 0.7952\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 1s 348us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 1.6173 - val_acc: 0.7899\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 1s 368us/step - loss: 0.0159 - acc: 0.9980 - val_loss: 1.5363 - val_acc: 0.7910\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 1s 373us/step - loss: 0.0194 - acc: 0.9967 - val_loss: 1.6404 - val_acc: 0.7925\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 1s 353us/step - loss: 0.0215 - acc: 0.9927 - val_loss: 1.5722 - val_acc: 0.7934\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 1s 360us/step - loss: 0.0135 - acc: 0.9967 - val_loss: 1.6808 - val_acc: 0.7904\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 1s 367us/step - loss: 0.0210 - acc: 0.9927 - val_loss: 1.5836 - val_acc: 0.7942\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 1s 353us/step - loss: 0.0224 - acc: 0.9913 - val_loss: 1.6907 - val_acc: 0.7902\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 1s 349us/step - loss: 0.0262 - acc: 0.9907 - val_loss: 1.6144 - val_acc: 0.7885\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 1s 370us/step - loss: 0.0235 - acc: 0.9887 - val_loss: 1.5969 - val_acc: 0.7880\n",
            "Test loss: 1.5968846809446813\n",
            "Test accuracy: 0.788\n",
            "Train loss: 0.0005391875277664439\n",
            "Train accuracy: 1.0\n",
            "x label: (1600, 28, 28)\n",
            "y label: (1600,)\n",
            "x unlabel: (58400, 28, 28)\n",
            "y unlabel: (58400,)\n",
            "15\n",
            "Train on 1600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1600/1600 [==============================] - 1s 358us/step - loss: 0.1295 - acc: 0.9688 - val_loss: 1.4524 - val_acc: 0.7898\n",
            "Epoch 2/20\n",
            "1600/1600 [==============================] - 1s 345us/step - loss: 0.0978 - acc: 0.9731 - val_loss: 1.2443 - val_acc: 0.7973\n",
            "Epoch 3/20\n",
            "1600/1600 [==============================] - 1s 338us/step - loss: 0.0508 - acc: 0.9863 - val_loss: 1.3846 - val_acc: 0.7943\n",
            "Epoch 4/20\n",
            "1600/1600 [==============================] - 1s 343us/step - loss: 0.0527 - acc: 0.9844 - val_loss: 1.4308 - val_acc: 0.7964\n",
            "Epoch 5/20\n",
            "1600/1600 [==============================] - 1s 359us/step - loss: 0.0514 - acc: 0.9819 - val_loss: 1.4550 - val_acc: 0.7909\n",
            "Epoch 6/20\n",
            "1600/1600 [==============================] - 1s 337us/step - loss: 0.0449 - acc: 0.9844 - val_loss: 1.2929 - val_acc: 0.7998\n",
            "Epoch 7/20\n",
            "1600/1600 [==============================] - 1s 334us/step - loss: 0.0408 - acc: 0.9863 - val_loss: 1.4879 - val_acc: 0.7918\n",
            "Epoch 8/20\n",
            "1600/1600 [==============================] - 1s 347us/step - loss: 0.0342 - acc: 0.9888 - val_loss: 1.4459 - val_acc: 0.7999\n",
            "Epoch 9/20\n",
            "1600/1600 [==============================] - 1s 346us/step - loss: 0.0372 - acc: 0.9900 - val_loss: 1.3725 - val_acc: 0.8010\n",
            "Epoch 10/20\n",
            "1600/1600 [==============================] - 1s 354us/step - loss: 0.0284 - acc: 0.9913 - val_loss: 1.4935 - val_acc: 0.7994\n",
            "Epoch 11/20\n",
            "1600/1600 [==============================] - 1s 333us/step - loss: 0.0279 - acc: 0.9900 - val_loss: 1.5743 - val_acc: 0.7942\n",
            "Epoch 12/20\n",
            "1600/1600 [==============================] - 1s 350us/step - loss: 0.0210 - acc: 0.9925 - val_loss: 1.5463 - val_acc: 0.7964\n",
            "Epoch 13/20\n",
            "1600/1600 [==============================] - 1s 343us/step - loss: 0.0238 - acc: 0.9944 - val_loss: 1.5962 - val_acc: 0.7939\n",
            "Epoch 14/20\n",
            "1600/1600 [==============================] - 1s 342us/step - loss: 0.0182 - acc: 0.9931 - val_loss: 1.5356 - val_acc: 0.7954\n",
            "Epoch 15/20\n",
            "1600/1600 [==============================] - 1s 360us/step - loss: 0.0162 - acc: 0.9950 - val_loss: 1.5991 - val_acc: 0.7963\n",
            "Epoch 16/20\n",
            "1600/1600 [==============================] - 1s 338us/step - loss: 0.0211 - acc: 0.9938 - val_loss: 1.5574 - val_acc: 0.7997\n",
            "Epoch 17/20\n",
            "1600/1600 [==============================] - 1s 342us/step - loss: 0.0210 - acc: 0.9944 - val_loss: 1.5855 - val_acc: 0.7998\n",
            "Epoch 18/20\n",
            "1600/1600 [==============================] - 1s 335us/step - loss: 0.0188 - acc: 0.9944 - val_loss: 1.5586 - val_acc: 0.7938\n",
            "Epoch 19/20\n",
            "1600/1600 [==============================] - 1s 356us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 1.6535 - val_acc: 0.7987\n",
            "Epoch 20/20\n",
            "1600/1600 [==============================] - 1s 324us/step - loss: 0.0171 - acc: 0.9944 - val_loss: 1.6840 - val_acc: 0.7983\n",
            "Test loss: 1.6840100555717945\n",
            "Test accuracy: 0.7983\n",
            "Train loss: 0.00022519777564923517\n",
            "Train accuracy: 1.0\n",
            "x label: (1700, 28, 28)\n",
            "y label: (1700,)\n",
            "x unlabel: (58300, 28, 28)\n",
            "y unlabel: (58300,)\n",
            "16\n",
            "Train on 1700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1700/1700 [==============================] - 1s 349us/step - loss: 0.1211 - acc: 0.9671 - val_loss: 1.2980 - val_acc: 0.7997\n",
            "Epoch 2/20\n",
            "1700/1700 [==============================] - 1s 342us/step - loss: 0.0911 - acc: 0.9694 - val_loss: 1.2586 - val_acc: 0.7972\n",
            "Epoch 3/20\n",
            "1700/1700 [==============================] - 1s 344us/step - loss: 0.0556 - acc: 0.9788 - val_loss: 1.5327 - val_acc: 0.7990\n",
            "Epoch 4/20\n",
            "1700/1700 [==============================] - 1s 347us/step - loss: 0.0484 - acc: 0.9818 - val_loss: 1.4108 - val_acc: 0.8018\n",
            "Epoch 5/20\n",
            "1700/1700 [==============================] - 1s 337us/step - loss: 0.0488 - acc: 0.9871 - val_loss: 1.5567 - val_acc: 0.7958\n",
            "Epoch 6/20\n",
            "1700/1700 [==============================] - 1s 349us/step - loss: 0.0325 - acc: 0.9900 - val_loss: 1.4824 - val_acc: 0.8017\n",
            "Epoch 7/20\n",
            "1700/1700 [==============================] - 1s 326us/step - loss: 0.0312 - acc: 0.9882 - val_loss: 1.4021 - val_acc: 0.8013\n",
            "Epoch 8/20\n",
            "1700/1700 [==============================] - 1s 326us/step - loss: 0.0225 - acc: 0.9918 - val_loss: 1.4906 - val_acc: 0.8037\n",
            "Epoch 9/20\n",
            "1700/1700 [==============================] - 1s 325us/step - loss: 0.0317 - acc: 0.9882 - val_loss: 1.5434 - val_acc: 0.8023\n",
            "Epoch 10/20\n",
            "1700/1700 [==============================] - 1s 327us/step - loss: 0.0236 - acc: 0.9924 - val_loss: 1.4767 - val_acc: 0.8050\n",
            "Epoch 11/20\n",
            "1700/1700 [==============================] - 1s 325us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 1.5314 - val_acc: 0.8058\n",
            "Epoch 12/20\n",
            "1700/1700 [==============================] - 1s 334us/step - loss: 0.0192 - acc: 0.9918 - val_loss: 1.5739 - val_acc: 0.8056\n",
            "Epoch 13/20\n",
            "1700/1700 [==============================] - 1s 336us/step - loss: 0.0220 - acc: 0.9906 - val_loss: 1.5616 - val_acc: 0.8040\n",
            "Epoch 14/20\n",
            "1700/1700 [==============================] - 1s 339us/step - loss: 0.0153 - acc: 0.9947 - val_loss: 1.5941 - val_acc: 0.8067\n",
            "Epoch 15/20\n",
            "1700/1700 [==============================] - 1s 335us/step - loss: 0.0233 - acc: 0.9918 - val_loss: 1.6334 - val_acc: 0.8048\n",
            "Epoch 16/20\n",
            "1700/1700 [==============================] - 1s 321us/step - loss: 0.0243 - acc: 0.9918 - val_loss: 1.5580 - val_acc: 0.8072\n",
            "Epoch 17/20\n",
            "1700/1700 [==============================] - 1s 337us/step - loss: 0.0150 - acc: 0.9947 - val_loss: 1.6264 - val_acc: 0.8082\n",
            "Epoch 18/20\n",
            "1700/1700 [==============================] - 1s 327us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 1.5978 - val_acc: 0.8063\n",
            "Epoch 19/20\n",
            "1700/1700 [==============================] - 1s 347us/step - loss: 0.0131 - acc: 0.9953 - val_loss: 1.5489 - val_acc: 0.8058\n",
            "Epoch 20/20\n",
            "1700/1700 [==============================] - 1s 334us/step - loss: 0.0162 - acc: 0.9941 - val_loss: 1.6224 - val_acc: 0.8029\n",
            "Test loss: 1.6224326159894467\n",
            "Test accuracy: 0.8029\n",
            "Train loss: 0.00023913551725399885\n",
            "Train accuracy: 1.0\n",
            "x label: (1800, 28, 28)\n",
            "y label: (1800,)\n",
            "x unlabel: (58200, 28, 28)\n",
            "y unlabel: (58200,)\n",
            "17\n",
            "Train on 1800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1800/1800 [==============================] - 1s 335us/step - loss: 0.1120 - acc: 0.9739 - val_loss: 1.3623 - val_acc: 0.8084\n",
            "Epoch 2/20\n",
            "1800/1800 [==============================] - 1s 308us/step - loss: 0.0738 - acc: 0.9800 - val_loss: 1.2852 - val_acc: 0.8072\n",
            "Epoch 3/20\n",
            "1800/1800 [==============================] - 1s 330us/step - loss: 0.0500 - acc: 0.9844 - val_loss: 1.4114 - val_acc: 0.8081\n",
            "Epoch 4/20\n",
            "1800/1800 [==============================] - 1s 326us/step - loss: 0.0548 - acc: 0.9822 - val_loss: 1.3535 - val_acc: 0.8036\n",
            "Epoch 5/20\n",
            "1800/1800 [==============================] - 1s 323us/step - loss: 0.0407 - acc: 0.9878 - val_loss: 1.3613 - val_acc: 0.8063\n",
            "Epoch 6/20\n",
            "1800/1800 [==============================] - 1s 318us/step - loss: 0.0371 - acc: 0.9867 - val_loss: 1.3818 - val_acc: 0.8071\n",
            "Epoch 7/20\n",
            "1800/1800 [==============================] - 1s 329us/step - loss: 0.0201 - acc: 0.9933 - val_loss: 1.4132 - val_acc: 0.8092\n",
            "Epoch 8/20\n",
            "1800/1800 [==============================] - 1s 315us/step - loss: 0.0336 - acc: 0.9894 - val_loss: 1.4226 - val_acc: 0.8106\n",
            "Epoch 9/20\n",
            "1800/1800 [==============================] - 1s 311us/step - loss: 0.0236 - acc: 0.9911 - val_loss: 1.3993 - val_acc: 0.8076\n",
            "Epoch 10/20\n",
            "1800/1800 [==============================] - 1s 314us/step - loss: 0.0204 - acc: 0.9944 - val_loss: 1.4893 - val_acc: 0.8107\n",
            "Epoch 11/20\n",
            "1800/1800 [==============================] - 1s 332us/step - loss: 0.0241 - acc: 0.9950 - val_loss: 1.4701 - val_acc: 0.8084\n",
            "Epoch 12/20\n",
            "1800/1800 [==============================] - 1s 323us/step - loss: 0.0200 - acc: 0.9944 - val_loss: 1.5690 - val_acc: 0.8067\n",
            "Epoch 13/20\n",
            "1800/1800 [==============================] - 1s 310us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 1.5615 - val_acc: 0.8072\n",
            "Epoch 14/20\n",
            "1800/1800 [==============================] - 1s 308us/step - loss: 0.0201 - acc: 0.9944 - val_loss: 1.5322 - val_acc: 0.8063\n",
            "Epoch 15/20\n",
            "1800/1800 [==============================] - 1s 330us/step - loss: 0.0166 - acc: 0.9939 - val_loss: 1.5742 - val_acc: 0.8075\n",
            "Epoch 16/20\n",
            "1800/1800 [==============================] - 1s 320us/step - loss: 0.0150 - acc: 0.9944 - val_loss: 1.5251 - val_acc: 0.8096\n",
            "Epoch 17/20\n",
            "1800/1800 [==============================] - 1s 342us/step - loss: 0.0112 - acc: 0.9967 - val_loss: 1.6309 - val_acc: 0.8075\n",
            "Epoch 18/20\n",
            "1800/1800 [==============================] - 1s 316us/step - loss: 0.0130 - acc: 0.9950 - val_loss: 1.6316 - val_acc: 0.8024\n",
            "Epoch 19/20\n",
            "1800/1800 [==============================] - 1s 327us/step - loss: 0.0099 - acc: 0.9956 - val_loss: 1.6339 - val_acc: 0.8051\n",
            "Epoch 20/20\n",
            "1800/1800 [==============================] - 1s 321us/step - loss: 0.0097 - acc: 0.9989 - val_loss: 1.6636 - val_acc: 0.8040\n",
            "Test loss: 1.663580422103405\n",
            "Test accuracy: 0.804\n",
            "Train loss: 9.662901786416316e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (1900, 28, 28)\n",
            "y label: (1900,)\n",
            "x unlabel: (58100, 28, 28)\n",
            "y unlabel: (58100,)\n",
            "18\n",
            "Train on 1900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "1900/1900 [==============================] - 1s 310us/step - loss: 0.0908 - acc: 0.9721 - val_loss: 1.4153 - val_acc: 0.8002\n",
            "Epoch 2/20\n",
            "1900/1900 [==============================] - 1s 313us/step - loss: 0.0800 - acc: 0.9784 - val_loss: 1.3752 - val_acc: 0.8039\n",
            "Epoch 3/20\n",
            "1900/1900 [==============================] - 1s 312us/step - loss: 0.0564 - acc: 0.9847 - val_loss: 1.3467 - val_acc: 0.8058\n",
            "Epoch 4/20\n",
            "1900/1900 [==============================] - 1s 319us/step - loss: 0.0569 - acc: 0.9832 - val_loss: 1.3621 - val_acc: 0.8062\n",
            "Epoch 5/20\n",
            "1900/1900 [==============================] - 1s 307us/step - loss: 0.0373 - acc: 0.9884 - val_loss: 1.5183 - val_acc: 0.8040\n",
            "Epoch 6/20\n",
            "1900/1900 [==============================] - 1s 299us/step - loss: 0.0304 - acc: 0.9884 - val_loss: 1.5985 - val_acc: 0.8024\n",
            "Epoch 7/20\n",
            "1900/1900 [==============================] - 1s 313us/step - loss: 0.0325 - acc: 0.9895 - val_loss: 1.5845 - val_acc: 0.7994\n",
            "Epoch 8/20\n",
            "1900/1900 [==============================] - 1s 314us/step - loss: 0.0273 - acc: 0.9916 - val_loss: 1.4771 - val_acc: 0.8045\n",
            "Epoch 9/20\n",
            "1900/1900 [==============================] - 1s 315us/step - loss: 0.0221 - acc: 0.9905 - val_loss: 1.5262 - val_acc: 0.8083\n",
            "Epoch 10/20\n",
            "1900/1900 [==============================] - 1s 300us/step - loss: 0.0259 - acc: 0.9932 - val_loss: 1.5307 - val_acc: 0.8097\n",
            "Epoch 11/20\n",
            "1900/1900 [==============================] - 1s 301us/step - loss: 0.0191 - acc: 0.9926 - val_loss: 1.5972 - val_acc: 0.8031\n",
            "Epoch 12/20\n",
            "1900/1900 [==============================] - 1s 305us/step - loss: 0.0203 - acc: 0.9932 - val_loss: 1.5834 - val_acc: 0.8084\n",
            "Epoch 13/20\n",
            "1900/1900 [==============================] - 1s 310us/step - loss: 0.0184 - acc: 0.9937 - val_loss: 1.5996 - val_acc: 0.8058\n",
            "Epoch 14/20\n",
            "1900/1900 [==============================] - 1s 299us/step - loss: 0.0195 - acc: 0.9926 - val_loss: 1.5000 - val_acc: 0.8102\n",
            "Epoch 15/20\n",
            "1900/1900 [==============================] - 1s 313us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 1.6028 - val_acc: 0.8057\n",
            "Epoch 16/20\n",
            "1900/1900 [==============================] - 1s 301us/step - loss: 0.0166 - acc: 0.9937 - val_loss: 1.6406 - val_acc: 0.8074\n",
            "Epoch 17/20\n",
            "1900/1900 [==============================] - 1s 311us/step - loss: 0.0122 - acc: 0.9963 - val_loss: 1.6091 - val_acc: 0.8093\n",
            "Epoch 18/20\n",
            "1900/1900 [==============================] - 1s 309us/step - loss: 0.0239 - acc: 0.9905 - val_loss: 1.5916 - val_acc: 0.8069\n",
            "Epoch 19/20\n",
            "1900/1900 [==============================] - 1s 310us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 1.5715 - val_acc: 0.8079\n",
            "Epoch 20/20\n",
            "1900/1900 [==============================] - 1s 318us/step - loss: 0.0147 - acc: 0.9958 - val_loss: 1.5539 - val_acc: 0.8048\n",
            "Test loss: 1.5539282278299331\n",
            "Test accuracy: 0.8048\n",
            "Train loss: 0.00032386409835716653\n",
            "Train accuracy: 1.0\n",
            "x label: (2000, 28, 28)\n",
            "y label: (2000,)\n",
            "x unlabel: (58000, 28, 28)\n",
            "y unlabel: (58000,)\n",
            "19\n",
            "Train on 2000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2000/2000 [==============================] - 1s 297us/step - loss: 0.1086 - acc: 0.9710 - val_loss: 1.2686 - val_acc: 0.8083\n",
            "Epoch 2/20\n",
            "2000/2000 [==============================] - 1s 306us/step - loss: 0.0629 - acc: 0.9815 - val_loss: 1.2979 - val_acc: 0.8134\n",
            "Epoch 3/20\n",
            "2000/2000 [==============================] - 1s 300us/step - loss: 0.0509 - acc: 0.9845 - val_loss: 1.3835 - val_acc: 0.8056\n",
            "Epoch 4/20\n",
            "2000/2000 [==============================] - 1s 289us/step - loss: 0.0439 - acc: 0.9890 - val_loss: 1.4501 - val_acc: 0.8109\n",
            "Epoch 5/20\n",
            "2000/2000 [==============================] - 1s 306us/step - loss: 0.0402 - acc: 0.9875 - val_loss: 1.4251 - val_acc: 0.8138\n",
            "Epoch 6/20\n",
            "2000/2000 [==============================] - 1s 278us/step - loss: 0.0356 - acc: 0.9845 - val_loss: 1.3496 - val_acc: 0.8160\n",
            "Epoch 7/20\n",
            "2000/2000 [==============================] - 1s 301us/step - loss: 0.0228 - acc: 0.9930 - val_loss: 1.4649 - val_acc: 0.8125\n",
            "Epoch 8/20\n",
            "2000/2000 [==============================] - 1s 289us/step - loss: 0.0232 - acc: 0.9925 - val_loss: 1.5770 - val_acc: 0.8113\n",
            "Epoch 9/20\n",
            "2000/2000 [==============================] - 1s 305us/step - loss: 0.0184 - acc: 0.9935 - val_loss: 1.5030 - val_acc: 0.8087\n",
            "Epoch 10/20\n",
            "2000/2000 [==============================] - 1s 294us/step - loss: 0.0267 - acc: 0.9925 - val_loss: 1.4650 - val_acc: 0.8124\n",
            "Epoch 11/20\n",
            "2000/2000 [==============================] - 1s 302us/step - loss: 0.0178 - acc: 0.9925 - val_loss: 1.4858 - val_acc: 0.8123\n",
            "Epoch 12/20\n",
            "2000/2000 [==============================] - 1s 304us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 1.4837 - val_acc: 0.8106\n",
            "Epoch 13/20\n",
            "2000/2000 [==============================] - 1s 303us/step - loss: 0.0206 - acc: 0.9930 - val_loss: 1.5449 - val_acc: 0.8133\n",
            "Epoch 14/20\n",
            "2000/2000 [==============================] - 1s 308us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 1.5336 - val_acc: 0.8120\n",
            "Epoch 15/20\n",
            "2000/2000 [==============================] - 1s 306us/step - loss: 0.0210 - acc: 0.9915 - val_loss: 1.5794 - val_acc: 0.8078\n",
            "Epoch 16/20\n",
            "2000/2000 [==============================] - 1s 290us/step - loss: 0.0185 - acc: 0.9950 - val_loss: 1.5247 - val_acc: 0.8144\n",
            "Epoch 17/20\n",
            "2000/2000 [==============================] - 1s 308us/step - loss: 0.0166 - acc: 0.9960 - val_loss: 1.6951 - val_acc: 0.8100\n",
            "Epoch 18/20\n",
            "2000/2000 [==============================] - 1s 296us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 1.6245 - val_acc: 0.8092\n",
            "Epoch 19/20\n",
            "2000/2000 [==============================] - 1s 298us/step - loss: 0.0213 - acc: 0.9920 - val_loss: 1.5619 - val_acc: 0.8122\n",
            "Epoch 20/20\n",
            "2000/2000 [==============================] - 1s 303us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 1.6032 - val_acc: 0.8123\n",
            "Test loss: 1.6032092633739115\n",
            "Test accuracy: 0.8123\n",
            "Train loss: 0.0001050838294750065\n",
            "Train accuracy: 1.0\n",
            "x label: (2100, 28, 28)\n",
            "y label: (2100,)\n",
            "x unlabel: (57900, 28, 28)\n",
            "y unlabel: (57900,)\n",
            "20\n",
            "Train on 2100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2100/2100 [==============================] - 1s 299us/step - loss: 0.0986 - acc: 0.9781 - val_loss: 1.4542 - val_acc: 0.8150\n",
            "Epoch 2/20\n",
            "2100/2100 [==============================] - 1s 291us/step - loss: 0.0767 - acc: 0.9781 - val_loss: 1.4078 - val_acc: 0.8156\n",
            "Epoch 3/20\n",
            "2100/2100 [==============================] - 1s 294us/step - loss: 0.0529 - acc: 0.9857 - val_loss: 1.3548 - val_acc: 0.8179\n",
            "Epoch 4/20\n",
            "2100/2100 [==============================] - 1s 298us/step - loss: 0.0513 - acc: 0.9876 - val_loss: 1.4833 - val_acc: 0.8127\n",
            "Epoch 5/20\n",
            "2100/2100 [==============================] - 1s 300us/step - loss: 0.0389 - acc: 0.9876 - val_loss: 1.5087 - val_acc: 0.8105\n",
            "Epoch 6/20\n",
            "2100/2100 [==============================] - 1s 287us/step - loss: 0.0461 - acc: 0.9857 - val_loss: 1.4344 - val_acc: 0.8119\n",
            "Epoch 7/20\n",
            "2100/2100 [==============================] - 1s 303us/step - loss: 0.0282 - acc: 0.9933 - val_loss: 1.4505 - val_acc: 0.8165\n",
            "Epoch 8/20\n",
            "2100/2100 [==============================] - 1s 283us/step - loss: 0.0437 - acc: 0.9876 - val_loss: 1.4827 - val_acc: 0.8123\n",
            "Epoch 9/20\n",
            "2100/2100 [==============================] - 1s 290us/step - loss: 0.0343 - acc: 0.9895 - val_loss: 1.5024 - val_acc: 0.8169\n",
            "Epoch 10/20\n",
            "2100/2100 [==============================] - 1s 282us/step - loss: 0.0292 - acc: 0.9919 - val_loss: 1.4842 - val_acc: 0.8169\n",
            "Epoch 11/20\n",
            "2100/2100 [==============================] - 1s 285us/step - loss: 0.0416 - acc: 0.9905 - val_loss: 1.5009 - val_acc: 0.8158\n",
            "Epoch 12/20\n",
            "2100/2100 [==============================] - 1s 292us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 1.5431 - val_acc: 0.8148\n",
            "Epoch 13/20\n",
            "2100/2100 [==============================] - 1s 272us/step - loss: 0.0250 - acc: 0.9933 - val_loss: 1.5020 - val_acc: 0.8131\n",
            "Epoch 14/20\n",
            "2100/2100 [==============================] - 1s 290us/step - loss: 0.0274 - acc: 0.9948 - val_loss: 1.5050 - val_acc: 0.8146\n",
            "Epoch 15/20\n",
            "2100/2100 [==============================] - 1s 277us/step - loss: 0.0198 - acc: 0.9957 - val_loss: 1.5540 - val_acc: 0.8150\n",
            "Epoch 16/20\n",
            "2100/2100 [==============================] - 1s 286us/step - loss: 0.0232 - acc: 0.9948 - val_loss: 1.4932 - val_acc: 0.8180\n",
            "Epoch 17/20\n",
            "2100/2100 [==============================] - 1s 291us/step - loss: 0.0248 - acc: 0.9929 - val_loss: 1.5882 - val_acc: 0.8109\n",
            "Epoch 18/20\n",
            "2100/2100 [==============================] - 1s 279us/step - loss: 0.0275 - acc: 0.9919 - val_loss: 1.4767 - val_acc: 0.8189\n",
            "Epoch 19/20\n",
            "2100/2100 [==============================] - 1s 288us/step - loss: 0.0216 - acc: 0.9929 - val_loss: 1.5461 - val_acc: 0.8154\n",
            "Epoch 20/20\n",
            "2100/2100 [==============================] - 1s 293us/step - loss: 0.0289 - acc: 0.9924 - val_loss: 1.4688 - val_acc: 0.8200\n",
            "Test loss: 1.4687701153442263\n",
            "Test accuracy: 0.82\n",
            "Train loss: 0.007973286150951416\n",
            "Train accuracy: 0.9995238095238095\n",
            "x label: (2200, 28, 28)\n",
            "y label: (2200,)\n",
            "x unlabel: (57800, 28, 28)\n",
            "y unlabel: (57800,)\n",
            "21\n",
            "Train on 2200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2200/2200 [==============================] - 1s 287us/step - loss: 0.1056 - acc: 0.9736 - val_loss: 1.3232 - val_acc: 0.8160\n",
            "Epoch 2/20\n",
            "2200/2200 [==============================] - 1s 278us/step - loss: 0.0747 - acc: 0.9791 - val_loss: 1.4356 - val_acc: 0.8150\n",
            "Epoch 3/20\n",
            "2200/2200 [==============================] - 1s 275us/step - loss: 0.0529 - acc: 0.9886 - val_loss: 1.4031 - val_acc: 0.8185\n",
            "Epoch 4/20\n",
            "2200/2200 [==============================] - 1s 280us/step - loss: 0.0457 - acc: 0.9886 - val_loss: 1.4021 - val_acc: 0.8205\n",
            "Epoch 5/20\n",
            "2200/2200 [==============================] - 1s 268us/step - loss: 0.0423 - acc: 0.9891 - val_loss: 1.3744 - val_acc: 0.8198\n",
            "Epoch 6/20\n",
            "2200/2200 [==============================] - 1s 287us/step - loss: 0.0418 - acc: 0.9868 - val_loss: 1.3805 - val_acc: 0.8197\n",
            "Epoch 7/20\n",
            "2200/2200 [==============================] - 1s 280us/step - loss: 0.0388 - acc: 0.9895 - val_loss: 1.5637 - val_acc: 0.8090\n",
            "Epoch 8/20\n",
            "2200/2200 [==============================] - 1s 289us/step - loss: 0.0303 - acc: 0.9909 - val_loss: 1.4524 - val_acc: 0.8207\n",
            "Epoch 9/20\n",
            "2200/2200 [==============================] - 1s 289us/step - loss: 0.0312 - acc: 0.9914 - val_loss: 1.5196 - val_acc: 0.8172\n",
            "Epoch 10/20\n",
            "2200/2200 [==============================] - 1s 279us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 1.4883 - val_acc: 0.8206\n",
            "Epoch 11/20\n",
            "2200/2200 [==============================] - 1s 293us/step - loss: 0.0305 - acc: 0.9909 - val_loss: 1.4832 - val_acc: 0.8157\n",
            "Epoch 12/20\n",
            "2200/2200 [==============================] - 1s 286us/step - loss: 0.0353 - acc: 0.9923 - val_loss: 1.4094 - val_acc: 0.8207\n",
            "Epoch 13/20\n",
            "2200/2200 [==============================] - 1s 277us/step - loss: 0.0205 - acc: 0.9955 - val_loss: 1.4793 - val_acc: 0.8223\n",
            "Epoch 14/20\n",
            "2200/2200 [==============================] - 1s 286us/step - loss: 0.0261 - acc: 0.9936 - val_loss: 1.4663 - val_acc: 0.8195\n",
            "Epoch 15/20\n",
            "2200/2200 [==============================] - 1s 266us/step - loss: 0.0183 - acc: 0.9955 - val_loss: 1.5604 - val_acc: 0.8198\n",
            "Epoch 16/20\n",
            "2200/2200 [==============================] - 1s 280us/step - loss: 0.0287 - acc: 0.9936 - val_loss: 1.5513 - val_acc: 0.8113\n",
            "Epoch 17/20\n",
            "2200/2200 [==============================] - 1s 290us/step - loss: 0.0230 - acc: 0.9950 - val_loss: 1.5767 - val_acc: 0.8160\n",
            "Epoch 18/20\n",
            "2200/2200 [==============================] - 1s 274us/step - loss: 0.0197 - acc: 0.9955 - val_loss: 1.5707 - val_acc: 0.8149\n",
            "Epoch 19/20\n",
            "2200/2200 [==============================] - 1s 281us/step - loss: 0.0189 - acc: 0.9950 - val_loss: 1.6678 - val_acc: 0.8164\n",
            "Epoch 20/20\n",
            "2200/2200 [==============================] - 1s 272us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 1.5416 - val_acc: 0.8209\n",
            "Test loss: 1.541603112745285\n",
            "Test accuracy: 0.8209\n",
            "Train loss: 0.007780719131912975\n",
            "Train accuracy: 0.9995454545454545\n",
            "x label: (2300, 28, 28)\n",
            "y label: (2300,)\n",
            "x unlabel: (57700, 28, 28)\n",
            "y unlabel: (57700,)\n",
            "22\n",
            "Train on 2300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2300/2300 [==============================] - 1s 275us/step - loss: 0.1157 - acc: 0.9743 - val_loss: 1.3428 - val_acc: 0.8175\n",
            "Epoch 2/20\n",
            "2300/2300 [==============================] - 1s 280us/step - loss: 0.0739 - acc: 0.9817 - val_loss: 1.3356 - val_acc: 0.8196\n",
            "Epoch 3/20\n",
            "2300/2300 [==============================] - 1s 291us/step - loss: 0.0603 - acc: 0.9835 - val_loss: 1.2568 - val_acc: 0.8210\n",
            "Epoch 4/20\n",
            "2300/2300 [==============================] - 1s 278us/step - loss: 0.0543 - acc: 0.9887 - val_loss: 1.3886 - val_acc: 0.8145\n",
            "Epoch 5/20\n",
            "2300/2300 [==============================] - 1s 277us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 1.3639 - val_acc: 0.8253\n",
            "Epoch 6/20\n",
            "2300/2300 [==============================] - 1s 284us/step - loss: 0.0392 - acc: 0.9896 - val_loss: 1.4620 - val_acc: 0.8201\n",
            "Epoch 7/20\n",
            "2300/2300 [==============================] - 1s 277us/step - loss: 0.0345 - acc: 0.9896 - val_loss: 1.4824 - val_acc: 0.8181\n",
            "Epoch 8/20\n",
            "2300/2300 [==============================] - 1s 268us/step - loss: 0.0433 - acc: 0.9865 - val_loss: 1.4244 - val_acc: 0.8176\n",
            "Epoch 9/20\n",
            "2300/2300 [==============================] - 1s 270us/step - loss: 0.0377 - acc: 0.9861 - val_loss: 1.4704 - val_acc: 0.8221\n",
            "Epoch 10/20\n",
            "2300/2300 [==============================] - 1s 272us/step - loss: 0.0276 - acc: 0.9917 - val_loss: 1.4185 - val_acc: 0.8218\n",
            "Epoch 11/20\n",
            "2300/2300 [==============================] - 1s 286us/step - loss: 0.0218 - acc: 0.9948 - val_loss: 1.4242 - val_acc: 0.8243\n",
            "Epoch 12/20\n",
            "2300/2300 [==============================] - 1s 267us/step - loss: 0.0475 - acc: 0.9852 - val_loss: 1.3563 - val_acc: 0.8215\n",
            "Epoch 13/20\n",
            "2300/2300 [==============================] - 1s 278us/step - loss: 0.0274 - acc: 0.9922 - val_loss: 1.4788 - val_acc: 0.8226\n",
            "Epoch 14/20\n",
            "2300/2300 [==============================] - 1s 265us/step - loss: 0.0251 - acc: 0.9952 - val_loss: 1.4624 - val_acc: 0.8227\n",
            "Epoch 15/20\n",
            "2300/2300 [==============================] - 1s 264us/step - loss: 0.0296 - acc: 0.9913 - val_loss: 1.4490 - val_acc: 0.8234\n",
            "Epoch 16/20\n",
            "2300/2300 [==============================] - 1s 260us/step - loss: 0.0240 - acc: 0.9935 - val_loss: 1.5297 - val_acc: 0.8214\n",
            "Epoch 17/20\n",
            "2300/2300 [==============================] - 1s 268us/step - loss: 0.0253 - acc: 0.9922 - val_loss: 1.4172 - val_acc: 0.8185\n",
            "Epoch 18/20\n",
            "2300/2300 [==============================] - 1s 280us/step - loss: 0.0236 - acc: 0.9935 - val_loss: 1.3707 - val_acc: 0.8244\n",
            "Epoch 19/20\n",
            "2300/2300 [==============================] - 1s 277us/step - loss: 0.0253 - acc: 0.9948 - val_loss: 1.4399 - val_acc: 0.8210\n",
            "Epoch 20/20\n",
            "2300/2300 [==============================] - 1s 271us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 1.5230 - val_acc: 0.8219\n",
            "Test loss: 1.5230393416404724\n",
            "Test accuracy: 0.8219\n",
            "Train loss: 0.004984475802218937\n",
            "Train accuracy: 0.9995652173913043\n",
            "x label: (2400, 28, 28)\n",
            "y label: (2400,)\n",
            "x unlabel: (57600, 28, 28)\n",
            "y unlabel: (57600,)\n",
            "23\n",
            "Train on 2400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2400/2400 [==============================] - 1s 264us/step - loss: 0.1011 - acc: 0.9750 - val_loss: 1.3961 - val_acc: 0.8189\n",
            "Epoch 2/20\n",
            "2400/2400 [==============================] - 1s 272us/step - loss: 0.0796 - acc: 0.9821 - val_loss: 1.3750 - val_acc: 0.8153\n",
            "Epoch 3/20\n",
            "2400/2400 [==============================] - 1s 267us/step - loss: 0.0657 - acc: 0.9821 - val_loss: 1.2625 - val_acc: 0.8215\n",
            "Epoch 4/20\n",
            "2400/2400 [==============================] - 1s 250us/step - loss: 0.0590 - acc: 0.9867 - val_loss: 1.3534 - val_acc: 0.8203\n",
            "Epoch 5/20\n",
            "2400/2400 [==============================] - 1s 264us/step - loss: 0.0461 - acc: 0.9904 - val_loss: 1.3284 - val_acc: 0.8221\n",
            "Epoch 6/20\n",
            "2400/2400 [==============================] - 1s 258us/step - loss: 0.0464 - acc: 0.9858 - val_loss: 1.3337 - val_acc: 0.8196\n",
            "Epoch 7/20\n",
            "2400/2400 [==============================] - 1s 263us/step - loss: 0.0235 - acc: 0.9921 - val_loss: 1.4752 - val_acc: 0.8200\n",
            "Epoch 8/20\n",
            "2400/2400 [==============================] - 1s 264us/step - loss: 0.0424 - acc: 0.9904 - val_loss: 1.3391 - val_acc: 0.8213\n",
            "Epoch 9/20\n",
            "2400/2400 [==============================] - 1s 262us/step - loss: 0.0335 - acc: 0.9933 - val_loss: 1.4414 - val_acc: 0.8208\n",
            "Epoch 10/20\n",
            "2400/2400 [==============================] - 1s 278us/step - loss: 0.0247 - acc: 0.9921 - val_loss: 1.3565 - val_acc: 0.8263\n",
            "Epoch 11/20\n",
            "2400/2400 [==============================] - 1s 256us/step - loss: 0.0248 - acc: 0.9921 - val_loss: 1.4181 - val_acc: 0.8234\n",
            "Epoch 12/20\n",
            "2400/2400 [==============================] - 1s 267us/step - loss: 0.0238 - acc: 0.9925 - val_loss: 1.4504 - val_acc: 0.8236\n",
            "Epoch 13/20\n",
            "2400/2400 [==============================] - 1s 267us/step - loss: 0.0220 - acc: 0.9921 - val_loss: 1.3865 - val_acc: 0.8256\n",
            "Epoch 14/20\n",
            "2400/2400 [==============================] - 1s 272us/step - loss: 0.0151 - acc: 0.9950 - val_loss: 1.4146 - val_acc: 0.8233\n",
            "Epoch 15/20\n",
            "2400/2400 [==============================] - 1s 266us/step - loss: 0.0205 - acc: 0.9917 - val_loss: 1.4051 - val_acc: 0.8243\n",
            "Epoch 16/20\n",
            "2400/2400 [==============================] - 1s 277us/step - loss: 0.0189 - acc: 0.9942 - val_loss: 1.4465 - val_acc: 0.8223\n",
            "Epoch 17/20\n",
            "2400/2400 [==============================] - 1s 271us/step - loss: 0.0198 - acc: 0.9942 - val_loss: 1.4429 - val_acc: 0.8262\n",
            "Epoch 18/20\n",
            "2400/2400 [==============================] - 1s 280us/step - loss: 0.0196 - acc: 0.9933 - val_loss: 1.5006 - val_acc: 0.8270\n",
            "Epoch 19/20\n",
            "2400/2400 [==============================] - 1s 274us/step - loss: 0.0137 - acc: 0.9954 - val_loss: 1.4937 - val_acc: 0.8265\n",
            "Epoch 20/20\n",
            "2400/2400 [==============================] - 1s 268us/step - loss: 0.0151 - acc: 0.9942 - val_loss: 1.5034 - val_acc: 0.8230\n",
            "Test loss: 1.503433164691925\n",
            "Test accuracy: 0.823\n",
            "Train loss: 0.00034933166480300315\n",
            "Train accuracy: 1.0\n",
            "x label: (2500, 28, 28)\n",
            "y label: (2500,)\n",
            "x unlabel: (57500, 28, 28)\n",
            "y unlabel: (57500,)\n",
            "24\n",
            "Train on 2500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2500/2500 [==============================] - 1s 269us/step - loss: 0.0968 - acc: 0.9768 - val_loss: 1.3106 - val_acc: 0.8218\n",
            "Epoch 2/20\n",
            "2500/2500 [==============================] - 1s 266us/step - loss: 0.0629 - acc: 0.9824 - val_loss: 1.2377 - val_acc: 0.8280\n",
            "Epoch 3/20\n",
            "2500/2500 [==============================] - 1s 266us/step - loss: 0.0542 - acc: 0.9840 - val_loss: 1.3806 - val_acc: 0.8242\n",
            "Epoch 4/20\n",
            "2500/2500 [==============================] - 1s 271us/step - loss: 0.0439 - acc: 0.9856 - val_loss: 1.3576 - val_acc: 0.8227\n",
            "Epoch 5/20\n",
            "2500/2500 [==============================] - 1s 270us/step - loss: 0.0386 - acc: 0.9872 - val_loss: 1.3246 - val_acc: 0.8249\n",
            "Epoch 6/20\n",
            "2500/2500 [==============================] - 1s 268us/step - loss: 0.0346 - acc: 0.9904 - val_loss: 1.2815 - val_acc: 0.8253\n",
            "Epoch 7/20\n",
            "2500/2500 [==============================] - 1s 268us/step - loss: 0.0400 - acc: 0.9872 - val_loss: 1.3601 - val_acc: 0.8258\n",
            "Epoch 8/20\n",
            "2500/2500 [==============================] - 1s 263us/step - loss: 0.0241 - acc: 0.9944 - val_loss: 1.4916 - val_acc: 0.8222\n",
            "Epoch 9/20\n",
            "2500/2500 [==============================] - 1s 255us/step - loss: 0.0206 - acc: 0.9912 - val_loss: 1.5180 - val_acc: 0.8198\n",
            "Epoch 10/20\n",
            "2500/2500 [==============================] - 1s 281us/step - loss: 0.0197 - acc: 0.9936 - val_loss: 1.4800 - val_acc: 0.8267\n",
            "Epoch 11/20\n",
            "2500/2500 [==============================] - 1s 258us/step - loss: 0.0280 - acc: 0.9912 - val_loss: 1.3950 - val_acc: 0.8257\n",
            "Epoch 12/20\n",
            "2500/2500 [==============================] - 1s 270us/step - loss: 0.0345 - acc: 0.9908 - val_loss: 1.3926 - val_acc: 0.8265\n",
            "Epoch 13/20\n",
            "2500/2500 [==============================] - 1s 273us/step - loss: 0.0149 - acc: 0.9964 - val_loss: 1.4994 - val_acc: 0.8218\n",
            "Epoch 14/20\n",
            "2500/2500 [==============================] - 1s 261us/step - loss: 0.0180 - acc: 0.9940 - val_loss: 1.4544 - val_acc: 0.8280\n",
            "Epoch 15/20\n",
            "2500/2500 [==============================] - 1s 273us/step - loss: 0.0290 - acc: 0.9900 - val_loss: 1.4879 - val_acc: 0.8230\n",
            "Epoch 16/20\n",
            "2500/2500 [==============================] - 1s 273us/step - loss: 0.0206 - acc: 0.9936 - val_loss: 1.5076 - val_acc: 0.8238\n",
            "Epoch 17/20\n",
            "2500/2500 [==============================] - 1s 264us/step - loss: 0.0136 - acc: 0.9944 - val_loss: 1.5200 - val_acc: 0.8258\n",
            "Epoch 18/20\n",
            "2500/2500 [==============================] - 1s 255us/step - loss: 0.0162 - acc: 0.9940 - val_loss: 1.5213 - val_acc: 0.8258\n",
            "Epoch 19/20\n",
            "2500/2500 [==============================] - 1s 251us/step - loss: 0.0148 - acc: 0.9944 - val_loss: 1.5385 - val_acc: 0.8228\n",
            "Epoch 20/20\n",
            "2500/2500 [==============================] - 1s 251us/step - loss: 0.0142 - acc: 0.9948 - val_loss: 1.4733 - val_acc: 0.8258\n",
            "Test loss: 1.4733090158939361\n",
            "Test accuracy: 0.8258\n",
            "Train loss: 0.000339535891205378\n",
            "Train accuracy: 1.0\n",
            "x label: (2600, 28, 28)\n",
            "y label: (2600,)\n",
            "x unlabel: (57400, 28, 28)\n",
            "y unlabel: (57400,)\n",
            "25\n",
            "Train on 2600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2600/2600 [==============================] - 1s 260us/step - loss: 0.0702 - acc: 0.9808 - val_loss: 1.3440 - val_acc: 0.8265\n",
            "Epoch 2/20\n",
            "2600/2600 [==============================] - 1s 258us/step - loss: 0.0575 - acc: 0.9846 - val_loss: 1.4308 - val_acc: 0.8247\n",
            "Epoch 3/20\n",
            "2600/2600 [==============================] - 1s 247us/step - loss: 0.0501 - acc: 0.9854 - val_loss: 1.3470 - val_acc: 0.8264\n",
            "Epoch 4/20\n",
            "2600/2600 [==============================] - 1s 258us/step - loss: 0.0257 - acc: 0.9927 - val_loss: 1.3511 - val_acc: 0.8269\n",
            "Epoch 5/20\n",
            "2600/2600 [==============================] - 1s 259us/step - loss: 0.0343 - acc: 0.9904 - val_loss: 1.3808 - val_acc: 0.8289\n",
            "Epoch 6/20\n",
            "2600/2600 [==============================] - 1s 260us/step - loss: 0.0346 - acc: 0.9892 - val_loss: 1.3224 - val_acc: 0.8223\n",
            "Epoch 7/20\n",
            "2600/2600 [==============================] - 1s 265us/step - loss: 0.0375 - acc: 0.9865 - val_loss: 1.3749 - val_acc: 0.8255\n",
            "Epoch 8/20\n",
            "2600/2600 [==============================] - 1s 250us/step - loss: 0.0369 - acc: 0.9862 - val_loss: 1.4024 - val_acc: 0.8223\n",
            "Epoch 9/20\n",
            "2600/2600 [==============================] - 1s 251us/step - loss: 0.0267 - acc: 0.9904 - val_loss: 1.3768 - val_acc: 0.8277\n",
            "Epoch 10/20\n",
            "2600/2600 [==============================] - 1s 263us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 1.4759 - val_acc: 0.8210\n",
            "Epoch 11/20\n",
            "2600/2600 [==============================] - 1s 249us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 1.4780 - val_acc: 0.8262\n",
            "Epoch 12/20\n",
            "2600/2600 [==============================] - 1s 255us/step - loss: 0.0126 - acc: 0.9977 - val_loss: 1.4817 - val_acc: 0.8270\n",
            "Epoch 13/20\n",
            "2600/2600 [==============================] - 1s 256us/step - loss: 0.0224 - acc: 0.9931 - val_loss: 1.4472 - val_acc: 0.8275\n",
            "Epoch 14/20\n",
            "2600/2600 [==============================] - 1s 258us/step - loss: 0.0113 - acc: 0.9973 - val_loss: 1.5613 - val_acc: 0.8270\n",
            "Epoch 15/20\n",
            "2600/2600 [==============================] - 1s 271us/step - loss: 0.0137 - acc: 0.9946 - val_loss: 1.5448 - val_acc: 0.8225\n",
            "Epoch 16/20\n",
            "2600/2600 [==============================] - 1s 253us/step - loss: 0.0186 - acc: 0.9938 - val_loss: 1.5032 - val_acc: 0.8259\n",
            "Epoch 17/20\n",
            "2600/2600 [==============================] - 1s 250us/step - loss: 0.0206 - acc: 0.9938 - val_loss: 1.5015 - val_acc: 0.8274\n",
            "Epoch 18/20\n",
            "2600/2600 [==============================] - 1s 250us/step - loss: 0.0157 - acc: 0.9958 - val_loss: 1.4476 - val_acc: 0.8298\n",
            "Epoch 19/20\n",
            "2600/2600 [==============================] - 1s 261us/step - loss: 0.0140 - acc: 0.9946 - val_loss: 1.5333 - val_acc: 0.8263\n",
            "Epoch 20/20\n",
            "2600/2600 [==============================] - 1s 249us/step - loss: 0.0189 - acc: 0.9950 - val_loss: 1.4582 - val_acc: 0.8271\n",
            "Test loss: 1.4582163592889905\n",
            "Test accuracy: 0.8271\n",
            "Train loss: 0.00021067719438895859\n",
            "Train accuracy: 1.0\n",
            "x label: (2700, 28, 28)\n",
            "y label: (2700,)\n",
            "x unlabel: (57300, 28, 28)\n",
            "y unlabel: (57300,)\n",
            "26\n",
            "Train on 2700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2700/2700 [==============================] - 1s 246us/step - loss: 0.0858 - acc: 0.9793 - val_loss: 1.3181 - val_acc: 0.8257\n",
            "Epoch 2/20\n",
            "2700/2700 [==============================] - 1s 257us/step - loss: 0.0670 - acc: 0.9822 - val_loss: 1.3311 - val_acc: 0.8241\n",
            "Epoch 3/20\n",
            "2700/2700 [==============================] - 1s 248us/step - loss: 0.0508 - acc: 0.9852 - val_loss: 1.3141 - val_acc: 0.8248\n",
            "Epoch 4/20\n",
            "2700/2700 [==============================] - 1s 253us/step - loss: 0.0445 - acc: 0.9856 - val_loss: 1.3774 - val_acc: 0.8248\n",
            "Epoch 5/20\n",
            "2700/2700 [==============================] - 1s 255us/step - loss: 0.0291 - acc: 0.9922 - val_loss: 1.3806 - val_acc: 0.8284\n",
            "Epoch 6/20\n",
            "2700/2700 [==============================] - 1s 248us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 1.4047 - val_acc: 0.8288\n",
            "Epoch 7/20\n",
            "2700/2700 [==============================] - 1s 251us/step - loss: 0.0360 - acc: 0.9863 - val_loss: 1.4015 - val_acc: 0.8265\n",
            "Epoch 8/20\n",
            "2700/2700 [==============================] - 1s 258us/step - loss: 0.0336 - acc: 0.9907 - val_loss: 1.4046 - val_acc: 0.8263\n",
            "Epoch 9/20\n",
            "2700/2700 [==============================] - 1s 249us/step - loss: 0.0281 - acc: 0.9911 - val_loss: 1.4497 - val_acc: 0.8254\n",
            "Epoch 10/20\n",
            "2700/2700 [==============================] - 1s 256us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 1.4381 - val_acc: 0.8220\n",
            "Epoch 11/20\n",
            "2700/2700 [==============================] - 1s 258us/step - loss: 0.0204 - acc: 0.9941 - val_loss: 1.4664 - val_acc: 0.8235\n",
            "Epoch 12/20\n",
            "2700/2700 [==============================] - 1s 240us/step - loss: 0.0237 - acc: 0.9944 - val_loss: 1.4381 - val_acc: 0.8254\n",
            "Epoch 13/20\n",
            "2700/2700 [==============================] - 1s 251us/step - loss: 0.0161 - acc: 0.9941 - val_loss: 1.4822 - val_acc: 0.8246\n",
            "Epoch 14/20\n",
            "2700/2700 [==============================] - 1s 250us/step - loss: 0.0202 - acc: 0.9922 - val_loss: 1.4203 - val_acc: 0.8283\n",
            "Epoch 15/20\n",
            "2700/2700 [==============================] - 1s 245us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 1.4673 - val_acc: 0.8272\n",
            "Epoch 16/20\n",
            "2700/2700 [==============================] - 1s 249us/step - loss: 0.0107 - acc: 0.9963 - val_loss: 1.5188 - val_acc: 0.8282\n",
            "Epoch 17/20\n",
            "2700/2700 [==============================] - 1s 244us/step - loss: 0.0105 - acc: 0.9967 - val_loss: 1.5106 - val_acc: 0.8299\n",
            "Epoch 18/20\n",
            "2700/2700 [==============================] - 1s 243us/step - loss: 0.0154 - acc: 0.9956 - val_loss: 1.4791 - val_acc: 0.8268\n",
            "Epoch 19/20\n",
            "2700/2700 [==============================] - 1s 255us/step - loss: 0.0184 - acc: 0.9941 - val_loss: 1.5195 - val_acc: 0.8259\n",
            "Epoch 20/20\n",
            "2700/2700 [==============================] - 1s 253us/step - loss: 0.0097 - acc: 0.9967 - val_loss: 1.5375 - val_acc: 0.8244\n",
            "Test loss: 1.5374747329771519\n",
            "Test accuracy: 0.8244\n",
            "Train loss: 0.00015496566322393914\n",
            "Train accuracy: 1.0\n",
            "x label: (2800, 28, 28)\n",
            "y label: (2800,)\n",
            "x unlabel: (57200, 28, 28)\n",
            "y unlabel: (57200,)\n",
            "27\n",
            "Train on 2800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2800/2800 [==============================] - 1s 250us/step - loss: 0.0928 - acc: 0.9796 - val_loss: 1.3014 - val_acc: 0.8290\n",
            "Epoch 2/20\n",
            "2800/2800 [==============================] - 1s 241us/step - loss: 0.0732 - acc: 0.9786 - val_loss: 1.3168 - val_acc: 0.8223\n",
            "Epoch 3/20\n",
            "2800/2800 [==============================] - 1s 247us/step - loss: 0.0606 - acc: 0.9843 - val_loss: 1.3425 - val_acc: 0.8238\n",
            "Epoch 4/20\n",
            "2800/2800 [==============================] - 1s 247us/step - loss: 0.0296 - acc: 0.9904 - val_loss: 1.3429 - val_acc: 0.8287\n",
            "Epoch 5/20\n",
            "2800/2800 [==============================] - 1s 253us/step - loss: 0.0330 - acc: 0.9886 - val_loss: 1.4164 - val_acc: 0.8255\n",
            "Epoch 6/20\n",
            "2800/2800 [==============================] - 1s 245us/step - loss: 0.0434 - acc: 0.9875 - val_loss: 1.3304 - val_acc: 0.8242\n",
            "Epoch 7/20\n",
            "2800/2800 [==============================] - 1s 240us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 1.3292 - val_acc: 0.8270\n",
            "Epoch 8/20\n",
            "2800/2800 [==============================] - 1s 252us/step - loss: 0.0326 - acc: 0.9889 - val_loss: 1.3289 - val_acc: 0.8283\n",
            "Epoch 9/20\n",
            "2800/2800 [==============================] - 1s 246us/step - loss: 0.0194 - acc: 0.9932 - val_loss: 1.4315 - val_acc: 0.8267\n",
            "Epoch 10/20\n",
            "2800/2800 [==============================] - 1s 243us/step - loss: 0.0306 - acc: 0.9911 - val_loss: 1.4297 - val_acc: 0.8293\n",
            "Epoch 11/20\n",
            "2800/2800 [==============================] - 1s 251us/step - loss: 0.0256 - acc: 0.9918 - val_loss: 1.4653 - val_acc: 0.8284\n",
            "Epoch 12/20\n",
            "2800/2800 [==============================] - 1s 250us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 1.4877 - val_acc: 0.8265\n",
            "Epoch 13/20\n",
            "2800/2800 [==============================] - 1s 230us/step - loss: 0.0289 - acc: 0.9904 - val_loss: 1.3627 - val_acc: 0.8262\n",
            "Epoch 14/20\n",
            "2800/2800 [==============================] - 1s 247us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 1.5421 - val_acc: 0.8260\n",
            "Epoch 15/20\n",
            "2800/2800 [==============================] - 1s 247us/step - loss: 0.0191 - acc: 0.9932 - val_loss: 1.4070 - val_acc: 0.8277\n",
            "Epoch 16/20\n",
            "2800/2800 [==============================] - 1s 247us/step - loss: 0.0210 - acc: 0.9932 - val_loss: 1.5223 - val_acc: 0.8218\n",
            "Epoch 17/20\n",
            "2800/2800 [==============================] - 1s 251us/step - loss: 0.0200 - acc: 0.9946 - val_loss: 1.4703 - val_acc: 0.8243\n",
            "Epoch 18/20\n",
            "2800/2800 [==============================] - 1s 237us/step - loss: 0.0156 - acc: 0.9943 - val_loss: 1.4766 - val_acc: 0.8262\n",
            "Epoch 19/20\n",
            "2800/2800 [==============================] - 1s 234us/step - loss: 0.0218 - acc: 0.9925 - val_loss: 1.3955 - val_acc: 0.8289\n",
            "Epoch 20/20\n",
            "2800/2800 [==============================] - 1s 239us/step - loss: 0.0160 - acc: 0.9957 - val_loss: 1.4555 - val_acc: 0.8311\n",
            "Test loss: 1.4555020668566228\n",
            "Test accuracy: 0.8311\n",
            "Train loss: 0.0003726904929580347\n",
            "Train accuracy: 1.0\n",
            "x label: (2900, 28, 28)\n",
            "y label: (2900,)\n",
            "x unlabel: (57100, 28, 28)\n",
            "y unlabel: (57100,)\n",
            "28\n",
            "Train on 2900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "2900/2900 [==============================] - 1s 240us/step - loss: 0.0855 - acc: 0.9776 - val_loss: 1.3199 - val_acc: 0.8271\n",
            "Epoch 2/20\n",
            "2900/2900 [==============================] - 1s 249us/step - loss: 0.0584 - acc: 0.9831 - val_loss: 1.3316 - val_acc: 0.8267\n",
            "Epoch 3/20\n",
            "2900/2900 [==============================] - 1s 238us/step - loss: 0.0528 - acc: 0.9855 - val_loss: 1.3993 - val_acc: 0.8261\n",
            "Epoch 4/20\n",
            "2900/2900 [==============================] - 1s 237us/step - loss: 0.0376 - acc: 0.9866 - val_loss: 1.3000 - val_acc: 0.8282\n",
            "Epoch 5/20\n",
            "2900/2900 [==============================] - 1s 247us/step - loss: 0.0458 - acc: 0.9869 - val_loss: 1.3081 - val_acc: 0.8238\n",
            "Epoch 6/20\n",
            "2900/2900 [==============================] - 1s 240us/step - loss: 0.0301 - acc: 0.9890 - val_loss: 1.4666 - val_acc: 0.8284\n",
            "Epoch 7/20\n",
            "2900/2900 [==============================] - 1s 242us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 1.4578 - val_acc: 0.8266\n",
            "Epoch 8/20\n",
            "2900/2900 [==============================] - 1s 238us/step - loss: 0.0297 - acc: 0.9910 - val_loss: 1.4024 - val_acc: 0.8263\n",
            "Epoch 9/20\n",
            "2900/2900 [==============================] - 1s 237us/step - loss: 0.0276 - acc: 0.9910 - val_loss: 1.4805 - val_acc: 0.8282\n",
            "Epoch 10/20\n",
            "2900/2900 [==============================] - 1s 234us/step - loss: 0.0146 - acc: 0.9945 - val_loss: 1.5231 - val_acc: 0.8242\n",
            "Epoch 11/20\n",
            "2900/2900 [==============================] - 1s 238us/step - loss: 0.0247 - acc: 0.9900 - val_loss: 1.4649 - val_acc: 0.8245\n",
            "Epoch 12/20\n",
            "2900/2900 [==============================] - 1s 238us/step - loss: 0.0245 - acc: 0.9928 - val_loss: 1.4555 - val_acc: 0.8279\n",
            "Epoch 13/20\n",
            "2900/2900 [==============================] - 1s 236us/step - loss: 0.0166 - acc: 0.9948 - val_loss: 1.4958 - val_acc: 0.8289\n",
            "Epoch 14/20\n",
            "2900/2900 [==============================] - 1s 244us/step - loss: 0.0210 - acc: 0.9941 - val_loss: 1.5480 - val_acc: 0.8270\n",
            "Epoch 15/20\n",
            "2900/2900 [==============================] - 1s 227us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 1.4948 - val_acc: 0.8289\n",
            "Epoch 16/20\n",
            "2900/2900 [==============================] - 1s 248us/step - loss: 0.0122 - acc: 0.9952 - val_loss: 1.5040 - val_acc: 0.8297\n",
            "Epoch 17/20\n",
            "2900/2900 [==============================] - 1s 244us/step - loss: 0.0218 - acc: 0.9934 - val_loss: 1.4332 - val_acc: 0.8287\n",
            "Epoch 18/20\n",
            "2900/2900 [==============================] - 1s 233us/step - loss: 0.0239 - acc: 0.9924 - val_loss: 1.4805 - val_acc: 0.8294\n",
            "Epoch 19/20\n",
            "2900/2900 [==============================] - 1s 236us/step - loss: 0.0159 - acc: 0.9948 - val_loss: 1.4726 - val_acc: 0.8254\n",
            "Epoch 20/20\n",
            "2900/2900 [==============================] - 1s 246us/step - loss: 0.0119 - acc: 0.9955 - val_loss: 1.5555 - val_acc: 0.8281\n",
            "Test loss: 1.5554954614639283\n",
            "Test accuracy: 0.8281\n",
            "Train loss: 0.0004617326839278858\n",
            "Train accuracy: 0.9996551724137931\n",
            "x label: (3000, 28, 28)\n",
            "y label: (3000,)\n",
            "x unlabel: (57000, 28, 28)\n",
            "y unlabel: (57000,)\n",
            "29\n",
            "Train on 3000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3000/3000 [==============================] - 1s 242us/step - loss: 0.0865 - acc: 0.9790 - val_loss: 1.2907 - val_acc: 0.8298\n",
            "Epoch 2/20\n",
            "3000/3000 [==============================] - 1s 243us/step - loss: 0.0460 - acc: 0.9890 - val_loss: 1.3506 - val_acc: 0.8301\n",
            "Epoch 3/20\n",
            "3000/3000 [==============================] - 1s 239us/step - loss: 0.0502 - acc: 0.9847 - val_loss: 1.3714 - val_acc: 0.8275\n",
            "Epoch 4/20\n",
            "3000/3000 [==============================] - 1s 231us/step - loss: 0.0337 - acc: 0.9883 - val_loss: 1.4579 - val_acc: 0.8288\n",
            "Epoch 5/20\n",
            "3000/3000 [==============================] - 1s 243us/step - loss: 0.0398 - acc: 0.9880 - val_loss: 1.3609 - val_acc: 0.8287\n",
            "Epoch 6/20\n",
            "3000/3000 [==============================] - 1s 231us/step - loss: 0.0416 - acc: 0.9880 - val_loss: 1.3765 - val_acc: 0.8311\n",
            "Epoch 7/20\n",
            "3000/3000 [==============================] - 1s 234us/step - loss: 0.0337 - acc: 0.9903 - val_loss: 1.3615 - val_acc: 0.8309\n",
            "Epoch 8/20\n",
            "3000/3000 [==============================] - 1s 239us/step - loss: 0.0269 - acc: 0.9910 - val_loss: 1.3761 - val_acc: 0.8286\n",
            "Epoch 9/20\n",
            "3000/3000 [==============================] - 1s 237us/step - loss: 0.0240 - acc: 0.9907 - val_loss: 1.4351 - val_acc: 0.8282\n",
            "Epoch 10/20\n",
            "3000/3000 [==============================] - 1s 242us/step - loss: 0.0225 - acc: 0.9937 - val_loss: 1.4391 - val_acc: 0.8295\n",
            "Epoch 11/20\n",
            "3000/3000 [==============================] - 1s 238us/step - loss: 0.0225 - acc: 0.9917 - val_loss: 1.4374 - val_acc: 0.8290\n",
            "Epoch 12/20\n",
            "3000/3000 [==============================] - 1s 240us/step - loss: 0.0236 - acc: 0.9927 - val_loss: 1.5185 - val_acc: 0.8288\n",
            "Epoch 13/20\n",
            "3000/3000 [==============================] - 1s 235us/step - loss: 0.0300 - acc: 0.9910 - val_loss: 1.3547 - val_acc: 0.8271\n",
            "Epoch 14/20\n",
            "3000/3000 [==============================] - 1s 243us/step - loss: 0.0231 - acc: 0.9937 - val_loss: 1.4540 - val_acc: 0.8260\n",
            "Epoch 15/20\n",
            "3000/3000 [==============================] - 1s 225us/step - loss: 0.0181 - acc: 0.9943 - val_loss: 1.4411 - val_acc: 0.8304\n",
            "Epoch 16/20\n",
            "3000/3000 [==============================] - 1s 237us/step - loss: 0.0185 - acc: 0.9953 - val_loss: 1.4439 - val_acc: 0.8297\n",
            "Epoch 17/20\n",
            "3000/3000 [==============================] - 1s 234us/step - loss: 0.0189 - acc: 0.9943 - val_loss: 1.4344 - val_acc: 0.8309\n",
            "Epoch 18/20\n",
            "3000/3000 [==============================] - 1s 234us/step - loss: 0.0181 - acc: 0.9930 - val_loss: 1.4637 - val_acc: 0.8306\n",
            "Epoch 19/20\n",
            "3000/3000 [==============================] - 1s 233us/step - loss: 0.0180 - acc: 0.9930 - val_loss: 1.4512 - val_acc: 0.8327\n",
            "Epoch 20/20\n",
            "3000/3000 [==============================] - 1s 233us/step - loss: 0.0114 - acc: 0.9960 - val_loss: 1.5666 - val_acc: 0.8272\n",
            "Test loss: 1.5665890187561513\n",
            "Test accuracy: 0.8272\n",
            "Train loss: 0.00019645803230787352\n",
            "Train accuracy: 1.0\n",
            "x label: (3100, 28, 28)\n",
            "y label: (3100,)\n",
            "x unlabel: (56900, 28, 28)\n",
            "y unlabel: (56900,)\n",
            "30\n",
            "Train on 3100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3100/3100 [==============================] - 1s 236us/step - loss: 0.0804 - acc: 0.9771 - val_loss: 1.2916 - val_acc: 0.8286\n",
            "Epoch 2/20\n",
            "3100/3100 [==============================] - 1s 232us/step - loss: 0.0487 - acc: 0.9871 - val_loss: 1.3525 - val_acc: 0.8284\n",
            "Epoch 3/20\n",
            "3100/3100 [==============================] - 1s 225us/step - loss: 0.0564 - acc: 0.9855 - val_loss: 1.3140 - val_acc: 0.8307\n",
            "Epoch 4/20\n",
            "3100/3100 [==============================] - 1s 230us/step - loss: 0.0419 - acc: 0.9890 - val_loss: 1.3061 - val_acc: 0.8306\n",
            "Epoch 5/20\n",
            "3100/3100 [==============================] - 1s 228us/step - loss: 0.0348 - acc: 0.9900 - val_loss: 1.4174 - val_acc: 0.8307\n",
            "Epoch 6/20\n",
            "3100/3100 [==============================] - 1s 230us/step - loss: 0.0485 - acc: 0.9865 - val_loss: 1.4092 - val_acc: 0.8316\n",
            "Epoch 7/20\n",
            "3100/3100 [==============================] - 1s 227us/step - loss: 0.0366 - acc: 0.9871 - val_loss: 1.3157 - val_acc: 0.8338\n",
            "Epoch 8/20\n",
            "3100/3100 [==============================] - 1s 230us/step - loss: 0.0302 - acc: 0.9906 - val_loss: 1.3833 - val_acc: 0.8324\n",
            "Epoch 9/20\n",
            "3100/3100 [==============================] - 1s 241us/step - loss: 0.0298 - acc: 0.9935 - val_loss: 1.4371 - val_acc: 0.8281\n",
            "Epoch 10/20\n",
            "3100/3100 [==============================] - 1s 233us/step - loss: 0.0188 - acc: 0.9926 - val_loss: 1.3663 - val_acc: 0.8344\n",
            "Epoch 11/20\n",
            "3100/3100 [==============================] - 1s 241us/step - loss: 0.0259 - acc: 0.9926 - val_loss: 1.3678 - val_acc: 0.8347\n",
            "Epoch 12/20\n",
            "3100/3100 [==============================] - 1s 229us/step - loss: 0.0187 - acc: 0.9935 - val_loss: 1.4488 - val_acc: 0.8312\n",
            "Epoch 13/20\n",
            "3100/3100 [==============================] - 1s 236us/step - loss: 0.0202 - acc: 0.9939 - val_loss: 1.4503 - val_acc: 0.8327\n",
            "Epoch 14/20\n",
            "3100/3100 [==============================] - 1s 233us/step - loss: 0.0198 - acc: 0.9939 - val_loss: 1.4148 - val_acc: 0.8320\n",
            "Epoch 15/20\n",
            "3100/3100 [==============================] - 1s 226us/step - loss: 0.0214 - acc: 0.9926 - val_loss: 1.4435 - val_acc: 0.8314\n",
            "Epoch 16/20\n",
            "3100/3100 [==============================] - 1s 231us/step - loss: 0.0173 - acc: 0.9948 - val_loss: 1.4627 - val_acc: 0.8311\n",
            "Epoch 17/20\n",
            "3100/3100 [==============================] - 1s 239us/step - loss: 0.0207 - acc: 0.9939 - val_loss: 1.4986 - val_acc: 0.8290\n",
            "Epoch 18/20\n",
            "3100/3100 [==============================] - 1s 230us/step - loss: 0.0194 - acc: 0.9923 - val_loss: 1.4047 - val_acc: 0.8295\n",
            "Epoch 19/20\n",
            "3100/3100 [==============================] - 1s 233us/step - loss: 0.0207 - acc: 0.9932 - val_loss: 1.3578 - val_acc: 0.8321\n",
            "Epoch 20/20\n",
            "3100/3100 [==============================] - 1s 241us/step - loss: 0.0151 - acc: 0.9942 - val_loss: 1.4595 - val_acc: 0.8313\n",
            "Test loss: 1.4595125398278237\n",
            "Test accuracy: 0.8313\n",
            "Train loss: 0.00024660990221462035\n",
            "Train accuracy: 1.0\n",
            "x label: (3200, 28, 28)\n",
            "y label: (3200,)\n",
            "x unlabel: (56800, 28, 28)\n",
            "y unlabel: (56800,)\n",
            "31\n",
            "Train on 3200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3200/3200 [==============================] - 1s 232us/step - loss: 0.0689 - acc: 0.9850 - val_loss: 1.2775 - val_acc: 0.8308\n",
            "Epoch 2/20\n",
            "3200/3200 [==============================] - 1s 225us/step - loss: 0.0755 - acc: 0.9853 - val_loss: 1.2083 - val_acc: 0.8300\n",
            "Epoch 3/20\n",
            "3200/3200 [==============================] - 1s 229us/step - loss: 0.0612 - acc: 0.9834 - val_loss: 1.4501 - val_acc: 0.8243\n",
            "Epoch 4/20\n",
            "3200/3200 [==============================] - 1s 225us/step - loss: 0.0498 - acc: 0.9850 - val_loss: 1.4111 - val_acc: 0.8322\n",
            "Epoch 5/20\n",
            "3200/3200 [==============================] - 1s 229us/step - loss: 0.0423 - acc: 0.9897 - val_loss: 1.3162 - val_acc: 0.8328\n",
            "Epoch 6/20\n",
            "3200/3200 [==============================] - 1s 226us/step - loss: 0.0333 - acc: 0.9928 - val_loss: 1.4418 - val_acc: 0.8289\n",
            "Epoch 7/20\n",
            "3200/3200 [==============================] - 1s 232us/step - loss: 0.0359 - acc: 0.9897 - val_loss: 1.4618 - val_acc: 0.8324\n",
            "Epoch 8/20\n",
            "3200/3200 [==============================] - 1s 229us/step - loss: 0.0323 - acc: 0.9891 - val_loss: 1.4708 - val_acc: 0.8310\n",
            "Epoch 9/20\n",
            "3200/3200 [==============================] - 1s 230us/step - loss: 0.0336 - acc: 0.9906 - val_loss: 1.4095 - val_acc: 0.8289\n",
            "Epoch 10/20\n",
            "3200/3200 [==============================] - 1s 226us/step - loss: 0.0249 - acc: 0.9934 - val_loss: 1.3934 - val_acc: 0.8315\n",
            "Epoch 11/20\n",
            "3200/3200 [==============================] - 1s 226us/step - loss: 0.0387 - acc: 0.9878 - val_loss: 1.3014 - val_acc: 0.8344\n",
            "Epoch 12/20\n",
            "3200/3200 [==============================] - 1s 223us/step - loss: 0.0246 - acc: 0.9938 - val_loss: 1.4033 - val_acc: 0.8313\n",
            "Epoch 13/20\n",
            "3200/3200 [==============================] - 1s 224us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 1.4506 - val_acc: 0.8318\n",
            "Epoch 14/20\n",
            "3200/3200 [==============================] - 1s 228us/step - loss: 0.0192 - acc: 0.9934 - val_loss: 1.4476 - val_acc: 0.8332\n",
            "Epoch 15/20\n",
            "3200/3200 [==============================] - 1s 237us/step - loss: 0.0186 - acc: 0.9941 - val_loss: 1.4469 - val_acc: 0.8319\n",
            "Epoch 16/20\n",
            "3200/3200 [==============================] - 1s 238us/step - loss: 0.0164 - acc: 0.9944 - val_loss: 1.5149 - val_acc: 0.8320\n",
            "Epoch 17/20\n",
            "3200/3200 [==============================] - 1s 231us/step - loss: 0.0209 - acc: 0.9925 - val_loss: 1.4285 - val_acc: 0.8360\n",
            "Epoch 18/20\n",
            "3200/3200 [==============================] - 1s 225us/step - loss: 0.0218 - acc: 0.9931 - val_loss: 1.4198 - val_acc: 0.8337\n",
            "Epoch 19/20\n",
            "3200/3200 [==============================] - 1s 231us/step - loss: 0.0174 - acc: 0.9938 - val_loss: 1.4358 - val_acc: 0.8343\n",
            "Epoch 20/20\n",
            "3200/3200 [==============================] - 1s 235us/step - loss: 0.0139 - acc: 0.9966 - val_loss: 1.4178 - val_acc: 0.8329\n",
            "Test loss: 1.41782453376092\n",
            "Test accuracy: 0.8329\n",
            "Train loss: 9.879122225569858e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (3300, 28, 28)\n",
            "y label: (3300,)\n",
            "x unlabel: (56700, 28, 28)\n",
            "y unlabel: (56700,)\n",
            "32\n",
            "Train on 3300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3300/3300 [==============================] - 1s 229us/step - loss: 0.0867 - acc: 0.9803 - val_loss: 1.2045 - val_acc: 0.8348\n",
            "Epoch 2/20\n",
            "3300/3300 [==============================] - 1s 233us/step - loss: 0.0616 - acc: 0.9852 - val_loss: 1.3464 - val_acc: 0.8352\n",
            "Epoch 3/20\n",
            "3300/3300 [==============================] - 1s 216us/step - loss: 0.0515 - acc: 0.9903 - val_loss: 1.3393 - val_acc: 0.8319\n",
            "Epoch 4/20\n",
            "3300/3300 [==============================] - 1s 223us/step - loss: 0.0528 - acc: 0.9867 - val_loss: 1.2842 - val_acc: 0.8324\n",
            "Epoch 5/20\n",
            "3300/3300 [==============================] - 1s 233us/step - loss: 0.0457 - acc: 0.9864 - val_loss: 1.3079 - val_acc: 0.8317\n",
            "Epoch 6/20\n",
            "3300/3300 [==============================] - 1s 227us/step - loss: 0.0415 - acc: 0.9858 - val_loss: 1.3472 - val_acc: 0.8329\n",
            "Epoch 7/20\n",
            "3300/3300 [==============================] - 1s 219us/step - loss: 0.0366 - acc: 0.9921 - val_loss: 1.3517 - val_acc: 0.8357\n",
            "Epoch 8/20\n",
            "3300/3300 [==============================] - 1s 225us/step - loss: 0.0355 - acc: 0.9891 - val_loss: 1.2998 - val_acc: 0.8348\n",
            "Epoch 9/20\n",
            "3300/3300 [==============================] - 1s 238us/step - loss: 0.0292 - acc: 0.9912 - val_loss: 1.3242 - val_acc: 0.8340\n",
            "Epoch 10/20\n",
            "3300/3300 [==============================] - 1s 225us/step - loss: 0.0274 - acc: 0.9915 - val_loss: 1.3573 - val_acc: 0.8353\n",
            "Epoch 11/20\n",
            "3300/3300 [==============================] - 1s 224us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 1.3727 - val_acc: 0.8349\n",
            "Epoch 12/20\n",
            "3300/3300 [==============================] - 1s 226us/step - loss: 0.0311 - acc: 0.9921 - val_loss: 1.4125 - val_acc: 0.8278\n",
            "Epoch 13/20\n",
            "3300/3300 [==============================] - 1s 223us/step - loss: 0.0175 - acc: 0.9948 - val_loss: 1.4510 - val_acc: 0.8309\n",
            "Epoch 14/20\n",
            "3300/3300 [==============================] - 1s 227us/step - loss: 0.0187 - acc: 0.9945 - val_loss: 1.4173 - val_acc: 0.8338\n",
            "Epoch 15/20\n",
            "3300/3300 [==============================] - 1s 217us/step - loss: 0.0223 - acc: 0.9933 - val_loss: 1.4664 - val_acc: 0.8335\n",
            "Epoch 16/20\n",
            "3300/3300 [==============================] - 1s 226us/step - loss: 0.0182 - acc: 0.9945 - val_loss: 1.4890 - val_acc: 0.8342\n",
            "Epoch 17/20\n",
            "3300/3300 [==============================] - 1s 224us/step - loss: 0.0219 - acc: 0.9939 - val_loss: 1.4811 - val_acc: 0.8304\n",
            "Epoch 18/20\n",
            "3300/3300 [==============================] - 1s 224us/step - loss: 0.0156 - acc: 0.9955 - val_loss: 1.4763 - val_acc: 0.8290\n",
            "Epoch 19/20\n",
            "3300/3300 [==============================] - 1s 236us/step - loss: 0.0131 - acc: 0.9964 - val_loss: 1.5219 - val_acc: 0.8293\n",
            "Epoch 20/20\n",
            "3300/3300 [==============================] - 1s 221us/step - loss: 0.0122 - acc: 0.9958 - val_loss: 1.5360 - val_acc: 0.8320\n",
            "Test loss: 1.535968170750886\n",
            "Test accuracy: 0.832\n",
            "Train loss: 0.00012153280861363756\n",
            "Train accuracy: 1.0\n",
            "x label: (3400, 28, 28)\n",
            "y label: (3400,)\n",
            "x unlabel: (56600, 28, 28)\n",
            "y unlabel: (56600,)\n",
            "33\n",
            "Train on 3400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3400/3400 [==============================] - 1s 212us/step - loss: 0.0585 - acc: 0.9865 - val_loss: 1.2570 - val_acc: 0.8326\n",
            "Epoch 2/20\n",
            "3400/3400 [==============================] - 1s 221us/step - loss: 0.0587 - acc: 0.9850 - val_loss: 1.3232 - val_acc: 0.8345\n",
            "Epoch 3/20\n",
            "3400/3400 [==============================] - 1s 226us/step - loss: 0.0378 - acc: 0.9888 - val_loss: 1.3822 - val_acc: 0.8327\n",
            "Epoch 4/20\n",
            "3400/3400 [==============================] - 1s 219us/step - loss: 0.0301 - acc: 0.9906 - val_loss: 1.4369 - val_acc: 0.8334\n",
            "Epoch 5/20\n",
            "3400/3400 [==============================] - 1s 218us/step - loss: 0.0336 - acc: 0.9918 - val_loss: 1.3336 - val_acc: 0.8338\n",
            "Epoch 6/20\n",
            "3400/3400 [==============================] - 1s 224us/step - loss: 0.0197 - acc: 0.9932 - val_loss: 1.5041 - val_acc: 0.8378\n",
            "Epoch 7/20\n",
            "3400/3400 [==============================] - 1s 217us/step - loss: 0.0274 - acc: 0.9915 - val_loss: 1.2290 - val_acc: 0.8366\n",
            "Epoch 8/20\n",
            "3400/3400 [==============================] - 1s 232us/step - loss: 0.0250 - acc: 0.9921 - val_loss: 1.4794 - val_acc: 0.8308\n",
            "Epoch 9/20\n",
            "3400/3400 [==============================] - 1s 215us/step - loss: 0.0265 - acc: 0.9903 - val_loss: 1.4428 - val_acc: 0.8365\n",
            "Epoch 10/20\n",
            "3400/3400 [==============================] - 1s 222us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 1.4660 - val_acc: 0.8330\n",
            "Epoch 11/20\n",
            "3400/3400 [==============================] - 1s 218us/step - loss: 0.0272 - acc: 0.9915 - val_loss: 1.4336 - val_acc: 0.8340\n",
            "Epoch 12/20\n",
            "3400/3400 [==============================] - 1s 216us/step - loss: 0.0202 - acc: 0.9929 - val_loss: 1.2548 - val_acc: 0.8364\n",
            "Epoch 13/20\n",
            "3400/3400 [==============================] - 1s 216us/step - loss: 0.0216 - acc: 0.9924 - val_loss: 1.3908 - val_acc: 0.8379\n",
            "Epoch 14/20\n",
            "3400/3400 [==============================] - 1s 219us/step - loss: 0.0177 - acc: 0.9944 - val_loss: 1.4782 - val_acc: 0.8358\n",
            "Epoch 15/20\n",
            "3400/3400 [==============================] - 1s 215us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 1.4253 - val_acc: 0.8303\n",
            "Epoch 16/20\n",
            "3400/3400 [==============================] - 1s 216us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 1.4404 - val_acc: 0.8371\n",
            "Epoch 17/20\n",
            "3400/3400 [==============================] - 1s 223us/step - loss: 0.0140 - acc: 0.9956 - val_loss: 1.4556 - val_acc: 0.8351\n",
            "Epoch 18/20\n",
            "3400/3400 [==============================] - 1s 226us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 1.4415 - val_acc: 0.8383\n",
            "Epoch 19/20\n",
            "3400/3400 [==============================] - 1s 225us/step - loss: 0.0164 - acc: 0.9938 - val_loss: 1.4898 - val_acc: 0.8343\n",
            "Epoch 20/20\n",
            "3400/3400 [==============================] - 1s 220us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 1.4437 - val_acc: 0.8345\n",
            "Test loss: 1.4437449204564095\n",
            "Test accuracy: 0.8345\n",
            "Train loss: 0.000224819997429222\n",
            "Train accuracy: 1.0\n",
            "x label: (3500, 28, 28)\n",
            "y label: (3500,)\n",
            "x unlabel: (56500, 28, 28)\n",
            "y unlabel: (56500,)\n",
            "34\n",
            "Train on 3500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3500/3500 [==============================] - 1s 222us/step - loss: 0.0887 - acc: 0.9809 - val_loss: 1.2139 - val_acc: 0.8328\n",
            "Epoch 2/20\n",
            "3500/3500 [==============================] - 1s 214us/step - loss: 0.0594 - acc: 0.9869 - val_loss: 1.3935 - val_acc: 0.8338\n",
            "Epoch 3/20\n",
            "3500/3500 [==============================] - 1s 214us/step - loss: 0.0572 - acc: 0.9857 - val_loss: 1.2459 - val_acc: 0.8377\n",
            "Epoch 4/20\n",
            "3500/3500 [==============================] - 1s 215us/step - loss: 0.0441 - acc: 0.9874 - val_loss: 1.3731 - val_acc: 0.8350\n",
            "Epoch 5/20\n",
            "3500/3500 [==============================] - 1s 214us/step - loss: 0.0451 - acc: 0.9897 - val_loss: 1.4136 - val_acc: 0.8365\n",
            "Epoch 6/20\n",
            "3500/3500 [==============================] - 1s 220us/step - loss: 0.0245 - acc: 0.9934 - val_loss: 1.4279 - val_acc: 0.8388\n",
            "Epoch 7/20\n",
            "3500/3500 [==============================] - 1s 216us/step - loss: 0.0299 - acc: 0.9923 - val_loss: 1.3944 - val_acc: 0.8353\n",
            "Epoch 8/20\n",
            "3500/3500 [==============================] - 1s 212us/step - loss: 0.0256 - acc: 0.9906 - val_loss: 1.4996 - val_acc: 0.8362\n",
            "Epoch 9/20\n",
            "3500/3500 [==============================] - 1s 217us/step - loss: 0.0256 - acc: 0.9914 - val_loss: 1.4707 - val_acc: 0.8343\n",
            "Epoch 10/20\n",
            "3500/3500 [==============================] - 1s 218us/step - loss: 0.0246 - acc: 0.9926 - val_loss: 1.3330 - val_acc: 0.8375\n",
            "Epoch 11/20\n",
            "3500/3500 [==============================] - 1s 223us/step - loss: 0.0202 - acc: 0.9943 - val_loss: 1.4550 - val_acc: 0.8376\n",
            "Epoch 12/20\n",
            "3500/3500 [==============================] - 1s 211us/step - loss: 0.0275 - acc: 0.9903 - val_loss: 1.3918 - val_acc: 0.8366\n",
            "Epoch 13/20\n",
            "3500/3500 [==============================] - 1s 212us/step - loss: 0.0216 - acc: 0.9926 - val_loss: 1.4672 - val_acc: 0.8369\n",
            "Epoch 14/20\n",
            "3500/3500 [==============================] - 1s 218us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 1.4116 - val_acc: 0.8325\n",
            "Epoch 15/20\n",
            "3500/3500 [==============================] - 1s 213us/step - loss: 0.0149 - acc: 0.9954 - val_loss: 1.3925 - val_acc: 0.8362\n",
            "Epoch 16/20\n",
            "3500/3500 [==============================] - 1s 218us/step - loss: 0.0228 - acc: 0.9929 - val_loss: 1.4735 - val_acc: 0.8359\n",
            "Epoch 17/20\n",
            "3500/3500 [==============================] - 1s 218us/step - loss: 0.0158 - acc: 0.9946 - val_loss: 1.4006 - val_acc: 0.8405\n",
            "Epoch 18/20\n",
            "3500/3500 [==============================] - 1s 216us/step - loss: 0.0128 - acc: 0.9966 - val_loss: 1.4058 - val_acc: 0.8382\n",
            "Epoch 19/20\n",
            "3500/3500 [==============================] - 1s 217us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 1.3987 - val_acc: 0.8385\n",
            "Epoch 20/20\n",
            "3500/3500 [==============================] - 1s 216us/step - loss: 0.0161 - acc: 0.9960 - val_loss: 1.3597 - val_acc: 0.8405\n",
            "Test loss: 1.359659547048807\n",
            "Test accuracy: 0.8405\n",
            "Train loss: 0.00015378374999363585\n",
            "Train accuracy: 1.0\n",
            "x label: (3600, 28, 28)\n",
            "y label: (3600,)\n",
            "x unlabel: (56400, 28, 28)\n",
            "y unlabel: (56400,)\n",
            "35\n",
            "Train on 3600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3600/3600 [==============================] - 1s 222us/step - loss: 0.0617 - acc: 0.9839 - val_loss: 1.2995 - val_acc: 0.8369\n",
            "Epoch 2/20\n",
            "3600/3600 [==============================] - 1s 216us/step - loss: 0.0516 - acc: 0.9864 - val_loss: 1.3872 - val_acc: 0.8405\n",
            "Epoch 3/20\n",
            "3600/3600 [==============================] - 1s 211us/step - loss: 0.0450 - acc: 0.9856 - val_loss: 1.2964 - val_acc: 0.8400\n",
            "Epoch 4/20\n",
            "3600/3600 [==============================] - 1s 210us/step - loss: 0.0377 - acc: 0.9889 - val_loss: 1.3068 - val_acc: 0.8393\n",
            "Epoch 5/20\n",
            "3600/3600 [==============================] - 1s 215us/step - loss: 0.0262 - acc: 0.9922 - val_loss: 1.3318 - val_acc: 0.8389\n",
            "Epoch 6/20\n",
            "3600/3600 [==============================] - 1s 215us/step - loss: 0.0330 - acc: 0.9889 - val_loss: 1.4145 - val_acc: 0.8352\n",
            "Epoch 7/20\n",
            "3600/3600 [==============================] - 1s 217us/step - loss: 0.0274 - acc: 0.9903 - val_loss: 1.3354 - val_acc: 0.8392\n",
            "Epoch 8/20\n",
            "3600/3600 [==============================] - 1s 207us/step - loss: 0.0228 - acc: 0.9928 - val_loss: 1.4026 - val_acc: 0.8361\n",
            "Epoch 9/20\n",
            "3600/3600 [==============================] - 1s 215us/step - loss: 0.0292 - acc: 0.9911 - val_loss: 1.3918 - val_acc: 0.8397\n",
            "Epoch 10/20\n",
            "3600/3600 [==============================] - 1s 219us/step - loss: 0.0196 - acc: 0.9936 - val_loss: 1.4812 - val_acc: 0.8390\n",
            "Epoch 11/20\n",
            "3600/3600 [==============================] - 1s 217us/step - loss: 0.0226 - acc: 0.9917 - val_loss: 1.3935 - val_acc: 0.8378\n",
            "Epoch 12/20\n",
            "3600/3600 [==============================] - 1s 224us/step - loss: 0.0127 - acc: 0.9972 - val_loss: 1.4464 - val_acc: 0.8392\n",
            "Epoch 13/20\n",
            "3600/3600 [==============================] - 1s 204us/step - loss: 0.0188 - acc: 0.9939 - val_loss: 1.4104 - val_acc: 0.8407\n",
            "Epoch 14/20\n",
            "3600/3600 [==============================] - 1s 223us/step - loss: 0.0203 - acc: 0.9922 - val_loss: 1.3759 - val_acc: 0.8403\n",
            "Epoch 15/20\n",
            "3600/3600 [==============================] - 1s 216us/step - loss: 0.0205 - acc: 0.9933 - val_loss: 1.4497 - val_acc: 0.8440\n",
            "Epoch 16/20\n",
            "3600/3600 [==============================] - 1s 220us/step - loss: 0.0173 - acc: 0.9925 - val_loss: 1.4401 - val_acc: 0.8424\n",
            "Epoch 17/20\n",
            "3600/3600 [==============================] - 1s 212us/step - loss: 0.0186 - acc: 0.9936 - val_loss: 1.4386 - val_acc: 0.8396\n",
            "Epoch 18/20\n",
            "3600/3600 [==============================] - 1s 223us/step - loss: 0.0171 - acc: 0.9950 - val_loss: 1.4050 - val_acc: 0.8421\n",
            "Epoch 19/20\n",
            "3600/3600 [==============================] - 1s 216us/step - loss: 0.0127 - acc: 0.9953 - val_loss: 1.4326 - val_acc: 0.8418\n",
            "Epoch 20/20\n",
            "3600/3600 [==============================] - 1s 218us/step - loss: 0.0197 - acc: 0.9928 - val_loss: 1.4403 - val_acc: 0.8409\n",
            "Test loss: 1.440326754897833\n",
            "Test accuracy: 0.8409\n",
            "Train loss: 0.0001564201483736522\n",
            "Train accuracy: 1.0\n",
            "x label: (3700, 28, 28)\n",
            "y label: (3700,)\n",
            "x unlabel: (56300, 28, 28)\n",
            "y unlabel: (56300,)\n",
            "36\n",
            "Train on 3700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3700/3700 [==============================] - 1s 220us/step - loss: 0.0836 - acc: 0.9819 - val_loss: 1.3728 - val_acc: 0.8389\n",
            "Epoch 2/20\n",
            "3700/3700 [==============================] - 1s 210us/step - loss: 0.0562 - acc: 0.9841 - val_loss: 1.2885 - val_acc: 0.8387\n",
            "Epoch 3/20\n",
            "3700/3700 [==============================] - 1s 225us/step - loss: 0.0521 - acc: 0.9868 - val_loss: 1.3987 - val_acc: 0.8406\n",
            "Epoch 4/20\n",
            "3700/3700 [==============================] - 1s 216us/step - loss: 0.0433 - acc: 0.9881 - val_loss: 1.3380 - val_acc: 0.8397\n",
            "Epoch 5/20\n",
            "3700/3700 [==============================] - 1s 211us/step - loss: 0.0314 - acc: 0.9886 - val_loss: 1.3189 - val_acc: 0.8424\n",
            "Epoch 6/20\n",
            "3700/3700 [==============================] - 1s 210us/step - loss: 0.0230 - acc: 0.9943 - val_loss: 1.3404 - val_acc: 0.8432\n",
            "Epoch 7/20\n",
            "3700/3700 [==============================] - 1s 210us/step - loss: 0.0380 - acc: 0.9900 - val_loss: 1.3560 - val_acc: 0.8387\n",
            "Epoch 8/20\n",
            "3700/3700 [==============================] - 1s 210us/step - loss: 0.0330 - acc: 0.9908 - val_loss: 1.3699 - val_acc: 0.8388\n",
            "Epoch 9/20\n",
            "3700/3700 [==============================] - 1s 212us/step - loss: 0.0265 - acc: 0.9911 - val_loss: 1.4370 - val_acc: 0.8399\n",
            "Epoch 10/20\n",
            "3700/3700 [==============================] - 1s 204us/step - loss: 0.0305 - acc: 0.9914 - val_loss: 1.3957 - val_acc: 0.8440\n",
            "Epoch 11/20\n",
            "3700/3700 [==============================] - 1s 215us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 1.3668 - val_acc: 0.8439\n",
            "Epoch 12/20\n",
            "3700/3700 [==============================] - 1s 209us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 1.4091 - val_acc: 0.8361\n",
            "Epoch 13/20\n",
            "3700/3700 [==============================] - 1s 213us/step - loss: 0.0199 - acc: 0.9919 - val_loss: 1.3783 - val_acc: 0.8426\n",
            "Epoch 14/20\n",
            "3700/3700 [==============================] - 1s 215us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 1.4134 - val_acc: 0.8435\n",
            "Epoch 15/20\n",
            "3700/3700 [==============================] - 1s 207us/step - loss: 0.0195 - acc: 0.9924 - val_loss: 1.3497 - val_acc: 0.8413\n",
            "Epoch 16/20\n",
            "3700/3700 [==============================] - 1s 207us/step - loss: 0.0183 - acc: 0.9927 - val_loss: 1.3571 - val_acc: 0.8411\n",
            "Epoch 17/20\n",
            "3700/3700 [==============================] - 1s 217us/step - loss: 0.0208 - acc: 0.9930 - val_loss: 1.4783 - val_acc: 0.8394\n",
            "Epoch 18/20\n",
            "3700/3700 [==============================] - 1s 214us/step - loss: 0.0206 - acc: 0.9932 - val_loss: 1.4567 - val_acc: 0.8399\n",
            "Epoch 19/20\n",
            "3700/3700 [==============================] - 1s 209us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 1.5330 - val_acc: 0.8391\n",
            "Epoch 20/20\n",
            "3700/3700 [==============================] - 1s 212us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 1.4448 - val_acc: 0.8436\n",
            "Test loss: 1.4448222469747067\n",
            "Test accuracy: 0.8436\n",
            "Train loss: 0.000169306302357138\n",
            "Train accuracy: 1.0\n",
            "x label: (3800, 28, 28)\n",
            "y label: (3800,)\n",
            "x unlabel: (56200, 28, 28)\n",
            "y unlabel: (56200,)\n",
            "37\n",
            "Train on 3800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3800/3800 [==============================] - 1s 210us/step - loss: 0.0634 - acc: 0.9845 - val_loss: 1.3077 - val_acc: 0.8358\n",
            "Epoch 2/20\n",
            "3800/3800 [==============================] - 1s 211us/step - loss: 0.0588 - acc: 0.9850 - val_loss: 1.2896 - val_acc: 0.8380\n",
            "Epoch 3/20\n",
            "3800/3800 [==============================] - 1s 208us/step - loss: 0.0445 - acc: 0.9889 - val_loss: 1.3190 - val_acc: 0.8445\n",
            "Epoch 4/20\n",
            "3800/3800 [==============================] - 1s 208us/step - loss: 0.0343 - acc: 0.9879 - val_loss: 1.3173 - val_acc: 0.8413\n",
            "Epoch 5/20\n",
            "3800/3800 [==============================] - 1s 214us/step - loss: 0.0346 - acc: 0.9889 - val_loss: 1.2861 - val_acc: 0.8422\n",
            "Epoch 6/20\n",
            "3800/3800 [==============================] - 1s 219us/step - loss: 0.0227 - acc: 0.9929 - val_loss: 1.3542 - val_acc: 0.8407\n",
            "Epoch 7/20\n",
            "3800/3800 [==============================] - 1s 212us/step - loss: 0.0349 - acc: 0.9900 - val_loss: 1.3624 - val_acc: 0.8418\n",
            "Epoch 8/20\n",
            "3800/3800 [==============================] - 1s 215us/step - loss: 0.0276 - acc: 0.9908 - val_loss: 1.3497 - val_acc: 0.8425\n",
            "Epoch 9/20\n",
            "3800/3800 [==============================] - 1s 211us/step - loss: 0.0242 - acc: 0.9921 - val_loss: 1.4065 - val_acc: 0.8409\n",
            "Epoch 10/20\n",
            "3800/3800 [==============================] - 1s 215us/step - loss: 0.0235 - acc: 0.9924 - val_loss: 1.3262 - val_acc: 0.8420\n",
            "Epoch 11/20\n",
            "3800/3800 [==============================] - 1s 205us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 1.3802 - val_acc: 0.8415\n",
            "Epoch 12/20\n",
            "3800/3800 [==============================] - 1s 208us/step - loss: 0.0199 - acc: 0.9937 - val_loss: 1.4124 - val_acc: 0.8394\n",
            "Epoch 13/20\n",
            "3800/3800 [==============================] - 1s 209us/step - loss: 0.0201 - acc: 0.9929 - val_loss: 1.3702 - val_acc: 0.8430\n",
            "Epoch 14/20\n",
            "3800/3800 [==============================] - 1s 208us/step - loss: 0.0222 - acc: 0.9939 - val_loss: 1.3400 - val_acc: 0.8458\n",
            "Epoch 15/20\n",
            "3800/3800 [==============================] - 1s 205us/step - loss: 0.0186 - acc: 0.9921 - val_loss: 1.3765 - val_acc: 0.8423\n",
            "Epoch 16/20\n",
            "3800/3800 [==============================] - 1s 208us/step - loss: 0.0205 - acc: 0.9934 - val_loss: 1.3364 - val_acc: 0.8423\n",
            "Epoch 17/20\n",
            "3800/3800 [==============================] - 1s 211us/step - loss: 0.0205 - acc: 0.9937 - val_loss: 1.3667 - val_acc: 0.8448\n",
            "Epoch 18/20\n",
            "3800/3800 [==============================] - 1s 205us/step - loss: 0.0249 - acc: 0.9932 - val_loss: 1.4227 - val_acc: 0.8416\n",
            "Epoch 19/20\n",
            "3800/3800 [==============================] - 1s 217us/step - loss: 0.0167 - acc: 0.9942 - val_loss: 1.4531 - val_acc: 0.8394\n",
            "Epoch 20/20\n",
            "3800/3800 [==============================] - 1s 203us/step - loss: 0.0171 - acc: 0.9937 - val_loss: 1.4570 - val_acc: 0.8428\n",
            "Test loss: 1.4569954323120415\n",
            "Test accuracy: 0.8428\n",
            "Train loss: 8.326014242201334e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (3900, 28, 28)\n",
            "y label: (3900,)\n",
            "x unlabel: (56100, 28, 28)\n",
            "y unlabel: (56100,)\n",
            "38\n",
            "Train on 3900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "3900/3900 [==============================] - 1s 210us/step - loss: 0.0549 - acc: 0.9872 - val_loss: 1.2907 - val_acc: 0.8339\n",
            "Epoch 2/20\n",
            "3900/3900 [==============================] - 1s 204us/step - loss: 0.0495 - acc: 0.9836 - val_loss: 1.3342 - val_acc: 0.8425\n",
            "Epoch 3/20\n",
            "3900/3900 [==============================] - 1s 210us/step - loss: 0.0383 - acc: 0.9874 - val_loss: 1.1882 - val_acc: 0.8453\n",
            "Epoch 4/20\n",
            "3900/3900 [==============================] - 1s 201us/step - loss: 0.0290 - acc: 0.9890 - val_loss: 1.4107 - val_acc: 0.8422\n",
            "Epoch 5/20\n",
            "3900/3900 [==============================] - 1s 207us/step - loss: 0.0334 - acc: 0.9895 - val_loss: 1.3355 - val_acc: 0.8449\n",
            "Epoch 6/20\n",
            "3900/3900 [==============================] - 1s 207us/step - loss: 0.0256 - acc: 0.9903 - val_loss: 1.4197 - val_acc: 0.8430\n",
            "Epoch 7/20\n",
            "3900/3900 [==============================] - 1s 205us/step - loss: 0.0292 - acc: 0.9897 - val_loss: 1.3872 - val_acc: 0.8443\n",
            "Epoch 8/20\n",
            "3900/3900 [==============================] - 1s 209us/step - loss: 0.0188 - acc: 0.9928 - val_loss: 1.3700 - val_acc: 0.8462\n",
            "Epoch 9/20\n",
            "3900/3900 [==============================] - 1s 212us/step - loss: 0.0214 - acc: 0.9921 - val_loss: 1.4173 - val_acc: 0.8425\n",
            "Epoch 10/20\n",
            "3900/3900 [==============================] - 1s 209us/step - loss: 0.0195 - acc: 0.9928 - val_loss: 1.3662 - val_acc: 0.8432\n",
            "Epoch 11/20\n",
            "3900/3900 [==============================] - 1s 206us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 1.4324 - val_acc: 0.8456\n",
            "Epoch 12/20\n",
            "3900/3900 [==============================] - 1s 210us/step - loss: 0.0175 - acc: 0.9936 - val_loss: 1.4507 - val_acc: 0.8444\n",
            "Epoch 13/20\n",
            "3900/3900 [==============================] - 1s 204us/step - loss: 0.0187 - acc: 0.9944 - val_loss: 1.4292 - val_acc: 0.8412\n",
            "Epoch 14/20\n",
            "3900/3900 [==============================] - 1s 206us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 1.3668 - val_acc: 0.8453\n",
            "Epoch 15/20\n",
            "3900/3900 [==============================] - 1s 214us/step - loss: 0.0139 - acc: 0.9964 - val_loss: 1.4264 - val_acc: 0.8446\n",
            "Epoch 16/20\n",
            "3900/3900 [==============================] - 1s 205us/step - loss: 0.0173 - acc: 0.9938 - val_loss: 1.4201 - val_acc: 0.8441\n",
            "Epoch 17/20\n",
            "3900/3900 [==============================] - 1s 205us/step - loss: 0.0182 - acc: 0.9936 - val_loss: 1.4565 - val_acc: 0.8451\n",
            "Epoch 18/20\n",
            "3900/3900 [==============================] - 1s 209us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 1.4560 - val_acc: 0.8416\n",
            "Epoch 19/20\n",
            "3900/3900 [==============================] - 1s 214us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 1.4939 - val_acc: 0.8423\n",
            "Epoch 20/20\n",
            "3900/3900 [==============================] - 1s 207us/step - loss: 0.0162 - acc: 0.9941 - val_loss: 1.4183 - val_acc: 0.8444\n",
            "Test loss: 1.418298661056161\n",
            "Test accuracy: 0.8444\n",
            "Train loss: 0.00027053828110638276\n",
            "Train accuracy: 1.0\n",
            "x label: (4000, 28, 28)\n",
            "y label: (4000,)\n",
            "x unlabel: (56000, 28, 28)\n",
            "y unlabel: (56000,)\n",
            "39\n",
            "Train on 4000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4000/4000 [==============================] - 1s 204us/step - loss: 0.0740 - acc: 0.9853 - val_loss: 1.2699 - val_acc: 0.8433\n",
            "Epoch 2/20\n",
            "4000/4000 [==============================] - 1s 200us/step - loss: 0.0455 - acc: 0.9870 - val_loss: 1.2979 - val_acc: 0.8391\n",
            "Epoch 3/20\n",
            "4000/4000 [==============================] - 1s 201us/step - loss: 0.0478 - acc: 0.9900 - val_loss: 1.2483 - val_acc: 0.8458\n",
            "Epoch 4/20\n",
            "4000/4000 [==============================] - 1s 205us/step - loss: 0.0369 - acc: 0.9893 - val_loss: 1.3537 - val_acc: 0.8438\n",
            "Epoch 5/20\n",
            "4000/4000 [==============================] - 1s 207us/step - loss: 0.0309 - acc: 0.9905 - val_loss: 1.3607 - val_acc: 0.8392\n",
            "Epoch 6/20\n",
            "4000/4000 [==============================] - 1s 206us/step - loss: 0.0330 - acc: 0.9915 - val_loss: 1.3268 - val_acc: 0.8485\n",
            "Epoch 7/20\n",
            "4000/4000 [==============================] - 1s 205us/step - loss: 0.0351 - acc: 0.9905 - val_loss: 1.3206 - val_acc: 0.8420\n",
            "Epoch 8/20\n",
            "4000/4000 [==============================] - 1s 200us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 1.3316 - val_acc: 0.8462\n",
            "Epoch 9/20\n",
            "4000/4000 [==============================] - 1s 198us/step - loss: 0.0208 - acc: 0.9918 - val_loss: 1.3370 - val_acc: 0.8441\n",
            "Epoch 10/20\n",
            "4000/4000 [==============================] - 1s 211us/step - loss: 0.0264 - acc: 0.9908 - val_loss: 1.4014 - val_acc: 0.8456\n",
            "Epoch 11/20\n",
            "4000/4000 [==============================] - 1s 205us/step - loss: 0.0181 - acc: 0.9938 - val_loss: 1.3733 - val_acc: 0.8457\n",
            "Epoch 12/20\n",
            "4000/4000 [==============================] - 1s 201us/step - loss: 0.0196 - acc: 0.9945 - val_loss: 1.3690 - val_acc: 0.8425\n",
            "Epoch 13/20\n",
            "4000/4000 [==============================] - 1s 203us/step - loss: 0.0223 - acc: 0.9930 - val_loss: 1.3260 - val_acc: 0.8435\n",
            "Epoch 14/20\n",
            "4000/4000 [==============================] - 1s 210us/step - loss: 0.0155 - acc: 0.9955 - val_loss: 1.3532 - val_acc: 0.8451\n",
            "Epoch 15/20\n",
            "4000/4000 [==============================] - 1s 199us/step - loss: 0.0164 - acc: 0.9943 - val_loss: 1.4436 - val_acc: 0.8425\n",
            "Epoch 16/20\n",
            "4000/4000 [==============================] - 1s 208us/step - loss: 0.0156 - acc: 0.9933 - val_loss: 1.4426 - val_acc: 0.8461\n",
            "Epoch 17/20\n",
            "4000/4000 [==============================] - 1s 205us/step - loss: 0.0115 - acc: 0.9960 - val_loss: 1.4157 - val_acc: 0.8443\n",
            "Epoch 18/20\n",
            "4000/4000 [==============================] - 1s 199us/step - loss: 0.0179 - acc: 0.9935 - val_loss: 1.4860 - val_acc: 0.8429\n",
            "Epoch 19/20\n",
            "4000/4000 [==============================] - 1s 204us/step - loss: 0.0162 - acc: 0.9955 - val_loss: 1.4681 - val_acc: 0.8456\n",
            "Epoch 20/20\n",
            "4000/4000 [==============================] - 1s 207us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 1.4095 - val_acc: 0.8441\n",
            "Test loss: 1.4094724132597447\n",
            "Test accuracy: 0.8441\n",
            "Train loss: 0.00011991490619311662\n",
            "Train accuracy: 1.0\n",
            "x label: (4100, 28, 28)\n",
            "y label: (4100,)\n",
            "x unlabel: (55900, 28, 28)\n",
            "y unlabel: (55900,)\n",
            "40\n",
            "Train on 4100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4100/4100 [==============================] - 1s 207us/step - loss: 0.0603 - acc: 0.9854 - val_loss: 1.4073 - val_acc: 0.8421\n",
            "Epoch 2/20\n",
            "4100/4100 [==============================] - 1s 206us/step - loss: 0.0415 - acc: 0.9885 - val_loss: 1.3700 - val_acc: 0.8448\n",
            "Epoch 3/20\n",
            "4100/4100 [==============================] - 1s 209us/step - loss: 0.0399 - acc: 0.9890 - val_loss: 1.4014 - val_acc: 0.8479\n",
            "Epoch 4/20\n",
            "4100/4100 [==============================] - 1s 215us/step - loss: 0.0308 - acc: 0.9910 - val_loss: 1.3535 - val_acc: 0.8472\n",
            "Epoch 5/20\n",
            "4100/4100 [==============================] - 1s 199us/step - loss: 0.0237 - acc: 0.9917 - val_loss: 1.3648 - val_acc: 0.8463\n",
            "Epoch 6/20\n",
            "4100/4100 [==============================] - 1s 209us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 1.3894 - val_acc: 0.8461\n",
            "Epoch 7/20\n",
            "4100/4100 [==============================] - 1s 209us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 1.4434 - val_acc: 0.8480\n",
            "Epoch 8/20\n",
            "4100/4100 [==============================] - 1s 208us/step - loss: 0.0270 - acc: 0.9900 - val_loss: 1.3757 - val_acc: 0.8447\n",
            "Epoch 9/20\n",
            "4100/4100 [==============================] - 1s 209us/step - loss: 0.0183 - acc: 0.9944 - val_loss: 1.4502 - val_acc: 0.8496\n",
            "Epoch 10/20\n",
            "4100/4100 [==============================] - 1s 214us/step - loss: 0.0271 - acc: 0.9905 - val_loss: 1.3876 - val_acc: 0.8443\n",
            "Epoch 11/20\n",
            "4100/4100 [==============================] - 1s 207us/step - loss: 0.0236 - acc: 0.9937 - val_loss: 1.3724 - val_acc: 0.8458\n",
            "Epoch 12/20\n",
            "4100/4100 [==============================] - 1s 204us/step - loss: 0.0175 - acc: 0.9944 - val_loss: 1.4101 - val_acc: 0.8446\n",
            "Epoch 13/20\n",
            "4100/4100 [==============================] - 1s 202us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 1.4949 - val_acc: 0.8455\n",
            "Epoch 14/20\n",
            "4100/4100 [==============================] - 1s 206us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 1.3949 - val_acc: 0.8436\n",
            "Epoch 15/20\n",
            "4100/4100 [==============================] - 1s 201us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 1.4760 - val_acc: 0.8427\n",
            "Epoch 16/20\n",
            "4100/4100 [==============================] - 1s 203us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 1.4225 - val_acc: 0.8465\n",
            "Epoch 17/20\n",
            "4100/4100 [==============================] - 1s 200us/step - loss: 0.0179 - acc: 0.9941 - val_loss: 1.5300 - val_acc: 0.8470\n",
            "Epoch 18/20\n",
            "4100/4100 [==============================] - 1s 204us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 1.4058 - val_acc: 0.8446\n",
            "Epoch 19/20\n",
            "4100/4100 [==============================] - 1s 205us/step - loss: 0.0164 - acc: 0.9944 - val_loss: 1.4157 - val_acc: 0.8447\n",
            "Epoch 20/20\n",
            "4100/4100 [==============================] - 1s 198us/step - loss: 0.0085 - acc: 0.9966 - val_loss: 1.5289 - val_acc: 0.8442\n",
            "Test loss: 1.5289030599458142\n",
            "Test accuracy: 0.8442\n",
            "Train loss: 3.302516891502952e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (4200, 28, 28)\n",
            "y label: (4200,)\n",
            "x unlabel: (55800, 28, 28)\n",
            "y unlabel: (55800,)\n",
            "41\n",
            "Train on 4200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4200/4200 [==============================] - 1s 217us/step - loss: 0.0601 - acc: 0.9883 - val_loss: 1.2547 - val_acc: 0.8442\n",
            "Epoch 2/20\n",
            "4200/4200 [==============================] - 1s 209us/step - loss: 0.0488 - acc: 0.9862 - val_loss: 1.3236 - val_acc: 0.8433\n",
            "Epoch 3/20\n",
            "4200/4200 [==============================] - 1s 208us/step - loss: 0.0472 - acc: 0.9888 - val_loss: 1.2957 - val_acc: 0.8476\n",
            "Epoch 4/20\n",
            "4200/4200 [==============================] - 1s 213us/step - loss: 0.0399 - acc: 0.9876 - val_loss: 1.2282 - val_acc: 0.8488\n",
            "Epoch 5/20\n",
            "4200/4200 [==============================] - 1s 205us/step - loss: 0.0362 - acc: 0.9888 - val_loss: 1.3766 - val_acc: 0.8493\n",
            "Epoch 6/20\n",
            "4200/4200 [==============================] - 1s 204us/step - loss: 0.0283 - acc: 0.9929 - val_loss: 1.4091 - val_acc: 0.8505\n",
            "Epoch 7/20\n",
            "4200/4200 [==============================] - 1s 201us/step - loss: 0.0261 - acc: 0.9936 - val_loss: 1.3837 - val_acc: 0.8496\n",
            "Epoch 8/20\n",
            "4200/4200 [==============================] - 1s 204us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 1.3734 - val_acc: 0.8447\n",
            "Epoch 9/20\n",
            "4200/4200 [==============================] - 1s 210us/step - loss: 0.0240 - acc: 0.9936 - val_loss: 1.3356 - val_acc: 0.8502\n",
            "Epoch 10/20\n",
            "4200/4200 [==============================] - 1s 206us/step - loss: 0.0219 - acc: 0.9952 - val_loss: 1.3880 - val_acc: 0.8463\n",
            "Epoch 11/20\n",
            "4200/4200 [==============================] - 1s 199us/step - loss: 0.0266 - acc: 0.9938 - val_loss: 1.4398 - val_acc: 0.8485\n",
            "Epoch 12/20\n",
            "4200/4200 [==============================] - 1s 202us/step - loss: 0.0259 - acc: 0.9914 - val_loss: 1.3065 - val_acc: 0.8511\n",
            "Epoch 13/20\n",
            "4200/4200 [==============================] - 1s 193us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 1.4223 - val_acc: 0.8490\n",
            "Epoch 14/20\n",
            "4200/4200 [==============================] - 1s 196us/step - loss: 0.0262 - acc: 0.9933 - val_loss: 1.3957 - val_acc: 0.8496\n",
            "Epoch 15/20\n",
            "4200/4200 [==============================] - 1s 205us/step - loss: 0.0230 - acc: 0.9943 - val_loss: 1.4304 - val_acc: 0.8487\n",
            "Epoch 16/20\n",
            "4200/4200 [==============================] - 1s 192us/step - loss: 0.0158 - acc: 0.9960 - val_loss: 1.4318 - val_acc: 0.8452\n",
            "Epoch 17/20\n",
            "4200/4200 [==============================] - 1s 202us/step - loss: 0.0174 - acc: 0.9943 - val_loss: 1.4435 - val_acc: 0.8463\n",
            "Epoch 18/20\n",
            "4200/4200 [==============================] - 1s 202us/step - loss: 0.0233 - acc: 0.9931 - val_loss: 1.4227 - val_acc: 0.8431\n",
            "Epoch 19/20\n",
            "4200/4200 [==============================] - 1s 206us/step - loss: 0.0211 - acc: 0.9948 - val_loss: 1.3853 - val_acc: 0.8475\n",
            "Epoch 20/20\n",
            "4200/4200 [==============================] - 1s 197us/step - loss: 0.0161 - acc: 0.9957 - val_loss: 1.3387 - val_acc: 0.8505\n",
            "Test loss: 1.3387163176679984\n",
            "Test accuracy: 0.8505\n",
            "Train loss: 0.003721990994213662\n",
            "Train accuracy: 0.9997619047619047\n",
            "x label: (4300, 28, 28)\n",
            "y label: (4300,)\n",
            "x unlabel: (55700, 28, 28)\n",
            "y unlabel: (55700,)\n",
            "42\n",
            "Train on 4300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4300/4300 [==============================] - 1s 201us/step - loss: 0.0663 - acc: 0.9865 - val_loss: 1.3927 - val_acc: 0.8472\n",
            "Epoch 2/20\n",
            "4300/4300 [==============================] - 1s 195us/step - loss: 0.0458 - acc: 0.9874 - val_loss: 1.3224 - val_acc: 0.8449\n",
            "Epoch 3/20\n",
            "4300/4300 [==============================] - 1s 196us/step - loss: 0.0596 - acc: 0.9884 - val_loss: 1.3498 - val_acc: 0.8477\n",
            "Epoch 4/20\n",
            "4300/4300 [==============================] - 1s 203us/step - loss: 0.0476 - acc: 0.9888 - val_loss: 1.2981 - val_acc: 0.8499\n",
            "Epoch 5/20\n",
            "4300/4300 [==============================] - 1s 201us/step - loss: 0.0369 - acc: 0.9916 - val_loss: 1.2233 - val_acc: 0.8448\n",
            "Epoch 6/20\n",
            "4300/4300 [==============================] - 1s 201us/step - loss: 0.0292 - acc: 0.9907 - val_loss: 1.3199 - val_acc: 0.8470\n",
            "Epoch 7/20\n",
            "4300/4300 [==============================] - 1s 200us/step - loss: 0.0311 - acc: 0.9912 - val_loss: 1.3393 - val_acc: 0.8504\n",
            "Epoch 8/20\n",
            "4300/4300 [==============================] - 1s 202us/step - loss: 0.0351 - acc: 0.9912 - val_loss: 1.3173 - val_acc: 0.8502\n",
            "Epoch 9/20\n",
            "4300/4300 [==============================] - 1s 202us/step - loss: 0.0238 - acc: 0.9921 - val_loss: 1.3067 - val_acc: 0.8484\n",
            "Epoch 10/20\n",
            "4300/4300 [==============================] - 1s 196us/step - loss: 0.0275 - acc: 0.9919 - val_loss: 1.4085 - val_acc: 0.8432\n",
            "Epoch 11/20\n",
            "4300/4300 [==============================] - 1s 202us/step - loss: 0.0241 - acc: 0.9921 - val_loss: 1.4021 - val_acc: 0.8485\n",
            "Epoch 12/20\n",
            "4300/4300 [==============================] - 1s 201us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 1.3458 - val_acc: 0.8473\n",
            "Epoch 13/20\n",
            "4300/4300 [==============================] - 1s 200us/step - loss: 0.0272 - acc: 0.9923 - val_loss: 1.4162 - val_acc: 0.8478\n",
            "Epoch 14/20\n",
            "4300/4300 [==============================] - 1s 196us/step - loss: 0.0175 - acc: 0.9940 - val_loss: 1.3482 - val_acc: 0.8458\n",
            "Epoch 15/20\n",
            "4300/4300 [==============================] - 1s 202us/step - loss: 0.0186 - acc: 0.9937 - val_loss: 1.3967 - val_acc: 0.8476\n",
            "Epoch 16/20\n",
            "4300/4300 [==============================] - 1s 194us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 1.3427 - val_acc: 0.8524\n",
            "Epoch 17/20\n",
            "4300/4300 [==============================] - 1s 197us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 1.4560 - val_acc: 0.8476\n",
            "Epoch 18/20\n",
            "4300/4300 [==============================] - 1s 200us/step - loss: 0.0154 - acc: 0.9947 - val_loss: 1.4021 - val_acc: 0.8465\n",
            "Epoch 19/20\n",
            "4300/4300 [==============================] - 1s 195us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 1.3430 - val_acc: 0.8480\n",
            "Epoch 20/20\n",
            "4300/4300 [==============================] - 1s 199us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 1.4488 - val_acc: 0.8487\n",
            "Test loss: 1.4488029544561636\n",
            "Test accuracy: 0.8487\n",
            "Train loss: 0.0004587479195730381\n",
            "Train accuracy: 0.9997674418604651\n",
            "x label: (4400, 28, 28)\n",
            "y label: (4400,)\n",
            "x unlabel: (55600, 28, 28)\n",
            "y unlabel: (55600,)\n",
            "43\n",
            "Train on 4400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4400/4400 [==============================] - 1s 195us/step - loss: 0.0561 - acc: 0.9861 - val_loss: 1.3996 - val_acc: 0.8470\n",
            "Epoch 2/20\n",
            "4400/4400 [==============================] - 1s 201us/step - loss: 0.0612 - acc: 0.9857 - val_loss: 1.2227 - val_acc: 0.8509\n",
            "Epoch 3/20\n",
            "4400/4400 [==============================] - 1s 192us/step - loss: 0.0418 - acc: 0.9882 - val_loss: 1.2623 - val_acc: 0.8490\n",
            "Epoch 4/20\n",
            "4400/4400 [==============================] - 1s 195us/step - loss: 0.0447 - acc: 0.9880 - val_loss: 1.2336 - val_acc: 0.8466\n",
            "Epoch 5/20\n",
            "4400/4400 [==============================] - 1s 200us/step - loss: 0.0345 - acc: 0.9895 - val_loss: 1.2807 - val_acc: 0.8494\n",
            "Epoch 6/20\n",
            "4400/4400 [==============================] - 1s 211us/step - loss: 0.0203 - acc: 0.9930 - val_loss: 1.3635 - val_acc: 0.8492\n",
            "Epoch 7/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0293 - acc: 0.9898 - val_loss: 1.3833 - val_acc: 0.8483\n",
            "Epoch 8/20\n",
            "4400/4400 [==============================] - 1s 200us/step - loss: 0.0252 - acc: 0.9916 - val_loss: 1.3492 - val_acc: 0.8514\n",
            "Epoch 9/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0272 - acc: 0.9939 - val_loss: 1.3074 - val_acc: 0.8530\n",
            "Epoch 10/20\n",
            "4400/4400 [==============================] - 1s 193us/step - loss: 0.0200 - acc: 0.9927 - val_loss: 1.4324 - val_acc: 0.8510\n",
            "Epoch 11/20\n",
            "4400/4400 [==============================] - 1s 201us/step - loss: 0.0236 - acc: 0.9930 - val_loss: 1.3235 - val_acc: 0.8504\n",
            "Epoch 12/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0174 - acc: 0.9943 - val_loss: 1.4200 - val_acc: 0.8517\n",
            "Epoch 13/20\n",
            "4400/4400 [==============================] - 1s 193us/step - loss: 0.0167 - acc: 0.9945 - val_loss: 1.2753 - val_acc: 0.8451\n",
            "Epoch 14/20\n",
            "4400/4400 [==============================] - 1s 198us/step - loss: 0.0188 - acc: 0.9948 - val_loss: 1.3553 - val_acc: 0.8493\n",
            "Epoch 15/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0196 - acc: 0.9939 - val_loss: 1.4018 - val_acc: 0.8496\n",
            "Epoch 16/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 1.4531 - val_acc: 0.8488\n",
            "Epoch 17/20\n",
            "4400/4400 [==============================] - 1s 191us/step - loss: 0.0196 - acc: 0.9943 - val_loss: 1.3684 - val_acc: 0.8482\n",
            "Epoch 18/20\n",
            "4400/4400 [==============================] - 1s 194us/step - loss: 0.0157 - acc: 0.9952 - val_loss: 1.3675 - val_acc: 0.8518\n",
            "Epoch 19/20\n",
            "4400/4400 [==============================] - 1s 193us/step - loss: 0.0146 - acc: 0.9957 - val_loss: 1.3950 - val_acc: 0.8522\n",
            "Epoch 20/20\n",
            "4400/4400 [==============================] - 1s 196us/step - loss: 0.0115 - acc: 0.9964 - val_loss: 1.4689 - val_acc: 0.8508\n",
            "Test loss: 1.4689060816572979\n",
            "Test accuracy: 0.8508\n",
            "Train loss: 0.0001543905554885069\n",
            "Train accuracy: 1.0\n",
            "x label: (4500, 28, 28)\n",
            "y label: (4500,)\n",
            "x unlabel: (55500, 28, 28)\n",
            "y unlabel: (55500,)\n",
            "44\n",
            "Train on 4500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4500/4500 [==============================] - 1s 193us/step - loss: 0.0603 - acc: 0.9858 - val_loss: 1.3291 - val_acc: 0.8527\n",
            "Epoch 2/20\n",
            "4500/4500 [==============================] - 1s 195us/step - loss: 0.0424 - acc: 0.9896 - val_loss: 1.4537 - val_acc: 0.8492\n",
            "Epoch 3/20\n",
            "4500/4500 [==============================] - 1s 194us/step - loss: 0.0314 - acc: 0.9891 - val_loss: 1.2275 - val_acc: 0.8518\n",
            "Epoch 4/20\n",
            "4500/4500 [==============================] - 1s 192us/step - loss: 0.0266 - acc: 0.9902 - val_loss: 1.3410 - val_acc: 0.8503\n",
            "Epoch 5/20\n",
            "4500/4500 [==============================] - 1s 201us/step - loss: 0.0351 - acc: 0.9891 - val_loss: 1.2892 - val_acc: 0.8494\n",
            "Epoch 6/20\n",
            "4500/4500 [==============================] - 1s 190us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 1.2868 - val_acc: 0.8526\n",
            "Epoch 7/20\n",
            "4500/4500 [==============================] - 1s 198us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 1.2831 - val_acc: 0.8529\n",
            "Epoch 8/20\n",
            "4500/4500 [==============================] - 1s 197us/step - loss: 0.0238 - acc: 0.9904 - val_loss: 1.3285 - val_acc: 0.8507\n",
            "Epoch 9/20\n",
            "4500/4500 [==============================] - 1s 195us/step - loss: 0.0225 - acc: 0.9927 - val_loss: 1.3203 - val_acc: 0.8506\n",
            "Epoch 10/20\n",
            "4500/4500 [==============================] - 1s 203us/step - loss: 0.0181 - acc: 0.9942 - val_loss: 1.3478 - val_acc: 0.8470\n",
            "Epoch 11/20\n",
            "4500/4500 [==============================] - 1s 198us/step - loss: 0.0228 - acc: 0.9924 - val_loss: 1.3509 - val_acc: 0.8501\n",
            "Epoch 12/20\n",
            "4500/4500 [==============================] - 1s 192us/step - loss: 0.0172 - acc: 0.9947 - val_loss: 1.3795 - val_acc: 0.8453\n",
            "Epoch 13/20\n",
            "4500/4500 [==============================] - 1s 192us/step - loss: 0.0196 - acc: 0.9936 - val_loss: 1.3649 - val_acc: 0.8513\n",
            "Epoch 14/20\n",
            "4500/4500 [==============================] - 1s 190us/step - loss: 0.0208 - acc: 0.9920 - val_loss: 1.3385 - val_acc: 0.8485\n",
            "Epoch 15/20\n",
            "4500/4500 [==============================] - 1s 196us/step - loss: 0.0176 - acc: 0.9924 - val_loss: 1.3563 - val_acc: 0.8541\n",
            "Epoch 16/20\n",
            "4500/4500 [==============================] - 1s 195us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 1.3269 - val_acc: 0.8543\n",
            "Epoch 17/20\n",
            "4500/4500 [==============================] - 1s 196us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 1.3086 - val_acc: 0.8518\n",
            "Epoch 18/20\n",
            "4500/4500 [==============================] - 1s 200us/step - loss: 0.0155 - acc: 0.9949 - val_loss: 1.3398 - val_acc: 0.8529\n",
            "Epoch 19/20\n",
            "4500/4500 [==============================] - 1s 190us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 1.3927 - val_acc: 0.8519\n",
            "Epoch 20/20\n",
            "4500/4500 [==============================] - 1s 198us/step - loss: 0.0155 - acc: 0.9951 - val_loss: 1.3902 - val_acc: 0.8543\n",
            "Test loss: 1.390173495992372\n",
            "Test accuracy: 0.8543\n",
            "Train loss: 6.306554885052012e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (4600, 28, 28)\n",
            "y label: (4600,)\n",
            "x unlabel: (55400, 28, 28)\n",
            "y unlabel: (55400,)\n",
            "45\n",
            "Train on 4600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4600/4600 [==============================] - 1s 197us/step - loss: 0.0603 - acc: 0.9850 - val_loss: 1.1871 - val_acc: 0.8507\n",
            "Epoch 2/20\n",
            "4600/4600 [==============================] - 1s 195us/step - loss: 0.0462 - acc: 0.9872 - val_loss: 1.2421 - val_acc: 0.8554\n",
            "Epoch 3/20\n",
            "4600/4600 [==============================] - 1s 188us/step - loss: 0.0367 - acc: 0.9893 - val_loss: 1.2737 - val_acc: 0.8590\n",
            "Epoch 4/20\n",
            "4600/4600 [==============================] - 1s 193us/step - loss: 0.0333 - acc: 0.9898 - val_loss: 1.2347 - val_acc: 0.8581\n",
            "Epoch 5/20\n",
            "4600/4600 [==============================] - 1s 193us/step - loss: 0.0356 - acc: 0.9911 - val_loss: 1.1983 - val_acc: 0.8550\n",
            "Epoch 6/20\n",
            "4600/4600 [==============================] - 1s 194us/step - loss: 0.0194 - acc: 0.9939 - val_loss: 1.1408 - val_acc: 0.8550\n",
            "Epoch 7/20\n",
            "4600/4600 [==============================] - 1s 195us/step - loss: 0.0235 - acc: 0.9939 - val_loss: 1.1956 - val_acc: 0.8563\n",
            "Epoch 8/20\n",
            "4600/4600 [==============================] - 1s 192us/step - loss: 0.0281 - acc: 0.9917 - val_loss: 1.2321 - val_acc: 0.8546\n",
            "Epoch 9/20\n",
            "4600/4600 [==============================] - 1s 187us/step - loss: 0.0238 - acc: 0.9924 - val_loss: 1.2735 - val_acc: 0.8543\n",
            "Epoch 10/20\n",
            "4600/4600 [==============================] - 1s 197us/step - loss: 0.0220 - acc: 0.9937 - val_loss: 1.2679 - val_acc: 0.8540\n",
            "Epoch 11/20\n",
            "4600/4600 [==============================] - 1s 192us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 1.3386 - val_acc: 0.8552\n",
            "Epoch 12/20\n",
            "4600/4600 [==============================] - 1s 197us/step - loss: 0.0225 - acc: 0.9926 - val_loss: 1.3541 - val_acc: 0.8543\n",
            "Epoch 13/20\n",
            "4600/4600 [==============================] - 1s 191us/step - loss: 0.0148 - acc: 0.9941 - val_loss: 1.3370 - val_acc: 0.8587\n",
            "Epoch 14/20\n",
            "4600/4600 [==============================] - 1s 190us/step - loss: 0.0257 - acc: 0.9924 - val_loss: 1.3051 - val_acc: 0.8549\n",
            "Epoch 15/20\n",
            "4600/4600 [==============================] - 1s 191us/step - loss: 0.0186 - acc: 0.9946 - val_loss: 1.3286 - val_acc: 0.8557\n",
            "Epoch 16/20\n",
            "4600/4600 [==============================] - 1s 187us/step - loss: 0.0181 - acc: 0.9939 - val_loss: 1.3798 - val_acc: 0.8516\n",
            "Epoch 17/20\n",
            "4600/4600 [==============================] - 1s 192us/step - loss: 0.0170 - acc: 0.9950 - val_loss: 1.2841 - val_acc: 0.8545\n",
            "Epoch 18/20\n",
            "4600/4600 [==============================] - 1s 194us/step - loss: 0.0158 - acc: 0.9943 - val_loss: 1.3077 - val_acc: 0.8541\n",
            "Epoch 19/20\n",
            "4600/4600 [==============================] - 1s 195us/step - loss: 0.0108 - acc: 0.9957 - val_loss: 1.3584 - val_acc: 0.8549\n",
            "Epoch 20/20\n",
            "4600/4600 [==============================] - 1s 197us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 1.3711 - val_acc: 0.8550\n",
            "Test loss: 1.3711476194368093\n",
            "Test accuracy: 0.855\n",
            "Train loss: 9.103998832721988e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (4700, 28, 28)\n",
            "y label: (4700,)\n",
            "x unlabel: (55300, 28, 28)\n",
            "y unlabel: (55300,)\n",
            "46\n",
            "Train on 4700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4700/4700 [==============================] - 1s 189us/step - loss: 0.0382 - acc: 0.9913 - val_loss: 1.2959 - val_acc: 0.8550\n",
            "Epoch 2/20\n",
            "4700/4700 [==============================] - 1s 194us/step - loss: 0.0403 - acc: 0.9881 - val_loss: 1.2970 - val_acc: 0.8516\n",
            "Epoch 3/20\n",
            "4700/4700 [==============================] - 1s 196us/step - loss: 0.0365 - acc: 0.9891 - val_loss: 1.1970 - val_acc: 0.8558\n",
            "Epoch 4/20\n",
            "4700/4700 [==============================] - 1s 189us/step - loss: 0.0426 - acc: 0.9866 - val_loss: 1.2065 - val_acc: 0.8540\n",
            "Epoch 5/20\n",
            "4700/4700 [==============================] - 1s 187us/step - loss: 0.0224 - acc: 0.9934 - val_loss: 1.2850 - val_acc: 0.8555\n",
            "Epoch 6/20\n",
            "4700/4700 [==============================] - 1s 187us/step - loss: 0.0341 - acc: 0.9909 - val_loss: 1.1881 - val_acc: 0.8560\n",
            "Epoch 7/20\n",
            "4700/4700 [==============================] - 1s 188us/step - loss: 0.0242 - acc: 0.9926 - val_loss: 1.1337 - val_acc: 0.8567\n",
            "Epoch 8/20\n",
            "4700/4700 [==============================] - 1s 193us/step - loss: 0.0198 - acc: 0.9932 - val_loss: 1.3327 - val_acc: 0.8556\n",
            "Epoch 9/20\n",
            "4700/4700 [==============================] - 1s 195us/step - loss: 0.0259 - acc: 0.9936 - val_loss: 1.2459 - val_acc: 0.8550\n",
            "Epoch 10/20\n",
            "4700/4700 [==============================] - 1s 192us/step - loss: 0.0170 - acc: 0.9943 - val_loss: 1.2043 - val_acc: 0.8528\n",
            "Epoch 11/20\n",
            "4700/4700 [==============================] - 1s 193us/step - loss: 0.0221 - acc: 0.9926 - val_loss: 1.2888 - val_acc: 0.8541\n",
            "Epoch 12/20\n",
            "4700/4700 [==============================] - 1s 187us/step - loss: 0.0146 - acc: 0.9940 - val_loss: 1.2871 - val_acc: 0.8532\n",
            "Epoch 13/20\n",
            "4700/4700 [==============================] - 1s 188us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 1.3593 - val_acc: 0.8554\n",
            "Epoch 14/20\n",
            "4700/4700 [==============================] - 1s 192us/step - loss: 0.0202 - acc: 0.9945 - val_loss: 1.3160 - val_acc: 0.8548\n",
            "Epoch 15/20\n",
            "4700/4700 [==============================] - 1s 193us/step - loss: 0.0112 - acc: 0.9966 - val_loss: 1.3602 - val_acc: 0.8540\n",
            "Epoch 16/20\n",
            "4700/4700 [==============================] - 1s 191us/step - loss: 0.0161 - acc: 0.9947 - val_loss: 1.3619 - val_acc: 0.8519\n",
            "Epoch 17/20\n",
            "4700/4700 [==============================] - 1s 185us/step - loss: 0.0095 - acc: 0.9966 - val_loss: 1.3922 - val_acc: 0.8564\n",
            "Epoch 18/20\n",
            "4700/4700 [==============================] - 1s 187us/step - loss: 0.0175 - acc: 0.9945 - val_loss: 1.4097 - val_acc: 0.8539\n",
            "Epoch 19/20\n",
            "4700/4700 [==============================] - 1s 192us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 1.3087 - val_acc: 0.8559\n",
            "Epoch 20/20\n",
            "4700/4700 [==============================] - 1s 190us/step - loss: 0.0206 - acc: 0.9936 - val_loss: 1.3798 - val_acc: 0.8566\n",
            "Test loss: 1.3798485822646995\n",
            "Test accuracy: 0.8566\n",
            "Train loss: 5.6678517640319384e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (4800, 28, 28)\n",
            "y label: (4800,)\n",
            "x unlabel: (55200, 28, 28)\n",
            "y unlabel: (55200,)\n",
            "47\n",
            "Train on 4800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4800/4800 [==============================] - 1s 192us/step - loss: 0.0503 - acc: 0.9879 - val_loss: 1.2291 - val_acc: 0.8555\n",
            "Epoch 2/20\n",
            "4800/4800 [==============================] - 1s 190us/step - loss: 0.0498 - acc: 0.9881 - val_loss: 1.1257 - val_acc: 0.8629\n",
            "Epoch 3/20\n",
            "4800/4800 [==============================] - 1s 182us/step - loss: 0.0404 - acc: 0.9898 - val_loss: 1.0926 - val_acc: 0.8570\n",
            "Epoch 4/20\n",
            "4800/4800 [==============================] - 1s 189us/step - loss: 0.0356 - acc: 0.9881 - val_loss: 1.1762 - val_acc: 0.8604\n",
            "Epoch 5/20\n",
            "4800/4800 [==============================] - 1s 189us/step - loss: 0.0279 - acc: 0.9923 - val_loss: 1.2108 - val_acc: 0.8573\n",
            "Epoch 6/20\n",
            "4800/4800 [==============================] - 1s 192us/step - loss: 0.0250 - acc: 0.9917 - val_loss: 1.2039 - val_acc: 0.8606\n",
            "Epoch 7/20\n",
            "4800/4800 [==============================] - 1s 188us/step - loss: 0.0247 - acc: 0.9923 - val_loss: 1.2083 - val_acc: 0.8607\n",
            "Epoch 8/20\n",
            "4800/4800 [==============================] - 1s 193us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 1.1496 - val_acc: 0.8604\n",
            "Epoch 9/20\n",
            "4800/4800 [==============================] - 1s 197us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 1.2612 - val_acc: 0.8576\n",
            "Epoch 10/20\n",
            "4800/4800 [==============================] - 1s 193us/step - loss: 0.0183 - acc: 0.9944 - val_loss: 1.2702 - val_acc: 0.8576\n",
            "Epoch 11/20\n",
            "4800/4800 [==============================] - 1s 186us/step - loss: 0.0166 - acc: 0.9944 - val_loss: 1.1608 - val_acc: 0.8598\n",
            "Epoch 12/20\n",
            "4800/4800 [==============================] - 1s 191us/step - loss: 0.0164 - acc: 0.9948 - val_loss: 1.2702 - val_acc: 0.8558\n",
            "Epoch 13/20\n",
            "4800/4800 [==============================] - 1s 185us/step - loss: 0.0164 - acc: 0.9942 - val_loss: 1.2439 - val_acc: 0.8574\n",
            "Epoch 14/20\n",
            "4800/4800 [==============================] - 1s 193us/step - loss: 0.0168 - acc: 0.9938 - val_loss: 1.2527 - val_acc: 0.8595\n",
            "Epoch 15/20\n",
            "4800/4800 [==============================] - 1s 186us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 1.2302 - val_acc: 0.8575\n",
            "Epoch 16/20\n",
            "4800/4800 [==============================] - 1s 187us/step - loss: 0.0172 - acc: 0.9948 - val_loss: 1.2740 - val_acc: 0.8552\n",
            "Epoch 17/20\n",
            "4800/4800 [==============================] - 1s 184us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 1.3126 - val_acc: 0.8567\n",
            "Epoch 18/20\n",
            "4800/4800 [==============================] - 1s 190us/step - loss: 0.0143 - acc: 0.9950 - val_loss: 1.4054 - val_acc: 0.8551\n",
            "Epoch 19/20\n",
            "4800/4800 [==============================] - 1s 186us/step - loss: 0.0189 - acc: 0.9942 - val_loss: 1.2432 - val_acc: 0.8592\n",
            "Epoch 20/20\n",
            "4800/4800 [==============================] - 1s 193us/step - loss: 0.0171 - acc: 0.9942 - val_loss: 1.2726 - val_acc: 0.8569\n",
            "Test loss: 1.2725575817964971\n",
            "Test accuracy: 0.8569\n",
            "Train loss: 7.779652289523635e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (4900, 28, 28)\n",
            "y label: (4900,)\n",
            "x unlabel: (55100, 28, 28)\n",
            "y unlabel: (55100,)\n",
            "48\n",
            "Train on 4900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "4900/4900 [==============================] - 1s 192us/step - loss: 0.0560 - acc: 0.9880 - val_loss: 1.1714 - val_acc: 0.8490\n",
            "Epoch 2/20\n",
            "4900/4900 [==============================] - 1s 187us/step - loss: 0.0325 - acc: 0.9904 - val_loss: 1.2065 - val_acc: 0.8602\n",
            "Epoch 3/20\n",
            "4900/4900 [==============================] - 1s 189us/step - loss: 0.0422 - acc: 0.9900 - val_loss: 1.0503 - val_acc: 0.8595\n",
            "Epoch 4/20\n",
            "4900/4900 [==============================] - 1s 191us/step - loss: 0.0343 - acc: 0.9888 - val_loss: 1.1855 - val_acc: 0.8583\n",
            "Epoch 5/20\n",
            "4900/4900 [==============================] - 1s 184us/step - loss: 0.0262 - acc: 0.9902 - val_loss: 1.2469 - val_acc: 0.8606\n",
            "Epoch 6/20\n",
            "4900/4900 [==============================] - 1s 188us/step - loss: 0.0289 - acc: 0.9916 - val_loss: 1.2217 - val_acc: 0.8585\n",
            "Epoch 7/20\n",
            "4900/4900 [==============================] - 1s 193us/step - loss: 0.0202 - acc: 0.9937 - val_loss: 1.2428 - val_acc: 0.8571\n",
            "Epoch 8/20\n",
            "4900/4900 [==============================] - 1s 184us/step - loss: 0.0256 - acc: 0.9920 - val_loss: 1.1621 - val_acc: 0.8616\n",
            "Epoch 9/20\n",
            "4900/4900 [==============================] - 1s 187us/step - loss: 0.0164 - acc: 0.9935 - val_loss: 1.2310 - val_acc: 0.8584\n",
            "Epoch 10/20\n",
            "4900/4900 [==============================] - 1s 188us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 1.2465 - val_acc: 0.8601\n",
            "Epoch 11/20\n",
            "4900/4900 [==============================] - 1s 188us/step - loss: 0.0170 - acc: 0.9922 - val_loss: 1.2905 - val_acc: 0.8601\n",
            "Epoch 12/20\n",
            "4900/4900 [==============================] - 1s 192us/step - loss: 0.0179 - acc: 0.9937 - val_loss: 1.2972 - val_acc: 0.8591\n",
            "Epoch 13/20\n",
            "4900/4900 [==============================] - 1s 190us/step - loss: 0.0203 - acc: 0.9939 - val_loss: 1.2713 - val_acc: 0.8605\n",
            "Epoch 14/20\n",
            "4900/4900 [==============================] - 1s 184us/step - loss: 0.0211 - acc: 0.9937 - val_loss: 1.2183 - val_acc: 0.8574\n",
            "Epoch 15/20\n",
            "4900/4900 [==============================] - 1s 185us/step - loss: 0.0192 - acc: 0.9935 - val_loss: 1.2562 - val_acc: 0.8604\n",
            "Epoch 16/20\n",
            "4900/4900 [==============================] - 1s 186us/step - loss: 0.0170 - acc: 0.9947 - val_loss: 1.2157 - val_acc: 0.8596\n",
            "Epoch 17/20\n",
            "4900/4900 [==============================] - 1s 186us/step - loss: 0.0155 - acc: 0.9945 - val_loss: 1.3038 - val_acc: 0.8579\n",
            "Epoch 18/20\n",
            "4900/4900 [==============================] - 1s 192us/step - loss: 0.0164 - acc: 0.9949 - val_loss: 1.2009 - val_acc: 0.8600\n",
            "Epoch 19/20\n",
            "4900/4900 [==============================] - 1s 184us/step - loss: 0.0157 - acc: 0.9957 - val_loss: 1.2802 - val_acc: 0.8585\n",
            "Epoch 20/20\n",
            "4900/4900 [==============================] - 1s 184us/step - loss: 0.0143 - acc: 0.9949 - val_loss: 1.2996 - val_acc: 0.8592\n",
            "Test loss: 1.2995631950378723\n",
            "Test accuracy: 0.8592\n",
            "Train loss: 0.0001543064575942629\n",
            "Train accuracy: 1.0\n",
            "x label: (5000, 28, 28)\n",
            "y label: (5000,)\n",
            "x unlabel: (55000, 28, 28)\n",
            "y unlabel: (55000,)\n",
            "49\n",
            "Train on 5000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5000/5000 [==============================] - 1s 183us/step - loss: 0.0607 - acc: 0.9848 - val_loss: 1.1218 - val_acc: 0.8610\n",
            "Epoch 2/20\n",
            "5000/5000 [==============================] - 1s 187us/step - loss: 0.0378 - acc: 0.9878 - val_loss: 1.1583 - val_acc: 0.8598\n",
            "Epoch 3/20\n",
            "5000/5000 [==============================] - 1s 189us/step - loss: 0.0357 - acc: 0.9900 - val_loss: 1.1851 - val_acc: 0.8583\n",
            "Epoch 4/20\n",
            "5000/5000 [==============================] - 1s 189us/step - loss: 0.0282 - acc: 0.9926 - val_loss: 1.2742 - val_acc: 0.8561\n",
            "Epoch 5/20\n",
            "5000/5000 [==============================] - 1s 190us/step - loss: 0.0317 - acc: 0.9906 - val_loss: 1.2308 - val_acc: 0.8608\n",
            "Epoch 6/20\n",
            "5000/5000 [==============================] - 1s 187us/step - loss: 0.0297 - acc: 0.9916 - val_loss: 1.2667 - val_acc: 0.8581\n",
            "Epoch 7/20\n",
            "5000/5000 [==============================] - 1s 183us/step - loss: 0.0270 - acc: 0.9910 - val_loss: 1.1918 - val_acc: 0.8578\n",
            "Epoch 8/20\n",
            "5000/5000 [==============================] - 1s 186us/step - loss: 0.0251 - acc: 0.9918 - val_loss: 1.2659 - val_acc: 0.8564\n",
            "Epoch 9/20\n",
            "5000/5000 [==============================] - 1s 179us/step - loss: 0.0184 - acc: 0.9934 - val_loss: 1.3274 - val_acc: 0.8573\n",
            "Epoch 10/20\n",
            "5000/5000 [==============================] - 1s 182us/step - loss: 0.0303 - acc: 0.9914 - val_loss: 1.2594 - val_acc: 0.8583\n",
            "Epoch 11/20\n",
            "5000/5000 [==============================] - 1s 187us/step - loss: 0.0187 - acc: 0.9920 - val_loss: 1.1556 - val_acc: 0.8611\n",
            "Epoch 12/20\n",
            "5000/5000 [==============================] - 1s 184us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 1.2149 - val_acc: 0.8593\n",
            "Epoch 13/20\n",
            "5000/5000 [==============================] - 1s 187us/step - loss: 0.0234 - acc: 0.9918 - val_loss: 1.1717 - val_acc: 0.8599\n",
            "Epoch 14/20\n",
            "5000/5000 [==============================] - 1s 189us/step - loss: 0.0164 - acc: 0.9940 - val_loss: 1.2541 - val_acc: 0.8593\n",
            "Epoch 15/20\n",
            "5000/5000 [==============================] - 1s 188us/step - loss: 0.0214 - acc: 0.9928 - val_loss: 1.2742 - val_acc: 0.8576\n",
            "Epoch 16/20\n",
            "5000/5000 [==============================] - 1s 188us/step - loss: 0.0214 - acc: 0.9932 - val_loss: 1.2479 - val_acc: 0.8558\n",
            "Epoch 17/20\n",
            "5000/5000 [==============================] - 1s 186us/step - loss: 0.0146 - acc: 0.9956 - val_loss: 1.3368 - val_acc: 0.8602\n",
            "Epoch 18/20\n",
            "5000/5000 [==============================] - 1s 188us/step - loss: 0.0211 - acc: 0.9928 - val_loss: 1.2499 - val_acc: 0.8585\n",
            "Epoch 19/20\n",
            "5000/5000 [==============================] - 1s 182us/step - loss: 0.0155 - acc: 0.9946 - val_loss: 1.2434 - val_acc: 0.8608\n",
            "Epoch 20/20\n",
            "5000/5000 [==============================] - 1s 188us/step - loss: 0.0149 - acc: 0.9958 - val_loss: 1.3443 - val_acc: 0.8597\n",
            "Test loss: 1.3443113404679534\n",
            "Test accuracy: 0.8597\n",
            "Train loss: 0.00010418037731451476\n",
            "Train accuracy: 1.0\n",
            "x label: (5100, 28, 28)\n",
            "y label: (5100,)\n",
            "x unlabel: (54900, 28, 28)\n",
            "y unlabel: (54900,)\n",
            "50\n",
            "Train on 5100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5100/5100 [==============================] - 1s 187us/step - loss: 0.0442 - acc: 0.9878 - val_loss: 1.2324 - val_acc: 0.8572\n",
            "Epoch 2/20\n",
            "5100/5100 [==============================] - 1s 181us/step - loss: 0.0333 - acc: 0.9904 - val_loss: 1.2081 - val_acc: 0.8596\n",
            "Epoch 3/20\n",
            "5100/5100 [==============================] - 1s 187us/step - loss: 0.0405 - acc: 0.9906 - val_loss: 1.1582 - val_acc: 0.8576\n",
            "Epoch 4/20\n",
            "5100/5100 [==============================] - 1s 181us/step - loss: 0.0303 - acc: 0.9916 - val_loss: 1.1927 - val_acc: 0.8634\n",
            "Epoch 5/20\n",
            "5100/5100 [==============================] - 1s 181us/step - loss: 0.0231 - acc: 0.9933 - val_loss: 1.1815 - val_acc: 0.8623\n",
            "Epoch 6/20\n",
            "5100/5100 [==============================] - 1s 186us/step - loss: 0.0312 - acc: 0.9924 - val_loss: 1.2383 - val_acc: 0.8599\n",
            "Epoch 7/20\n",
            "5100/5100 [==============================] - 1s 184us/step - loss: 0.0248 - acc: 0.9927 - val_loss: 1.1968 - val_acc: 0.8613\n",
            "Epoch 8/20\n",
            "5100/5100 [==============================] - 1s 187us/step - loss: 0.0236 - acc: 0.9925 - val_loss: 1.2100 - val_acc: 0.8641\n",
            "Epoch 9/20\n",
            "5100/5100 [==============================] - 1s 180us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 1.0852 - val_acc: 0.8664\n",
            "Epoch 10/20\n",
            "5100/5100 [==============================] - 1s 180us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 1.1952 - val_acc: 0.8639\n",
            "Epoch 11/20\n",
            "5100/5100 [==============================] - 1s 184us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 1.2779 - val_acc: 0.8646\n",
            "Epoch 12/20\n",
            "5100/5100 [==============================] - 1s 182us/step - loss: 0.0199 - acc: 0.9945 - val_loss: 1.2199 - val_acc: 0.8639\n",
            "Epoch 13/20\n",
            "5100/5100 [==============================] - 1s 183us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 1.2107 - val_acc: 0.8639\n",
            "Epoch 14/20\n",
            "5100/5100 [==============================] - 1s 179us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 1.2132 - val_acc: 0.8682\n",
            "Epoch 15/20\n",
            "5100/5100 [==============================] - 1s 184us/step - loss: 0.0291 - acc: 0.9931 - val_loss: 1.2223 - val_acc: 0.8628\n",
            "Epoch 16/20\n",
            "5100/5100 [==============================] - 1s 189us/step - loss: 0.0307 - acc: 0.9900 - val_loss: 1.2605 - val_acc: 0.8626\n",
            "Epoch 17/20\n",
            "5100/5100 [==============================] - 1s 185us/step - loss: 0.0240 - acc: 0.9935 - val_loss: 1.2886 - val_acc: 0.8602\n",
            "Epoch 18/20\n",
            "5100/5100 [==============================] - 1s 177us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 1.2621 - val_acc: 0.8635\n",
            "Epoch 19/20\n",
            "5100/5100 [==============================] - 1s 186us/step - loss: 0.0196 - acc: 0.9935 - val_loss: 1.2575 - val_acc: 0.8639\n",
            "Epoch 20/20\n",
            "5100/5100 [==============================] - 1s 182us/step - loss: 0.0226 - acc: 0.9937 - val_loss: 1.2168 - val_acc: 0.8641\n",
            "Test loss: 1.2167989904726826\n",
            "Test accuracy: 0.8641\n",
            "Train loss: 0.003264228826270732\n",
            "Train accuracy: 0.9998039215686274\n",
            "x label: (5200, 28, 28)\n",
            "y label: (5200,)\n",
            "x unlabel: (54800, 28, 28)\n",
            "y unlabel: (54800,)\n",
            "51\n",
            "Train on 5200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5200/5200 [==============================] - 1s 188us/step - loss: 0.0605 - acc: 0.9862 - val_loss: 1.0934 - val_acc: 0.8651\n",
            "Epoch 2/20\n",
            "5200/5200 [==============================] - 1s 183us/step - loss: 0.0392 - acc: 0.9902 - val_loss: 1.1234 - val_acc: 0.8638\n",
            "Epoch 3/20\n",
            "5200/5200 [==============================] - 1s 185us/step - loss: 0.0370 - acc: 0.9906 - val_loss: 1.1419 - val_acc: 0.8637\n",
            "Epoch 4/20\n",
            "5200/5200 [==============================] - 1s 182us/step - loss: 0.0306 - acc: 0.9925 - val_loss: 1.3062 - val_acc: 0.8611\n",
            "Epoch 5/20\n",
            "5200/5200 [==============================] - 1s 181us/step - loss: 0.0333 - acc: 0.9913 - val_loss: 1.1521 - val_acc: 0.8654\n",
            "Epoch 6/20\n",
            "5200/5200 [==============================] - 1s 180us/step - loss: 0.0296 - acc: 0.9910 - val_loss: 1.1012 - val_acc: 0.8607\n",
            "Epoch 7/20\n",
            "5200/5200 [==============================] - 1s 182us/step - loss: 0.0310 - acc: 0.9933 - val_loss: 1.1598 - val_acc: 0.8633\n",
            "Epoch 8/20\n",
            "5200/5200 [==============================] - 1s 181us/step - loss: 0.0294 - acc: 0.9917 - val_loss: 1.1577 - val_acc: 0.8591\n",
            "Epoch 9/20\n",
            "5200/5200 [==============================] - 1s 184us/step - loss: 0.0268 - acc: 0.9929 - val_loss: 1.2683 - val_acc: 0.8633\n",
            "Epoch 10/20\n",
            "5200/5200 [==============================] - 1s 182us/step - loss: 0.0264 - acc: 0.9940 - val_loss: 1.1773 - val_acc: 0.8638\n",
            "Epoch 11/20\n",
            "5200/5200 [==============================] - 1s 186us/step - loss: 0.0188 - acc: 0.9940 - val_loss: 1.1596 - val_acc: 0.8654\n",
            "Epoch 12/20\n",
            "5200/5200 [==============================] - 1s 182us/step - loss: 0.0260 - acc: 0.9904 - val_loss: 1.2085 - val_acc: 0.8618\n",
            "Epoch 13/20\n",
            "5200/5200 [==============================] - 1s 183us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 1.2104 - val_acc: 0.8652\n",
            "Epoch 14/20\n",
            "5200/5200 [==============================] - 1s 192us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 1.2195 - val_acc: 0.8625\n",
            "Epoch 15/20\n",
            "5200/5200 [==============================] - 1s 181us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 1.2128 - val_acc: 0.8639\n",
            "Epoch 16/20\n",
            "5200/5200 [==============================] - 1s 185us/step - loss: 0.0192 - acc: 0.9946 - val_loss: 1.2179 - val_acc: 0.8614\n",
            "Epoch 17/20\n",
            "5200/5200 [==============================] - 1s 184us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 1.2497 - val_acc: 0.8629\n",
            "Epoch 18/20\n",
            "5200/5200 [==============================] - 1s 184us/step - loss: 0.0179 - acc: 0.9940 - val_loss: 1.2570 - val_acc: 0.8645\n",
            "Epoch 19/20\n",
            "5200/5200 [==============================] - 1s 184us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 1.2761 - val_acc: 0.8623\n",
            "Epoch 20/20\n",
            "5200/5200 [==============================] - 1s 182us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 1.3536 - val_acc: 0.8576\n",
            "Test loss: 1.3536101943975634\n",
            "Test accuracy: 0.8576\n",
            "Train loss: 0.0025857773681081235\n",
            "Train accuracy: 0.9998076923076923\n",
            "x label: (5300, 28, 28)\n",
            "y label: (5300,)\n",
            "x unlabel: (54700, 28, 28)\n",
            "y unlabel: (54700,)\n",
            "52\n",
            "Train on 5300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5300/5300 [==============================] - 1s 184us/step - loss: 0.0684 - acc: 0.9858 - val_loss: 1.2338 - val_acc: 0.8624\n",
            "Epoch 2/20\n",
            "5300/5300 [==============================] - 1s 179us/step - loss: 0.0454 - acc: 0.9883 - val_loss: 1.1249 - val_acc: 0.8618\n",
            "Epoch 3/20\n",
            "5300/5300 [==============================] - 1s 176us/step - loss: 0.0361 - acc: 0.9911 - val_loss: 1.2829 - val_acc: 0.8609\n",
            "Epoch 4/20\n",
            "5300/5300 [==============================] - 1s 176us/step - loss: 0.0350 - acc: 0.9911 - val_loss: 1.2565 - val_acc: 0.8610\n",
            "Epoch 5/20\n",
            "5300/5300 [==============================] - 1s 182us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 1.1122 - val_acc: 0.8653\n",
            "Epoch 6/20\n",
            "5300/5300 [==============================] - 1s 180us/step - loss: 0.0300 - acc: 0.9925 - val_loss: 1.1643 - val_acc: 0.8639\n",
            "Epoch 7/20\n",
            "5300/5300 [==============================] - 1s 184us/step - loss: 0.0218 - acc: 0.9936 - val_loss: 1.2611 - val_acc: 0.8623\n",
            "Epoch 8/20\n",
            "5300/5300 [==============================] - 1s 176us/step - loss: 0.0336 - acc: 0.9908 - val_loss: 1.2390 - val_acc: 0.8648\n",
            "Epoch 9/20\n",
            "5300/5300 [==============================] - 1s 185us/step - loss: 0.0214 - acc: 0.9942 - val_loss: 1.2682 - val_acc: 0.8585\n",
            "Epoch 10/20\n",
            "5300/5300 [==============================] - 1s 178us/step - loss: 0.0254 - acc: 0.9921 - val_loss: 1.2956 - val_acc: 0.8596\n",
            "Epoch 11/20\n",
            "5300/5300 [==============================] - 1s 176us/step - loss: 0.0244 - acc: 0.9911 - val_loss: 1.1750 - val_acc: 0.8631\n",
            "Epoch 12/20\n",
            "5300/5300 [==============================] - 1s 183us/step - loss: 0.0169 - acc: 0.9945 - val_loss: 1.2617 - val_acc: 0.8639\n",
            "Epoch 13/20\n",
            "5300/5300 [==============================] - 1s 181us/step - loss: 0.0260 - acc: 0.9926 - val_loss: 1.2383 - val_acc: 0.8628\n",
            "Epoch 14/20\n",
            "5300/5300 [==============================] - 1s 181us/step - loss: 0.0270 - acc: 0.9934 - val_loss: 1.1732 - val_acc: 0.8636\n",
            "Epoch 15/20\n",
            "5300/5300 [==============================] - 1s 181us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 1.2487 - val_acc: 0.8670\n",
            "Epoch 16/20\n",
            "5300/5300 [==============================] - 1s 182us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 1.1724 - val_acc: 0.8643\n",
            "Epoch 17/20\n",
            "5300/5300 [==============================] - 1s 180us/step - loss: 0.0154 - acc: 0.9955 - val_loss: 1.2962 - val_acc: 0.8591\n",
            "Epoch 18/20\n",
            "5300/5300 [==============================] - 1s 176us/step - loss: 0.0227 - acc: 0.9938 - val_loss: 1.3276 - val_acc: 0.8589\n",
            "Epoch 19/20\n",
            "5300/5300 [==============================] - 1s 186us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 1.2466 - val_acc: 0.8628\n",
            "Epoch 20/20\n",
            "5300/5300 [==============================] - 1s 182us/step - loss: 0.0178 - acc: 0.9945 - val_loss: 1.1789 - val_acc: 0.8618\n",
            "Test loss: 1.1788692415358615\n",
            "Test accuracy: 0.8618\n",
            "Train loss: 0.00022135841694625067\n",
            "Train accuracy: 1.0\n",
            "x label: (5400, 28, 28)\n",
            "y label: (5400,)\n",
            "x unlabel: (54600, 28, 28)\n",
            "y unlabel: (54600,)\n",
            "53\n",
            "Train on 5400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5400/5400 [==============================] - 1s 181us/step - loss: 0.0663 - acc: 0.9861 - val_loss: 1.0954 - val_acc: 0.8644\n",
            "Epoch 2/20\n",
            "5400/5400 [==============================] - 1s 182us/step - loss: 0.0323 - acc: 0.9904 - val_loss: 1.1534 - val_acc: 0.8632\n",
            "Epoch 3/20\n",
            "5400/5400 [==============================] - 1s 175us/step - loss: 0.0443 - acc: 0.9883 - val_loss: 1.1197 - val_acc: 0.8634\n",
            "Epoch 4/20\n",
            "5400/5400 [==============================] - 1s 176us/step - loss: 0.0371 - acc: 0.9891 - val_loss: 1.1742 - val_acc: 0.8614\n",
            "Epoch 5/20\n",
            "5400/5400 [==============================] - 1s 178us/step - loss: 0.0452 - acc: 0.9891 - val_loss: 1.1529 - val_acc: 0.8620\n",
            "Epoch 6/20\n",
            "5400/5400 [==============================] - 1s 182us/step - loss: 0.0316 - acc: 0.9894 - val_loss: 1.1909 - val_acc: 0.8602\n",
            "Epoch 7/20\n",
            "5400/5400 [==============================] - 1s 177us/step - loss: 0.0344 - acc: 0.9900 - val_loss: 1.2566 - val_acc: 0.8603\n",
            "Epoch 8/20\n",
            "5400/5400 [==============================] - 1s 183us/step - loss: 0.0284 - acc: 0.9933 - val_loss: 1.2576 - val_acc: 0.8611\n",
            "Epoch 9/20\n",
            "5400/5400 [==============================] - 1s 177us/step - loss: 0.0215 - acc: 0.9935 - val_loss: 1.1659 - val_acc: 0.8624\n",
            "Epoch 10/20\n",
            "5400/5400 [==============================] - 1s 183us/step - loss: 0.0208 - acc: 0.9933 - val_loss: 1.2136 - val_acc: 0.8624\n",
            "Epoch 11/20\n",
            "5400/5400 [==============================] - 1s 179us/step - loss: 0.0197 - acc: 0.9939 - val_loss: 1.2206 - val_acc: 0.8660\n",
            "Epoch 12/20\n",
            "5400/5400 [==============================] - 1s 177us/step - loss: 0.0284 - acc: 0.9922 - val_loss: 1.2252 - val_acc: 0.8651\n",
            "Epoch 13/20\n",
            "5400/5400 [==============================] - 1s 177us/step - loss: 0.0229 - acc: 0.9935 - val_loss: 1.1792 - val_acc: 0.8661\n",
            "Epoch 14/20\n",
            "5400/5400 [==============================] - 1s 181us/step - loss: 0.0231 - acc: 0.9937 - val_loss: 1.2596 - val_acc: 0.8648\n",
            "Epoch 15/20\n",
            "5400/5400 [==============================] - 1s 181us/step - loss: 0.0266 - acc: 0.9944 - val_loss: 1.2725 - val_acc: 0.8643\n",
            "Epoch 16/20\n",
            "5400/5400 [==============================] - 1s 184us/step - loss: 0.0250 - acc: 0.9911 - val_loss: 1.1672 - val_acc: 0.8624\n",
            "Epoch 17/20\n",
            "5400/5400 [==============================] - 1s 181us/step - loss: 0.0154 - acc: 0.9950 - val_loss: 1.2842 - val_acc: 0.8642\n",
            "Epoch 18/20\n",
            "5400/5400 [==============================] - 1s 172us/step - loss: 0.0200 - acc: 0.9946 - val_loss: 1.2824 - val_acc: 0.8665\n",
            "Epoch 19/20\n",
            "5400/5400 [==============================] - 1s 178us/step - loss: 0.0207 - acc: 0.9956 - val_loss: 1.2480 - val_acc: 0.8665\n",
            "Epoch 20/20\n",
            "5400/5400 [==============================] - 1s 174us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 1.2265 - val_acc: 0.8688\n",
            "Test loss: 1.2264611304163966\n",
            "Test accuracy: 0.8688\n",
            "Train loss: 0.003021867075946863\n",
            "Train accuracy: 0.9998148147265117\n",
            "x label: (5500, 28, 28)\n",
            "y label: (5500,)\n",
            "x unlabel: (54500, 28, 28)\n",
            "y unlabel: (54500,)\n",
            "54\n",
            "Train on 5500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5500/5500 [==============================] - 1s 185us/step - loss: 0.0530 - acc: 0.9878 - val_loss: 1.1477 - val_acc: 0.8655\n",
            "Epoch 2/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0410 - acc: 0.9889 - val_loss: 1.1652 - val_acc: 0.8665\n",
            "Epoch 3/20\n",
            "5500/5500 [==============================] - 1s 182us/step - loss: 0.0489 - acc: 0.9895 - val_loss: 1.0449 - val_acc: 0.8642\n",
            "Epoch 4/20\n",
            "5500/5500 [==============================] - 1s 176us/step - loss: 0.0358 - acc: 0.9911 - val_loss: 1.1609 - val_acc: 0.8644\n",
            "Epoch 5/20\n",
            "5500/5500 [==============================] - 1s 179us/step - loss: 0.0302 - acc: 0.9907 - val_loss: 1.3050 - val_acc: 0.8621\n",
            "Epoch 6/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0301 - acc: 0.9904 - val_loss: 1.2838 - val_acc: 0.8615\n",
            "Epoch 7/20\n",
            "5500/5500 [==============================] - 1s 176us/step - loss: 0.0305 - acc: 0.9922 - val_loss: 1.1910 - val_acc: 0.8642\n",
            "Epoch 8/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0388 - acc: 0.9895 - val_loss: 1.1190 - val_acc: 0.8655\n",
            "Epoch 9/20\n",
            "5500/5500 [==============================] - 1s 175us/step - loss: 0.0216 - acc: 0.9935 - val_loss: 1.1365 - val_acc: 0.8638\n",
            "Epoch 10/20\n",
            "5500/5500 [==============================] - 1s 176us/step - loss: 0.0216 - acc: 0.9922 - val_loss: 1.2119 - val_acc: 0.8632\n",
            "Epoch 11/20\n",
            "5500/5500 [==============================] - 1s 177us/step - loss: 0.0159 - acc: 0.9962 - val_loss: 1.3027 - val_acc: 0.8612\n",
            "Epoch 12/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0179 - acc: 0.9935 - val_loss: 1.1918 - val_acc: 0.8636\n",
            "Epoch 13/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 1.1897 - val_acc: 0.8654\n",
            "Epoch 14/20\n",
            "5500/5500 [==============================] - 1s 182us/step - loss: 0.0189 - acc: 0.9956 - val_loss: 1.2037 - val_acc: 0.8673\n",
            "Epoch 15/20\n",
            "5500/5500 [==============================] - 1s 176us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 1.1957 - val_acc: 0.8642\n",
            "Epoch 16/20\n",
            "5500/5500 [==============================] - 1s 181us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 1.1969 - val_acc: 0.8637\n",
            "Epoch 17/20\n",
            "5500/5500 [==============================] - 1s 177us/step - loss: 0.0195 - acc: 0.9929 - val_loss: 1.2232 - val_acc: 0.8648\n",
            "Epoch 18/20\n",
            "5500/5500 [==============================] - 1s 179us/step - loss: 0.0205 - acc: 0.9942 - val_loss: 1.2592 - val_acc: 0.8642\n",
            "Epoch 19/20\n",
            "5500/5500 [==============================] - 1s 180us/step - loss: 0.0175 - acc: 0.9944 - val_loss: 1.2509 - val_acc: 0.8639\n",
            "Epoch 20/20\n",
            "5500/5500 [==============================] - 1s 178us/step - loss: 0.0178 - acc: 0.9933 - val_loss: 1.2855 - val_acc: 0.8652\n",
            "Test loss: 1.2854758184024904\n",
            "Test accuracy: 0.8652\n",
            "Train loss: 3.1648349815027774e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (5600, 28, 28)\n",
            "y label: (5600,)\n",
            "x unlabel: (54400, 28, 28)\n",
            "y unlabel: (54400,)\n",
            "55\n",
            "Train on 5600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5600/5600 [==============================] - 1s 177us/step - loss: 0.0479 - acc: 0.9884 - val_loss: 1.0066 - val_acc: 0.8661\n",
            "Epoch 2/20\n",
            "5600/5600 [==============================] - 1s 177us/step - loss: 0.0445 - acc: 0.9889 - val_loss: 1.0418 - val_acc: 0.8658\n",
            "Epoch 3/20\n",
            "5600/5600 [==============================] - 1s 178us/step - loss: 0.0436 - acc: 0.9888 - val_loss: 1.2091 - val_acc: 0.8620\n",
            "Epoch 4/20\n",
            "5600/5600 [==============================] - 1s 178us/step - loss: 0.0377 - acc: 0.9907 - val_loss: 1.1783 - val_acc: 0.8652\n",
            "Epoch 5/20\n",
            "5600/5600 [==============================] - 1s 183us/step - loss: 0.0277 - acc: 0.9929 - val_loss: 1.2164 - val_acc: 0.8650\n",
            "Epoch 6/20\n",
            "5600/5600 [==============================] - 1s 192us/step - loss: 0.0330 - acc: 0.9909 - val_loss: 1.1550 - val_acc: 0.8639\n",
            "Epoch 7/20\n",
            "5600/5600 [==============================] - 1s 191us/step - loss: 0.0308 - acc: 0.9911 - val_loss: 1.1419 - val_acc: 0.8653\n",
            "Epoch 8/20\n",
            "5600/5600 [==============================] - 1s 188us/step - loss: 0.0253 - acc: 0.9934 - val_loss: 1.1477 - val_acc: 0.8634\n",
            "Epoch 9/20\n",
            "5600/5600 [==============================] - 1s 188us/step - loss: 0.0297 - acc: 0.9913 - val_loss: 1.1817 - val_acc: 0.8641\n",
            "Epoch 10/20\n",
            "5600/5600 [==============================] - 1s 191us/step - loss: 0.0301 - acc: 0.9939 - val_loss: 1.2263 - val_acc: 0.8623\n",
            "Epoch 11/20\n",
            "5600/5600 [==============================] - 1s 188us/step - loss: 0.0270 - acc: 0.9918 - val_loss: 1.2038 - val_acc: 0.8637\n",
            "Epoch 12/20\n",
            "5600/5600 [==============================] - 1s 191us/step - loss: 0.0263 - acc: 0.9930 - val_loss: 1.2126 - val_acc: 0.8664\n",
            "Epoch 13/20\n",
            "5600/5600 [==============================] - 1s 186us/step - loss: 0.0285 - acc: 0.9920 - val_loss: 1.2145 - val_acc: 0.8659\n",
            "Epoch 14/20\n",
            "5600/5600 [==============================] - 1s 187us/step - loss: 0.0237 - acc: 0.9930 - val_loss: 1.1972 - val_acc: 0.8656\n",
            "Epoch 15/20\n",
            "5600/5600 [==============================] - 1s 186us/step - loss: 0.0193 - acc: 0.9941 - val_loss: 1.2763 - val_acc: 0.8622\n",
            "Epoch 16/20\n",
            "5600/5600 [==============================] - 1s 179us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 1.2560 - val_acc: 0.8647\n",
            "Epoch 17/20\n",
            "5600/5600 [==============================] - 1s 172us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 1.1953 - val_acc: 0.8633\n",
            "Epoch 18/20\n",
            "5600/5600 [==============================] - 1s 179us/step - loss: 0.0185 - acc: 0.9945 - val_loss: 1.2235 - val_acc: 0.8651\n",
            "Epoch 19/20\n",
            "5600/5600 [==============================] - 1s 182us/step - loss: 0.0218 - acc: 0.9939 - val_loss: 1.1936 - val_acc: 0.8692\n",
            "Epoch 20/20\n",
            "5600/5600 [==============================] - 1s 177us/step - loss: 0.0290 - acc: 0.9930 - val_loss: 1.2142 - val_acc: 0.8662\n",
            "Test loss: 1.214195478427544\n",
            "Test accuracy: 0.8662\n",
            "Train loss: 0.0026448193155229256\n",
            "Train accuracy: 0.9998214285714285\n",
            "x label: (5700, 28, 28)\n",
            "y label: (5700,)\n",
            "x unlabel: (54300, 28, 28)\n",
            "y unlabel: (54300,)\n",
            "56\n",
            "Train on 5700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5700/5700 [==============================] - 1s 174us/step - loss: 0.0555 - acc: 0.9888 - val_loss: 1.1935 - val_acc: 0.8662\n",
            "Epoch 2/20\n",
            "5700/5700 [==============================] - 1s 181us/step - loss: 0.0530 - acc: 0.9858 - val_loss: 1.0445 - val_acc: 0.8639\n",
            "Epoch 3/20\n",
            "5700/5700 [==============================] - 1s 181us/step - loss: 0.0383 - acc: 0.9909 - val_loss: 1.0905 - val_acc: 0.8682\n",
            "Epoch 4/20\n",
            "5700/5700 [==============================] - 1s 173us/step - loss: 0.0353 - acc: 0.9907 - val_loss: 1.1402 - val_acc: 0.8666\n",
            "Epoch 5/20\n",
            "5700/5700 [==============================] - 1s 178us/step - loss: 0.0336 - acc: 0.9898 - val_loss: 1.1015 - val_acc: 0.8658\n",
            "Epoch 6/20\n",
            "5700/5700 [==============================] - 1s 179us/step - loss: 0.0355 - acc: 0.9893 - val_loss: 1.2461 - val_acc: 0.8639\n",
            "Epoch 7/20\n",
            "5700/5700 [==============================] - 1s 173us/step - loss: 0.0372 - acc: 0.9886 - val_loss: 1.1610 - val_acc: 0.8629\n",
            "Epoch 8/20\n",
            "5700/5700 [==============================] - 1s 179us/step - loss: 0.0259 - acc: 0.9923 - val_loss: 1.1410 - val_acc: 0.8661\n",
            "Epoch 9/20\n",
            "5700/5700 [==============================] - 1s 177us/step - loss: 0.0276 - acc: 0.9912 - val_loss: 1.2384 - val_acc: 0.8667\n",
            "Epoch 10/20\n",
            "5700/5700 [==============================] - 1s 180us/step - loss: 0.0226 - acc: 0.9930 - val_loss: 1.0949 - val_acc: 0.8637\n",
            "Epoch 11/20\n",
            "5700/5700 [==============================] - 1s 177us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 1.1741 - val_acc: 0.8645\n",
            "Epoch 12/20\n",
            "5700/5700 [==============================] - 1s 178us/step - loss: 0.0239 - acc: 0.9932 - val_loss: 1.2147 - val_acc: 0.8678\n",
            "Epoch 13/20\n",
            "5700/5700 [==============================] - 1s 175us/step - loss: 0.0196 - acc: 0.9930 - val_loss: 1.1933 - val_acc: 0.8655\n",
            "Epoch 14/20\n",
            "5700/5700 [==============================] - 1s 174us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 1.1404 - val_acc: 0.8671\n",
            "Epoch 15/20\n",
            "5700/5700 [==============================] - 1s 182us/step - loss: 0.0191 - acc: 0.9944 - val_loss: 1.1314 - val_acc: 0.8654\n",
            "Epoch 16/20\n",
            "5700/5700 [==============================] - 1s 178us/step - loss: 0.0207 - acc: 0.9925 - val_loss: 1.2003 - val_acc: 0.8682\n",
            "Epoch 17/20\n",
            "5700/5700 [==============================] - 1s 178us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 1.1704 - val_acc: 0.8675\n",
            "Epoch 18/20\n",
            "5700/5700 [==============================] - 1s 178us/step - loss: 0.0193 - acc: 0.9932 - val_loss: 1.2305 - val_acc: 0.8649\n",
            "Epoch 19/20\n",
            "5700/5700 [==============================] - 1s 180us/step - loss: 0.0226 - acc: 0.9928 - val_loss: 1.1908 - val_acc: 0.8657\n",
            "Epoch 20/20\n",
            "5700/5700 [==============================] - 1s 177us/step - loss: 0.0158 - acc: 0.9940 - val_loss: 1.1549 - val_acc: 0.8652\n",
            "Test loss: 1.1549086351051578\n",
            "Test accuracy: 0.8652\n",
            "Train loss: 0.0006210176105712475\n",
            "Train accuracy: 0.9998245614035087\n",
            "x label: (5800, 28, 28)\n",
            "y label: (5800,)\n",
            "x unlabel: (54200, 28, 28)\n",
            "y unlabel: (54200,)\n",
            "57\n",
            "Train on 5800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5800/5800 [==============================] - 1s 179us/step - loss: 0.0501 - acc: 0.9874 - val_loss: 1.0461 - val_acc: 0.8692\n",
            "Epoch 2/20\n",
            "5800/5800 [==============================] - 1s 177us/step - loss: 0.0359 - acc: 0.9895 - val_loss: 1.1012 - val_acc: 0.8663\n",
            "Epoch 3/20\n",
            "5800/5800 [==============================] - 1s 170us/step - loss: 0.0347 - acc: 0.9900 - val_loss: 1.0698 - val_acc: 0.8679\n",
            "Epoch 4/20\n",
            "5800/5800 [==============================] - 1s 173us/step - loss: 0.0344 - acc: 0.9903 - val_loss: 1.0876 - val_acc: 0.8678\n",
            "Epoch 5/20\n",
            "5800/5800 [==============================] - 1s 175us/step - loss: 0.0272 - acc: 0.9907 - val_loss: 1.1780 - val_acc: 0.8663\n",
            "Epoch 6/20\n",
            "5800/5800 [==============================] - 1s 175us/step - loss: 0.0294 - acc: 0.9905 - val_loss: 1.1236 - val_acc: 0.8638\n",
            "Epoch 7/20\n",
            "5800/5800 [==============================] - 1s 178us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 1.2314 - val_acc: 0.8646\n",
            "Epoch 8/20\n",
            "5800/5800 [==============================] - 1s 173us/step - loss: 0.0268 - acc: 0.9921 - val_loss: 1.1492 - val_acc: 0.8692\n",
            "Epoch 9/20\n",
            "5800/5800 [==============================] - 1s 178us/step - loss: 0.0322 - acc: 0.9910 - val_loss: 1.2312 - val_acc: 0.8618\n",
            "Epoch 10/20\n",
            "5800/5800 [==============================] - 1s 179us/step - loss: 0.0265 - acc: 0.9921 - val_loss: 1.3039 - val_acc: 0.8606\n",
            "Epoch 11/20\n",
            "5800/5800 [==============================] - 1s 173us/step - loss: 0.0253 - acc: 0.9916 - val_loss: 1.1787 - val_acc: 0.8695\n",
            "Epoch 12/20\n",
            "5800/5800 [==============================] - 1s 179us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 1.1346 - val_acc: 0.8705\n",
            "Epoch 13/20\n",
            "5800/5800 [==============================] - 1s 171us/step - loss: 0.0244 - acc: 0.9924 - val_loss: 1.0918 - val_acc: 0.8701\n",
            "Epoch 14/20\n",
            "5800/5800 [==============================] - 1s 174us/step - loss: 0.0202 - acc: 0.9948 - val_loss: 1.1990 - val_acc: 0.8708\n",
            "Epoch 15/20\n",
            "5800/5800 [==============================] - 1s 172us/step - loss: 0.0183 - acc: 0.9943 - val_loss: 1.1341 - val_acc: 0.8701\n",
            "Epoch 16/20\n",
            "5800/5800 [==============================] - 1s 177us/step - loss: 0.0212 - acc: 0.9928 - val_loss: 1.1271 - val_acc: 0.8687\n",
            "Epoch 17/20\n",
            "5800/5800 [==============================] - 1s 171us/step - loss: 0.0192 - acc: 0.9948 - val_loss: 1.2459 - val_acc: 0.8674\n",
            "Epoch 18/20\n",
            "5800/5800 [==============================] - 1s 174us/step - loss: 0.0229 - acc: 0.9933 - val_loss: 1.1364 - val_acc: 0.8692\n",
            "Epoch 19/20\n",
            "5800/5800 [==============================] - 1s 181us/step - loss: 0.0193 - acc: 0.9948 - val_loss: 1.0999 - val_acc: 0.8672\n",
            "Epoch 20/20\n",
            "5800/5800 [==============================] - 1s 173us/step - loss: 0.0180 - acc: 0.9931 - val_loss: 1.1801 - val_acc: 0.8694\n",
            "Test loss: 1.1801420590056135\n",
            "Test accuracy: 0.8694\n",
            "Train loss: 0.00046604504778433064\n",
            "Train accuracy: 0.9998275862068966\n",
            "x label: (5900, 28, 28)\n",
            "y label: (5900,)\n",
            "x unlabel: (54100, 28, 28)\n",
            "y unlabel: (54100,)\n",
            "58\n",
            "Train on 5900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "5900/5900 [==============================] - 1s 181us/step - loss: 0.0586 - acc: 0.9873 - val_loss: 1.2099 - val_acc: 0.8672\n",
            "Epoch 2/20\n",
            "5900/5900 [==============================] - 1s 174us/step - loss: 0.0526 - acc: 0.9875 - val_loss: 1.0447 - val_acc: 0.8675\n",
            "Epoch 3/20\n",
            "5900/5900 [==============================] - 1s 174us/step - loss: 0.0386 - acc: 0.9910 - val_loss: 1.1185 - val_acc: 0.8661\n",
            "Epoch 4/20\n",
            "5900/5900 [==============================] - 1s 169us/step - loss: 0.0359 - acc: 0.9897 - val_loss: 1.0848 - val_acc: 0.8696\n",
            "Epoch 5/20\n",
            "5900/5900 [==============================] - 1s 175us/step - loss: 0.0345 - acc: 0.9903 - val_loss: 1.1421 - val_acc: 0.8673\n",
            "Epoch 6/20\n",
            "5900/5900 [==============================] - 1s 179us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 1.1094 - val_acc: 0.8653\n",
            "Epoch 7/20\n",
            "5900/5900 [==============================] - 1s 171us/step - loss: 0.0321 - acc: 0.9919 - val_loss: 1.0144 - val_acc: 0.8637\n",
            "Epoch 8/20\n",
            "5900/5900 [==============================] - 1s 175us/step - loss: 0.0330 - acc: 0.9903 - val_loss: 1.0968 - val_acc: 0.8703\n",
            "Epoch 9/20\n",
            "5900/5900 [==============================] - 1s 173us/step - loss: 0.0318 - acc: 0.9914 - val_loss: 1.1539 - val_acc: 0.8687\n",
            "Epoch 10/20\n",
            "5900/5900 [==============================] - 1s 174us/step - loss: 0.0240 - acc: 0.9927 - val_loss: 1.1695 - val_acc: 0.8687\n",
            "Epoch 11/20\n",
            "5900/5900 [==============================] - 1s 177us/step - loss: 0.0143 - acc: 0.9959 - val_loss: 1.2033 - val_acc: 0.8686\n",
            "Epoch 12/20\n",
            "5900/5900 [==============================] - 1s 177us/step - loss: 0.0240 - acc: 0.9939 - val_loss: 1.1606 - val_acc: 0.8651\n",
            "Epoch 13/20\n",
            "5900/5900 [==============================] - 1s 174us/step - loss: 0.0246 - acc: 0.9924 - val_loss: 1.1469 - val_acc: 0.8681\n",
            "Epoch 14/20\n",
            "5900/5900 [==============================] - 1s 171us/step - loss: 0.0245 - acc: 0.9934 - val_loss: 1.2091 - val_acc: 0.8680\n",
            "Epoch 15/20\n",
            "5900/5900 [==============================] - 1s 174us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 1.1301 - val_acc: 0.8663\n",
            "Epoch 16/20\n",
            "5900/5900 [==============================] - 1s 175us/step - loss: 0.0218 - acc: 0.9937 - val_loss: 1.0880 - val_acc: 0.8678\n",
            "Epoch 17/20\n",
            "5900/5900 [==============================] - 1s 171us/step - loss: 0.0199 - acc: 0.9936 - val_loss: 1.0850 - val_acc: 0.8699\n",
            "Epoch 18/20\n",
            "5900/5900 [==============================] - 1s 175us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 1.1197 - val_acc: 0.8704\n",
            "Epoch 19/20\n",
            "5900/5900 [==============================] - 1s 172us/step - loss: 0.0201 - acc: 0.9915 - val_loss: 1.2101 - val_acc: 0.8681\n",
            "Epoch 20/20\n",
            "5900/5900 [==============================] - 1s 170us/step - loss: 0.0176 - acc: 0.9936 - val_loss: 1.2134 - val_acc: 0.8695\n",
            "Test loss: 1.21344249273541\n",
            "Test accuracy: 0.8695\n",
            "Train loss: 0.00010994955266288358\n",
            "Train accuracy: 1.0\n",
            "x label: (6000, 28, 28)\n",
            "y label: (6000,)\n",
            "x unlabel: (54000, 28, 28)\n",
            "y unlabel: (54000,)\n",
            "59\n",
            "Train on 6000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6000/6000 [==============================] - 1s 168us/step - loss: 0.0449 - acc: 0.9887 - val_loss: 1.0872 - val_acc: 0.8712\n",
            "Epoch 2/20\n",
            "6000/6000 [==============================] - 1s 173us/step - loss: 0.0434 - acc: 0.9880 - val_loss: 1.0676 - val_acc: 0.8663\n",
            "Epoch 3/20\n",
            "6000/6000 [==============================] - 1s 169us/step - loss: 0.0381 - acc: 0.9882 - val_loss: 1.1099 - val_acc: 0.8712\n",
            "Epoch 4/20\n",
            "6000/6000 [==============================] - 1s 175us/step - loss: 0.0354 - acc: 0.9902 - val_loss: 1.0914 - val_acc: 0.8697\n",
            "Epoch 5/20\n",
            "6000/6000 [==============================] - 1s 167us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 1.1050 - val_acc: 0.8697\n",
            "Epoch 6/20\n",
            "6000/6000 [==============================] - 1s 173us/step - loss: 0.0337 - acc: 0.9898 - val_loss: 1.1521 - val_acc: 0.8706\n",
            "Epoch 7/20\n",
            "6000/6000 [==============================] - 1s 171us/step - loss: 0.0254 - acc: 0.9913 - val_loss: 1.1861 - val_acc: 0.8705\n",
            "Epoch 8/20\n",
            "6000/6000 [==============================] - 1s 174us/step - loss: 0.0267 - acc: 0.9912 - val_loss: 1.1012 - val_acc: 0.8724\n",
            "Epoch 9/20\n",
            "6000/6000 [==============================] - 1s 168us/step - loss: 0.0221 - acc: 0.9938 - val_loss: 1.1587 - val_acc: 0.8696\n",
            "Epoch 10/20\n",
            "6000/6000 [==============================] - 1s 170us/step - loss: 0.0183 - acc: 0.9933 - val_loss: 1.1781 - val_acc: 0.8702\n",
            "Epoch 11/20\n",
            "6000/6000 [==============================] - 1s 168us/step - loss: 0.0241 - acc: 0.9910 - val_loss: 1.1778 - val_acc: 0.8668\n",
            "Epoch 12/20\n",
            "6000/6000 [==============================] - 1s 171us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 1.1434 - val_acc: 0.8682\n",
            "Epoch 13/20\n",
            "6000/6000 [==============================] - 1s 169us/step - loss: 0.0271 - acc: 0.9910 - val_loss: 1.1926 - val_acc: 0.8680\n",
            "Epoch 14/20\n",
            "6000/6000 [==============================] - 1s 175us/step - loss: 0.0186 - acc: 0.9950 - val_loss: 1.1149 - val_acc: 0.8716\n",
            "Epoch 15/20\n",
            "6000/6000 [==============================] - 1s 170us/step - loss: 0.0207 - acc: 0.9928 - val_loss: 1.2652 - val_acc: 0.8687\n",
            "Epoch 16/20\n",
            "6000/6000 [==============================] - 1s 173us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 1.2138 - val_acc: 0.8694\n",
            "Epoch 17/20\n",
            "6000/6000 [==============================] - 1s 174us/step - loss: 0.0297 - acc: 0.9918 - val_loss: 1.1180 - val_acc: 0.8711\n",
            "Epoch 18/20\n",
            "6000/6000 [==============================] - 1s 169us/step - loss: 0.0142 - acc: 0.9947 - val_loss: 1.0864 - val_acc: 0.8666\n",
            "Epoch 19/20\n",
            "6000/6000 [==============================] - 1s 171us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 1.2033 - val_acc: 0.8706\n",
            "Epoch 20/20\n",
            "6000/6000 [==============================] - 1s 167us/step - loss: 0.0152 - acc: 0.9947 - val_loss: 1.1892 - val_acc: 0.8689\n",
            "Test loss: 1.1891812411185936\n",
            "Test accuracy: 0.8689\n",
            "Train loss: 0.00021351903151723187\n",
            "Train accuracy: 0.9998333333333334\n",
            "x label: (6100, 28, 28)\n",
            "y label: (6100,)\n",
            "x unlabel: (53900, 28, 28)\n",
            "y unlabel: (53900,)\n",
            "60\n",
            "Train on 6100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6100/6100 [==============================] - 1s 175us/step - loss: 0.0557 - acc: 0.9862 - val_loss: 1.0505 - val_acc: 0.8651\n",
            "Epoch 2/20\n",
            "6100/6100 [==============================] - 1s 170us/step - loss: 0.0393 - acc: 0.9882 - val_loss: 1.1140 - val_acc: 0.8705\n",
            "Epoch 3/20\n",
            "6100/6100 [==============================] - 1s 168us/step - loss: 0.0405 - acc: 0.9877 - val_loss: 1.1714 - val_acc: 0.8668\n",
            "Epoch 4/20\n",
            "6100/6100 [==============================] - 1s 168us/step - loss: 0.0299 - acc: 0.9903 - val_loss: 1.1425 - val_acc: 0.8683\n",
            "Epoch 5/20\n",
            "6100/6100 [==============================] - 1s 169us/step - loss: 0.0219 - acc: 0.9920 - val_loss: 1.2165 - val_acc: 0.8649\n",
            "Epoch 6/20\n",
            "6100/6100 [==============================] - 1s 169us/step - loss: 0.0291 - acc: 0.9916 - val_loss: 1.1251 - val_acc: 0.8679\n",
            "Epoch 7/20\n",
            "6100/6100 [==============================] - 1s 174us/step - loss: 0.0289 - acc: 0.9913 - val_loss: 1.1274 - val_acc: 0.8682\n",
            "Epoch 8/20\n",
            "6100/6100 [==============================] - 1s 173us/step - loss: 0.0219 - acc: 0.9926 - val_loss: 1.0985 - val_acc: 0.8694\n",
            "Epoch 9/20\n",
            "6100/6100 [==============================] - 1s 168us/step - loss: 0.0269 - acc: 0.9933 - val_loss: 1.2758 - val_acc: 0.8686\n",
            "Epoch 10/20\n",
            "6100/6100 [==============================] - 1s 173us/step - loss: 0.0302 - acc: 0.9921 - val_loss: 1.1845 - val_acc: 0.8721\n",
            "Epoch 11/20\n",
            "6100/6100 [==============================] - 1s 173us/step - loss: 0.0278 - acc: 0.9926 - val_loss: 1.2146 - val_acc: 0.8684\n",
            "Epoch 12/20\n",
            "6100/6100 [==============================] - 1s 173us/step - loss: 0.0196 - acc: 0.9936 - val_loss: 1.2385 - val_acc: 0.8693\n",
            "Epoch 13/20\n",
            "6100/6100 [==============================] - 1s 172us/step - loss: 0.0274 - acc: 0.9916 - val_loss: 1.2092 - val_acc: 0.8676\n",
            "Epoch 14/20\n",
            "6100/6100 [==============================] - 1s 170us/step - loss: 0.0173 - acc: 0.9944 - val_loss: 1.1145 - val_acc: 0.8744\n",
            "Epoch 15/20\n",
            "6100/6100 [==============================] - 1s 174us/step - loss: 0.0190 - acc: 0.9944 - val_loss: 1.1932 - val_acc: 0.8704\n",
            "Epoch 16/20\n",
            "6100/6100 [==============================] - 1s 170us/step - loss: 0.0254 - acc: 0.9928 - val_loss: 1.1766 - val_acc: 0.8702\n",
            "Epoch 17/20\n",
            "6100/6100 [==============================] - 1s 169us/step - loss: 0.0161 - acc: 0.9948 - val_loss: 1.1398 - val_acc: 0.8699\n",
            "Epoch 18/20\n",
            "6100/6100 [==============================] - 1s 171us/step - loss: 0.0179 - acc: 0.9941 - val_loss: 1.1524 - val_acc: 0.8676\n",
            "Epoch 19/20\n",
            "6100/6100 [==============================] - 1s 171us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 1.1202 - val_acc: 0.8709\n",
            "Epoch 20/20\n",
            "6100/6100 [==============================] - 1s 173us/step - loss: 0.0168 - acc: 0.9944 - val_loss: 1.1367 - val_acc: 0.8714\n",
            "Test loss: 1.1367391251638785\n",
            "Test accuracy: 0.8714\n",
            "Train loss: 0.00019568635767211432\n",
            "Train accuracy: 1.0\n",
            "x label: (6200, 28, 28)\n",
            "y label: (6200,)\n",
            "x unlabel: (53800, 28, 28)\n",
            "y unlabel: (53800,)\n",
            "61\n",
            "Train on 6200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6200/6200 [==============================] - 1s 173us/step - loss: 0.0554 - acc: 0.9873 - val_loss: 1.0154 - val_acc: 0.8723\n",
            "Epoch 2/20\n",
            "6200/6200 [==============================] - 1s 170us/step - loss: 0.0463 - acc: 0.9894 - val_loss: 1.0841 - val_acc: 0.8727\n",
            "Epoch 3/20\n",
            "6200/6200 [==============================] - 1s 176us/step - loss: 0.0336 - acc: 0.9908 - val_loss: 0.9701 - val_acc: 0.8743\n",
            "Epoch 4/20\n",
            "6200/6200 [==============================] - 1s 166us/step - loss: 0.0270 - acc: 0.9916 - val_loss: 1.1167 - val_acc: 0.8722\n",
            "Epoch 5/20\n",
            "6200/6200 [==============================] - 1s 172us/step - loss: 0.0313 - acc: 0.9898 - val_loss: 1.1242 - val_acc: 0.8739\n",
            "Epoch 6/20\n",
            "6200/6200 [==============================] - 1s 178us/step - loss: 0.0219 - acc: 0.9944 - val_loss: 1.0791 - val_acc: 0.8739\n",
            "Epoch 7/20\n",
            "6200/6200 [==============================] - 1s 167us/step - loss: 0.0309 - acc: 0.9890 - val_loss: 1.1570 - val_acc: 0.8697\n",
            "Epoch 8/20\n",
            "6200/6200 [==============================] - 1s 173us/step - loss: 0.0227 - acc: 0.9932 - val_loss: 1.1940 - val_acc: 0.8721\n",
            "Epoch 9/20\n",
            "6200/6200 [==============================] - 1s 169us/step - loss: 0.0348 - acc: 0.9892 - val_loss: 1.0561 - val_acc: 0.8702\n",
            "Epoch 10/20\n",
            "6200/6200 [==============================] - 1s 171us/step - loss: 0.0263 - acc: 0.9919 - val_loss: 1.2099 - val_acc: 0.8692\n",
            "Epoch 11/20\n",
            "6200/6200 [==============================] - 1s 171us/step - loss: 0.0268 - acc: 0.9937 - val_loss: 1.1321 - val_acc: 0.8708\n",
            "Epoch 12/20\n",
            "6200/6200 [==============================] - 1s 170us/step - loss: 0.0183 - acc: 0.9944 - val_loss: 1.1797 - val_acc: 0.8706\n",
            "Epoch 13/20\n",
            "6200/6200 [==============================] - 1s 176us/step - loss: 0.0276 - acc: 0.9906 - val_loss: 1.1834 - val_acc: 0.8667\n",
            "Epoch 14/20\n",
            "6200/6200 [==============================] - 1s 172us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 1.2084 - val_acc: 0.8673\n",
            "Epoch 15/20\n",
            "6200/6200 [==============================] - 1s 174us/step - loss: 0.0183 - acc: 0.9945 - val_loss: 1.1890 - val_acc: 0.8684\n",
            "Epoch 16/20\n",
            "6200/6200 [==============================] - 1s 167us/step - loss: 0.0219 - acc: 0.9932 - val_loss: 1.1917 - val_acc: 0.8696\n",
            "Epoch 17/20\n",
            "6200/6200 [==============================] - 1s 170us/step - loss: 0.0204 - acc: 0.9916 - val_loss: 1.2062 - val_acc: 0.8676\n",
            "Epoch 18/20\n",
            "6200/6200 [==============================] - 1s 177us/step - loss: 0.0174 - acc: 0.9942 - val_loss: 1.1391 - val_acc: 0.8665\n",
            "Epoch 19/20\n",
            "6200/6200 [==============================] - 1s 172us/step - loss: 0.0176 - acc: 0.9939 - val_loss: 1.1347 - val_acc: 0.8719\n",
            "Epoch 20/20\n",
            "6200/6200 [==============================] - 1s 174us/step - loss: 0.0160 - acc: 0.9942 - val_loss: 1.2387 - val_acc: 0.8674\n",
            "Test loss: 1.2386670774046216\n",
            "Test accuracy: 0.8674\n",
            "Train loss: 0.00021867160583907764\n",
            "Train accuracy: 1.0\n",
            "x label: (6300, 28, 28)\n",
            "y label: (6300,)\n",
            "x unlabel: (53700, 28, 28)\n",
            "y unlabel: (53700,)\n",
            "62\n",
            "Train on 6300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6300/6300 [==============================] - 1s 175us/step - loss: 0.0491 - acc: 0.9875 - val_loss: 1.1331 - val_acc: 0.8694\n",
            "Epoch 2/20\n",
            "6300/6300 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.9870 - val_loss: 1.0581 - val_acc: 0.8702\n",
            "Epoch 3/20\n",
            "6300/6300 [==============================] - 1s 170us/step - loss: 0.0354 - acc: 0.9911 - val_loss: 0.9936 - val_acc: 0.8711\n",
            "Epoch 4/20\n",
            "6300/6300 [==============================] - 1s 172us/step - loss: 0.0347 - acc: 0.9890 - val_loss: 0.9870 - val_acc: 0.8717\n",
            "Epoch 5/20\n",
            "6300/6300 [==============================] - 1s 163us/step - loss: 0.0351 - acc: 0.9908 - val_loss: 1.0890 - val_acc: 0.8725\n",
            "Epoch 6/20\n",
            "6300/6300 [==============================] - 1s 170us/step - loss: 0.0304 - acc: 0.9913 - val_loss: 1.0555 - val_acc: 0.8733\n",
            "Epoch 7/20\n",
            "6300/6300 [==============================] - 1s 170us/step - loss: 0.0283 - acc: 0.9921 - val_loss: 1.0894 - val_acc: 0.8737\n",
            "Epoch 8/20\n",
            "6300/6300 [==============================] - 1s 166us/step - loss: 0.0261 - acc: 0.9924 - val_loss: 1.1426 - val_acc: 0.8716\n",
            "Epoch 9/20\n",
            "6300/6300 [==============================] - 1s 172us/step - loss: 0.0243 - acc: 0.9917 - val_loss: 1.1173 - val_acc: 0.8719\n",
            "Epoch 10/20\n",
            "6300/6300 [==============================] - 1s 169us/step - loss: 0.0235 - acc: 0.9932 - val_loss: 0.9712 - val_acc: 0.8751\n",
            "Epoch 11/20\n",
            "6300/6300 [==============================] - 1s 173us/step - loss: 0.0246 - acc: 0.9935 - val_loss: 1.0004 - val_acc: 0.8731\n",
            "Epoch 12/20\n",
            "6300/6300 [==============================] - 1s 165us/step - loss: 0.0248 - acc: 0.9932 - val_loss: 1.1323 - val_acc: 0.8733\n",
            "Epoch 13/20\n",
            "6300/6300 [==============================] - 1s 166us/step - loss: 0.0215 - acc: 0.9921 - val_loss: 1.2092 - val_acc: 0.8701\n",
            "Epoch 14/20\n",
            "6300/6300 [==============================] - 1s 169us/step - loss: 0.0282 - acc: 0.9927 - val_loss: 1.1041 - val_acc: 0.8701\n",
            "Epoch 15/20\n",
            "6300/6300 [==============================] - 1s 166us/step - loss: 0.0205 - acc: 0.9941 - val_loss: 1.1264 - val_acc: 0.8756\n",
            "Epoch 16/20\n",
            "6300/6300 [==============================] - 1s 172us/step - loss: 0.0182 - acc: 0.9938 - val_loss: 1.1763 - val_acc: 0.8730\n",
            "Epoch 17/20\n",
            "6300/6300 [==============================] - 1s 175us/step - loss: 0.0208 - acc: 0.9925 - val_loss: 1.1890 - val_acc: 0.8759\n",
            "Epoch 18/20\n",
            "6300/6300 [==============================] - 1s 167us/step - loss: 0.0200 - acc: 0.9932 - val_loss: 1.2177 - val_acc: 0.8708\n",
            "Epoch 19/20\n",
            "6300/6300 [==============================] - 1s 167us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 1.1186 - val_acc: 0.8729\n",
            "Epoch 20/20\n",
            "6300/6300 [==============================] - 1s 168us/step - loss: 0.0256 - acc: 0.9916 - val_loss: 1.1323 - val_acc: 0.8709\n",
            "Test loss: 1.1323333855548117\n",
            "Test accuracy: 0.8709\n",
            "Train loss: 8.019180534567625e-05\n",
            "Train accuracy: 1.0\n",
            "x label: (6400, 28, 28)\n",
            "y label: (6400,)\n",
            "x unlabel: (53600, 28, 28)\n",
            "y unlabel: (53600,)\n",
            "63\n",
            "Train on 6400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6400/6400 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.9869 - val_loss: 1.0158 - val_acc: 0.8714\n",
            "Epoch 2/20\n",
            "6400/6400 [==============================] - 1s 170us/step - loss: 0.0418 - acc: 0.9903 - val_loss: 1.0901 - val_acc: 0.8734\n",
            "Epoch 3/20\n",
            "6400/6400 [==============================] - 1s 168us/step - loss: 0.0367 - acc: 0.9880 - val_loss: 1.0587 - val_acc: 0.8730\n",
            "Epoch 4/20\n",
            "6400/6400 [==============================] - 1s 167us/step - loss: 0.0279 - acc: 0.9903 - val_loss: 1.0664 - val_acc: 0.8717\n",
            "Epoch 5/20\n",
            "6400/6400 [==============================] - 1s 166us/step - loss: 0.0291 - acc: 0.9920 - val_loss: 1.0867 - val_acc: 0.8723\n",
            "Epoch 6/20\n",
            "6400/6400 [==============================] - 1s 169us/step - loss: 0.0298 - acc: 0.9895 - val_loss: 0.9936 - val_acc: 0.8747\n",
            "Epoch 7/20\n",
            "6400/6400 [==============================] - 1s 170us/step - loss: 0.0235 - acc: 0.9927 - val_loss: 1.0768 - val_acc: 0.8754\n",
            "Epoch 8/20\n",
            "6400/6400 [==============================] - 1s 169us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 1.1143 - val_acc: 0.8736\n",
            "Epoch 9/20\n",
            "6400/6400 [==============================] - 1s 167us/step - loss: 0.0207 - acc: 0.9930 - val_loss: 0.9577 - val_acc: 0.8732\n",
            "Epoch 10/20\n",
            "6400/6400 [==============================] - 1s 170us/step - loss: 0.0244 - acc: 0.9933 - val_loss: 1.0870 - val_acc: 0.8753\n",
            "Epoch 11/20\n",
            "6400/6400 [==============================] - 1s 168us/step - loss: 0.0232 - acc: 0.9942 - val_loss: 1.1025 - val_acc: 0.8754\n",
            "Epoch 12/20\n",
            "6400/6400 [==============================] - 1s 167us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 1.1244 - val_acc: 0.8728\n",
            "Epoch 13/20\n",
            "6400/6400 [==============================] - 1s 169us/step - loss: 0.0262 - acc: 0.9914 - val_loss: 1.0551 - val_acc: 0.8743\n",
            "Epoch 14/20\n",
            "6400/6400 [==============================] - 1s 166us/step - loss: 0.0282 - acc: 0.9922 - val_loss: 1.1927 - val_acc: 0.8719\n",
            "Epoch 15/20\n",
            "6400/6400 [==============================] - 1s 172us/step - loss: 0.0217 - acc: 0.9923 - val_loss: 1.1621 - val_acc: 0.8743\n",
            "Epoch 16/20\n",
            "6400/6400 [==============================] - 1s 166us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 1.1143 - val_acc: 0.8721\n",
            "Epoch 17/20\n",
            "6400/6400 [==============================] - 1s 166us/step - loss: 0.0176 - acc: 0.9933 - val_loss: 1.1763 - val_acc: 0.8731\n",
            "Epoch 18/20\n",
            "6400/6400 [==============================] - 1s 165us/step - loss: 0.0232 - acc: 0.9928 - val_loss: 1.2051 - val_acc: 0.8712\n",
            "Epoch 19/20\n",
            "6400/6400 [==============================] - 1s 166us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 1.0533 - val_acc: 0.8773\n",
            "Epoch 20/20\n",
            "6400/6400 [==============================] - 1s 169us/step - loss: 0.0149 - acc: 0.9948 - val_loss: 1.1759 - val_acc: 0.8699\n",
            "Test loss: 1.1759475187997548\n",
            "Test accuracy: 0.8699\n",
            "Train loss: 0.00016710082954716655\n",
            "Train accuracy: 0.99984375\n",
            "x label: (6500, 28, 28)\n",
            "y label: (6500,)\n",
            "x unlabel: (53500, 28, 28)\n",
            "y unlabel: (53500,)\n",
            "64\n",
            "Train on 6500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6500/6500 [==============================] - 1s 174us/step - loss: 0.0440 - acc: 0.9888 - val_loss: 1.1930 - val_acc: 0.8643\n",
            "Epoch 2/20\n",
            "6500/6500 [==============================] - 1s 170us/step - loss: 0.0364 - acc: 0.9909 - val_loss: 1.0973 - val_acc: 0.8722\n",
            "Epoch 3/20\n",
            "6500/6500 [==============================] - 1s 165us/step - loss: 0.0443 - acc: 0.9895 - val_loss: 1.1169 - val_acc: 0.8735\n",
            "Epoch 4/20\n",
            "6500/6500 [==============================] - 1s 171us/step - loss: 0.0265 - acc: 0.9931 - val_loss: 1.0792 - val_acc: 0.8737\n",
            "Epoch 5/20\n",
            "6500/6500 [==============================] - 1s 169us/step - loss: 0.0279 - acc: 0.9926 - val_loss: 1.1079 - val_acc: 0.8732\n",
            "Epoch 6/20\n",
            "6500/6500 [==============================] - 1s 174us/step - loss: 0.0340 - acc: 0.9906 - val_loss: 1.0814 - val_acc: 0.8765\n",
            "Epoch 7/20\n",
            "6500/6500 [==============================] - 1s 168us/step - loss: 0.0321 - acc: 0.9909 - val_loss: 1.0653 - val_acc: 0.8749\n",
            "Epoch 8/20\n",
            "6500/6500 [==============================] - 1s 169us/step - loss: 0.0287 - acc: 0.9905 - val_loss: 0.9362 - val_acc: 0.8781\n",
            "Epoch 9/20\n",
            "6500/6500 [==============================] - 1s 172us/step - loss: 0.0250 - acc: 0.9911 - val_loss: 1.1386 - val_acc: 0.8711\n",
            "Epoch 10/20\n",
            "6500/6500 [==============================] - 1s 168us/step - loss: 0.0264 - acc: 0.9926 - val_loss: 1.0266 - val_acc: 0.8765\n",
            "Epoch 11/20\n",
            "6500/6500 [==============================] - 1s 166us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 1.1415 - val_acc: 0.8754\n",
            "Epoch 12/20\n",
            "6500/6500 [==============================] - 1s 168us/step - loss: 0.0288 - acc: 0.9922 - val_loss: 1.0540 - val_acc: 0.8746\n",
            "Epoch 13/20\n",
            "6500/6500 [==============================] - 1s 168us/step - loss: 0.0239 - acc: 0.9929 - val_loss: 1.0610 - val_acc: 0.8750\n",
            "Epoch 14/20\n",
            "6500/6500 [==============================] - 1s 166us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 1.1748 - val_acc: 0.8730\n",
            "Epoch 15/20\n",
            "6500/6500 [==============================] - 1s 171us/step - loss: 0.0313 - acc: 0.9909 - val_loss: 1.0412 - val_acc: 0.8741\n",
            "Epoch 16/20\n",
            "6500/6500 [==============================] - 1s 169us/step - loss: 0.0169 - acc: 0.9940 - val_loss: 1.0250 - val_acc: 0.8749\n",
            "Epoch 17/20\n",
            "6500/6500 [==============================] - 1s 166us/step - loss: 0.0183 - acc: 0.9951 - val_loss: 1.1302 - val_acc: 0.8727\n",
            "Epoch 18/20\n",
            "6500/6500 [==============================] - 1s 165us/step - loss: 0.0198 - acc: 0.9934 - val_loss: 1.0861 - val_acc: 0.8751\n",
            "Epoch 19/20\n",
            "6500/6500 [==============================] - 1s 170us/step - loss: 0.0204 - acc: 0.9931 - val_loss: 1.1242 - val_acc: 0.8743\n",
            "Epoch 20/20\n",
            "6500/6500 [==============================] - 1s 167us/step - loss: 0.0158 - acc: 0.9937 - val_loss: 1.1908 - val_acc: 0.8694\n",
            "Test loss: 1.1907509961773002\n",
            "Test accuracy: 0.8694\n",
            "Train loss: 0.00011909523899591407\n",
            "Train accuracy: 1.0\n",
            "x label: (6600, 28, 28)\n",
            "y label: (6600,)\n",
            "x unlabel: (53400, 28, 28)\n",
            "y unlabel: (53400,)\n",
            "65\n",
            "Train on 6600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6600/6600 [==============================] - 1s 171us/step - loss: 0.0408 - acc: 0.9882 - val_loss: 1.0947 - val_acc: 0.8750\n",
            "Epoch 2/20\n",
            "6600/6600 [==============================] - 1s 169us/step - loss: 0.0423 - acc: 0.9880 - val_loss: 1.1051 - val_acc: 0.8713\n",
            "Epoch 3/20\n",
            "6600/6600 [==============================] - 1s 174us/step - loss: 0.0366 - acc: 0.9908 - val_loss: 1.0034 - val_acc: 0.8768\n",
            "Epoch 4/20\n",
            "6600/6600 [==============================] - 1s 168us/step - loss: 0.0382 - acc: 0.9891 - val_loss: 1.1355 - val_acc: 0.8750\n",
            "Epoch 5/20\n",
            "6600/6600 [==============================] - 1s 171us/step - loss: 0.0341 - acc: 0.9911 - val_loss: 1.0007 - val_acc: 0.8728\n",
            "Epoch 6/20\n",
            "6600/6600 [==============================] - 1s 167us/step - loss: 0.0292 - acc: 0.9923 - val_loss: 1.0977 - val_acc: 0.8744\n",
            "Epoch 7/20\n",
            "6600/6600 [==============================] - 1s 171us/step - loss: 0.0284 - acc: 0.9912 - val_loss: 1.1049 - val_acc: 0.8759\n",
            "Epoch 8/20\n",
            "6600/6600 [==============================] - 1s 169us/step - loss: 0.0197 - acc: 0.9920 - val_loss: 1.1218 - val_acc: 0.8782\n",
            "Epoch 9/20\n",
            "6600/6600 [==============================] - 1s 168us/step - loss: 0.0332 - acc: 0.9921 - val_loss: 1.0698 - val_acc: 0.8733\n",
            "Epoch 10/20\n",
            "6600/6600 [==============================] - 1s 169us/step - loss: 0.0177 - acc: 0.9952 - val_loss: 1.1715 - val_acc: 0.8713\n",
            "Epoch 11/20\n",
            "6600/6600 [==============================] - 1s 167us/step - loss: 0.0232 - acc: 0.9944 - val_loss: 1.1215 - val_acc: 0.8738\n",
            "Epoch 12/20\n",
            "6600/6600 [==============================] - 1s 163us/step - loss: 0.0229 - acc: 0.9930 - val_loss: 1.1497 - val_acc: 0.8752\n",
            "Epoch 13/20\n",
            "6600/6600 [==============================] - 1s 168us/step - loss: 0.0231 - acc: 0.9923 - val_loss: 1.2401 - val_acc: 0.8676\n",
            "Epoch 14/20\n",
            "6600/6600 [==============================] - 1s 170us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 1.1617 - val_acc: 0.8725\n",
            "Epoch 15/20\n",
            "6600/6600 [==============================] - 1s 168us/step - loss: 0.0215 - acc: 0.9939 - val_loss: 1.0994 - val_acc: 0.8708\n",
            "Epoch 16/20\n",
            "6600/6600 [==============================] - 1s 165us/step - loss: 0.0210 - acc: 0.9929 - val_loss: 1.0463 - val_acc: 0.8746\n",
            "Epoch 17/20\n",
            "6600/6600 [==============================] - 1s 167us/step - loss: 0.0221 - acc: 0.9936 - val_loss: 1.1283 - val_acc: 0.8728\n",
            "Epoch 18/20\n",
            "6600/6600 [==============================] - 1s 166us/step - loss: 0.0248 - acc: 0.9924 - val_loss: 1.0941 - val_acc: 0.8747\n",
            "Epoch 19/20\n",
            "6600/6600 [==============================] - 1s 169us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 1.1840 - val_acc: 0.8751\n",
            "Epoch 20/20\n",
            "6600/6600 [==============================] - 1s 170us/step - loss: 0.0241 - acc: 0.9917 - val_loss: 1.2029 - val_acc: 0.8714\n",
            "Test loss: 1.202944942849134\n",
            "Test accuracy: 0.8714\n",
            "Train loss: 0.00015078950240001978\n",
            "Train accuracy: 1.0\n",
            "x label: (6700, 28, 28)\n",
            "y label: (6700,)\n",
            "x unlabel: (53300, 28, 28)\n",
            "y unlabel: (53300,)\n",
            "66\n",
            "Train on 6700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6700/6700 [==============================] - 1s 165us/step - loss: 0.0382 - acc: 0.9893 - val_loss: 1.0233 - val_acc: 0.8761\n",
            "Epoch 2/20\n",
            "6700/6700 [==============================] - 1s 166us/step - loss: 0.0316 - acc: 0.9907 - val_loss: 1.0968 - val_acc: 0.8734\n",
            "Epoch 3/20\n",
            "6700/6700 [==============================] - 1s 164us/step - loss: 0.0342 - acc: 0.9888 - val_loss: 1.0592 - val_acc: 0.8757\n",
            "Epoch 4/20\n",
            "6700/6700 [==============================] - 1s 166us/step - loss: 0.0329 - acc: 0.9899 - val_loss: 1.1128 - val_acc: 0.8738\n",
            "Epoch 5/20\n",
            "6700/6700 [==============================] - 1s 165us/step - loss: 0.0368 - acc: 0.9899 - val_loss: 1.0394 - val_acc: 0.8751\n",
            "Epoch 6/20\n",
            "6700/6700 [==============================] - 1s 168us/step - loss: 0.0298 - acc: 0.9910 - val_loss: 1.1004 - val_acc: 0.8713\n",
            "Epoch 7/20\n",
            "6700/6700 [==============================] - 1s 160us/step - loss: 0.0248 - acc: 0.9934 - val_loss: 1.0086 - val_acc: 0.8759\n",
            "Epoch 8/20\n",
            "6700/6700 [==============================] - 1s 171us/step - loss: 0.0194 - acc: 0.9933 - val_loss: 1.0222 - val_acc: 0.8764\n",
            "Epoch 9/20\n",
            "6700/6700 [==============================] - 1s 165us/step - loss: 0.0220 - acc: 0.9930 - val_loss: 1.0964 - val_acc: 0.8784\n",
            "Epoch 10/20\n",
            "6700/6700 [==============================] - 1s 164us/step - loss: 0.0253 - acc: 0.9918 - val_loss: 1.1159 - val_acc: 0.8715\n",
            "Epoch 11/20\n",
            "6700/6700 [==============================] - 1s 175us/step - loss: 0.0288 - acc: 0.9909 - val_loss: 1.0674 - val_acc: 0.8785\n",
            "Epoch 12/20\n",
            "6700/6700 [==============================] - 1s 163us/step - loss: 0.0180 - acc: 0.9928 - val_loss: 1.1388 - val_acc: 0.8735\n",
            "Epoch 13/20\n",
            "6700/6700 [==============================] - 1s 167us/step - loss: 0.0241 - acc: 0.9931 - val_loss: 1.1860 - val_acc: 0.8740\n",
            "Epoch 14/20\n",
            "6700/6700 [==============================] - 1s 160us/step - loss: 0.0210 - acc: 0.9930 - val_loss: 1.1495 - val_acc: 0.8747\n",
            "Epoch 15/20\n",
            "6700/6700 [==============================] - 1s 167us/step - loss: 0.0216 - acc: 0.9934 - val_loss: 1.0289 - val_acc: 0.8744\n",
            "Epoch 16/20\n",
            "6700/6700 [==============================] - 1s 163us/step - loss: 0.0185 - acc: 0.9940 - val_loss: 1.0800 - val_acc: 0.8736\n",
            "Epoch 17/20\n",
            "6700/6700 [==============================] - 1s 161us/step - loss: 0.0216 - acc: 0.9946 - val_loss: 1.1631 - val_acc: 0.8781\n",
            "Epoch 18/20\n",
            "6700/6700 [==============================] - 1s 166us/step - loss: 0.0260 - acc: 0.9930 - val_loss: 1.1339 - val_acc: 0.8725\n",
            "Epoch 19/20\n",
            "6700/6700 [==============================] - 1s 164us/step - loss: 0.0181 - acc: 0.9940 - val_loss: 1.1163 - val_acc: 0.8763\n",
            "Epoch 20/20\n",
            "6700/6700 [==============================] - 1s 169us/step - loss: 0.0255 - acc: 0.9907 - val_loss: 1.1328 - val_acc: 0.8734\n",
            "Test loss: 1.1327532732856107\n",
            "Test accuracy: 0.8734\n",
            "Train loss: 0.00015049787021895886\n",
            "Train accuracy: 1.0\n",
            "x label: (6800, 28, 28)\n",
            "y label: (6800,)\n",
            "x unlabel: (53200, 28, 28)\n",
            "y unlabel: (53200,)\n",
            "67\n",
            "Train on 6800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6800/6800 [==============================] - 1s 165us/step - loss: 0.0499 - acc: 0.9871 - val_loss: 0.9467 - val_acc: 0.8749\n",
            "Epoch 2/20\n",
            "6800/6800 [==============================] - 1s 168us/step - loss: 0.0369 - acc: 0.9899 - val_loss: 1.1203 - val_acc: 0.8735\n",
            "Epoch 3/20\n",
            "6800/6800 [==============================] - 1s 162us/step - loss: 0.0408 - acc: 0.9893 - val_loss: 1.1034 - val_acc: 0.8725\n",
            "Epoch 4/20\n",
            "6800/6800 [==============================] - 1s 165us/step - loss: 0.0315 - acc: 0.9900 - val_loss: 1.0766 - val_acc: 0.8770\n",
            "Epoch 5/20\n",
            "6800/6800 [==============================] - 1s 164us/step - loss: 0.0332 - acc: 0.9913 - val_loss: 1.0829 - val_acc: 0.8763\n",
            "Epoch 6/20\n",
            "6800/6800 [==============================] - 1s 160us/step - loss: 0.0391 - acc: 0.9907 - val_loss: 1.1002 - val_acc: 0.8759\n",
            "Epoch 7/20\n",
            "6800/6800 [==============================] - 1s 163us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 1.0259 - val_acc: 0.8792\n",
            "Epoch 8/20\n",
            "6800/6800 [==============================] - 1s 163us/step - loss: 0.0357 - acc: 0.9897 - val_loss: 1.1266 - val_acc: 0.8740\n",
            "Epoch 9/20\n",
            "6800/6800 [==============================] - 1s 167us/step - loss: 0.0254 - acc: 0.9916 - val_loss: 1.0153 - val_acc: 0.8770\n",
            "Epoch 10/20\n",
            "6800/6800 [==============================] - 1s 168us/step - loss: 0.0204 - acc: 0.9943 - val_loss: 1.0683 - val_acc: 0.8774\n",
            "Epoch 11/20\n",
            "6800/6800 [==============================] - 1s 167us/step - loss: 0.0246 - acc: 0.9926 - val_loss: 1.0714 - val_acc: 0.8800\n",
            "Epoch 12/20\n",
            "6800/6800 [==============================] - 1s 167us/step - loss: 0.0180 - acc: 0.9941 - val_loss: 1.1574 - val_acc: 0.8756\n",
            "Epoch 13/20\n",
            "6800/6800 [==============================] - 1s 166us/step - loss: 0.0211 - acc: 0.9928 - val_loss: 1.1496 - val_acc: 0.8737\n",
            "Epoch 14/20\n",
            "6800/6800 [==============================] - 1s 172us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 1.1500 - val_acc: 0.8759\n",
            "Epoch 15/20\n",
            "6800/6800 [==============================] - 1s 172us/step - loss: 0.0256 - acc: 0.9922 - val_loss: 1.0496 - val_acc: 0.8760\n",
            "Epoch 16/20\n",
            "6800/6800 [==============================] - 1s 167us/step - loss: 0.0258 - acc: 0.9918 - val_loss: 0.9830 - val_acc: 0.8777\n",
            "Epoch 17/20\n",
            "6800/6800 [==============================] - 1s 168us/step - loss: 0.0222 - acc: 0.9932 - val_loss: 1.1509 - val_acc: 0.8767\n",
            "Epoch 18/20\n",
            "6800/6800 [==============================] - 1s 175us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 1.1749 - val_acc: 0.8811\n",
            "Epoch 19/20\n",
            "6800/6800 [==============================] - 1s 174us/step - loss: 0.0133 - acc: 0.9960 - val_loss: 1.1455 - val_acc: 0.8812\n",
            "Epoch 20/20\n",
            "6800/6800 [==============================] - 1s 173us/step - loss: 0.0217 - acc: 0.9926 - val_loss: 1.0442 - val_acc: 0.8757\n",
            "Test loss: 1.0441991852812411\n",
            "Test accuracy: 0.8757\n",
            "Train loss: 0.0007746971024472018\n",
            "Train accuracy: 0.9998529411764706\n",
            "x label: (6900, 28, 28)\n",
            "y label: (6900,)\n",
            "x unlabel: (53100, 28, 28)\n",
            "y unlabel: (53100,)\n",
            "68\n",
            "Train on 6900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "6900/6900 [==============================] - 1s 171us/step - loss: 0.0458 - acc: 0.9874 - val_loss: 0.9724 - val_acc: 0.8771\n",
            "Epoch 2/20\n",
            "6900/6900 [==============================] - 1s 172us/step - loss: 0.0413 - acc: 0.9893 - val_loss: 1.1226 - val_acc: 0.8783\n",
            "Epoch 3/20\n",
            "6900/6900 [==============================] - 1s 166us/step - loss: 0.0381 - acc: 0.9894 - val_loss: 1.0567 - val_acc: 0.8754\n",
            "Epoch 4/20\n",
            "6900/6900 [==============================] - 1s 167us/step - loss: 0.0323 - acc: 0.9920 - val_loss: 1.0592 - val_acc: 0.8774\n",
            "Epoch 5/20\n",
            "6900/6900 [==============================] - 1s 165us/step - loss: 0.0347 - acc: 0.9909 - val_loss: 1.0924 - val_acc: 0.8784\n",
            "Epoch 6/20\n",
            "6900/6900 [==============================] - 1s 164us/step - loss: 0.0259 - acc: 0.9920 - val_loss: 1.0345 - val_acc: 0.8788\n",
            "Epoch 7/20\n",
            "6900/6900 [==============================] - 1s 166us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 1.1054 - val_acc: 0.8789\n",
            "Epoch 8/20\n",
            "6900/6900 [==============================] - 1s 165us/step - loss: 0.0281 - acc: 0.9903 - val_loss: 1.1085 - val_acc: 0.8768\n",
            "Epoch 9/20\n",
            "6900/6900 [==============================] - 1s 162us/step - loss: 0.0379 - acc: 0.9899 - val_loss: 0.9945 - val_acc: 0.8792\n",
            "Epoch 10/20\n",
            "6900/6900 [==============================] - 1s 159us/step - loss: 0.0226 - acc: 0.9920 - val_loss: 1.0841 - val_acc: 0.8755\n",
            "Epoch 11/20\n",
            "6900/6900 [==============================] - 1s 163us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 1.1909 - val_acc: 0.8744\n",
            "Epoch 12/20\n",
            "6900/6900 [==============================] - 1s 163us/step - loss: 0.0216 - acc: 0.9926 - val_loss: 1.1154 - val_acc: 0.8759\n",
            "Epoch 13/20\n",
            "6900/6900 [==============================] - 1s 167us/step - loss: 0.0184 - acc: 0.9946 - val_loss: 1.1636 - val_acc: 0.8781\n",
            "Epoch 14/20\n",
            "6900/6900 [==============================] - 1s 164us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 1.1701 - val_acc: 0.8773\n",
            "Epoch 15/20\n",
            "6900/6900 [==============================] - 1s 167us/step - loss: 0.0224 - acc: 0.9925 - val_loss: 1.0806 - val_acc: 0.8768\n",
            "Epoch 16/20\n",
            "6900/6900 [==============================] - 1s 161us/step - loss: 0.0180 - acc: 0.9945 - val_loss: 1.0676 - val_acc: 0.8764\n",
            "Epoch 17/20\n",
            "6900/6900 [==============================] - 1s 167us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 1.1005 - val_acc: 0.8739\n",
            "Epoch 18/20\n",
            "6900/6900 [==============================] - 1s 168us/step - loss: 0.0187 - acc: 0.9926 - val_loss: 1.1438 - val_acc: 0.8777\n",
            "Epoch 19/20\n",
            "6900/6900 [==============================] - 1s 162us/step - loss: 0.0167 - acc: 0.9948 - val_loss: 1.1464 - val_acc: 0.8790\n",
            "Epoch 20/20\n",
            "6900/6900 [==============================] - 1s 165us/step - loss: 0.0151 - acc: 0.9942 - val_loss: 1.1948 - val_acc: 0.8736\n",
            "Test loss: 1.194822923360124\n",
            "Test accuracy: 0.8736\n",
            "Train loss: 0.00026382507417163626\n",
            "Train accuracy: 1.0\n",
            "x label: (7000, 28, 28)\n",
            "y label: (7000,)\n",
            "x unlabel: (53000, 28, 28)\n",
            "y unlabel: (53000,)\n",
            "69\n",
            "Train on 7000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7000/7000 [==============================] - 1s 164us/step - loss: 0.0402 - acc: 0.9891 - val_loss: 1.0351 - val_acc: 0.8771\n",
            "Epoch 2/20\n",
            "7000/7000 [==============================] - 1s 163us/step - loss: 0.0480 - acc: 0.9881 - val_loss: 1.0576 - val_acc: 0.8789\n",
            "Epoch 3/20\n",
            "7000/7000 [==============================] - 1s 164us/step - loss: 0.0342 - acc: 0.9920 - val_loss: 1.0621 - val_acc: 0.8749\n",
            "Epoch 4/20\n",
            "7000/7000 [==============================] - 1s 160us/step - loss: 0.0331 - acc: 0.9904 - val_loss: 1.1989 - val_acc: 0.8757\n",
            "Epoch 5/20\n",
            "7000/7000 [==============================] - 1s 162us/step - loss: 0.0329 - acc: 0.9913 - val_loss: 1.0742 - val_acc: 0.8784\n",
            "Epoch 6/20\n",
            "7000/7000 [==============================] - 1s 161us/step - loss: 0.0232 - acc: 0.9930 - val_loss: 1.1072 - val_acc: 0.8774\n",
            "Epoch 7/20\n",
            "7000/7000 [==============================] - 1s 160us/step - loss: 0.0296 - acc: 0.9916 - val_loss: 1.0572 - val_acc: 0.8731\n",
            "Epoch 8/20\n",
            "7000/7000 [==============================] - 1s 160us/step - loss: 0.0235 - acc: 0.9917 - val_loss: 1.1330 - val_acc: 0.8763\n",
            "Epoch 9/20\n",
            "7000/7000 [==============================] - 1s 163us/step - loss: 0.0257 - acc: 0.9927 - val_loss: 1.0956 - val_acc: 0.8814\n",
            "Epoch 10/20\n",
            "7000/7000 [==============================] - 1s 166us/step - loss: 0.0196 - acc: 0.9930 - val_loss: 1.1911 - val_acc: 0.8759\n",
            "Epoch 11/20\n",
            "7000/7000 [==============================] - 1s 164us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 1.0759 - val_acc: 0.8766\n",
            "Epoch 12/20\n",
            "7000/7000 [==============================] - 1s 164us/step - loss: 0.0188 - acc: 0.9937 - val_loss: 1.1808 - val_acc: 0.8742\n",
            "Epoch 13/20\n",
            "7000/7000 [==============================] - 1s 161us/step - loss: 0.0164 - acc: 0.9949 - val_loss: 1.1480 - val_acc: 0.8783\n",
            "Epoch 14/20\n",
            "7000/7000 [==============================] - 1s 161us/step - loss: 0.0240 - acc: 0.9934 - val_loss: 1.0960 - val_acc: 0.8789\n",
            "Epoch 15/20\n",
            "7000/7000 [==============================] - 1s 159us/step - loss: 0.0326 - acc: 0.9907 - val_loss: 1.0247 - val_acc: 0.8732\n",
            "Epoch 16/20\n",
            "7000/7000 [==============================] - 1s 163us/step - loss: 0.0242 - acc: 0.9929 - val_loss: 1.0876 - val_acc: 0.8772\n",
            "Epoch 17/20\n",
            "7000/7000 [==============================] - 1s 160us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 1.0433 - val_acc: 0.8766\n",
            "Epoch 18/20\n",
            "7000/7000 [==============================] - 1s 165us/step - loss: 0.0237 - acc: 0.9929 - val_loss: 1.0935 - val_acc: 0.8738\n",
            "Epoch 19/20\n",
            "7000/7000 [==============================] - 1s 159us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 1.0768 - val_acc: 0.8789\n",
            "Epoch 20/20\n",
            "7000/7000 [==============================] - 1s 163us/step - loss: 0.0198 - acc: 0.9939 - val_loss: 1.0570 - val_acc: 0.8776\n",
            "Test loss: 1.0569677714460441\n",
            "Test accuracy: 0.8776\n",
            "Train loss: 0.00025685821569777545\n",
            "Train accuracy: 1.0\n",
            "x label: (7100, 28, 28)\n",
            "y label: (7100,)\n",
            "x unlabel: (52900, 28, 28)\n",
            "y unlabel: (52900,)\n",
            "70\n",
            "Train on 7100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7100/7100 [==============================] - 1s 164us/step - loss: 0.0433 - acc: 0.9892 - val_loss: 0.9870 - val_acc: 0.8773\n",
            "Epoch 2/20\n",
            "7100/7100 [==============================] - 1s 161us/step - loss: 0.0409 - acc: 0.9900 - val_loss: 0.9659 - val_acc: 0.8814\n",
            "Epoch 3/20\n",
            "7100/7100 [==============================] - 1s 163us/step - loss: 0.0318 - acc: 0.9903 - val_loss: 1.1858 - val_acc: 0.8738\n",
            "Epoch 4/20\n",
            "7100/7100 [==============================] - 1s 164us/step - loss: 0.0309 - acc: 0.9896 - val_loss: 1.1367 - val_acc: 0.8793\n",
            "Epoch 5/20\n",
            "7100/7100 [==============================] - 1s 167us/step - loss: 0.0304 - acc: 0.9901 - val_loss: 1.0618 - val_acc: 0.8771\n",
            "Epoch 6/20\n",
            "7100/7100 [==============================] - 1s 162us/step - loss: 0.0228 - acc: 0.9928 - val_loss: 1.0856 - val_acc: 0.8703\n",
            "Epoch 7/20\n",
            "7100/7100 [==============================] - 1s 159us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 1.1149 - val_acc: 0.8740\n",
            "Epoch 8/20\n",
            "7100/7100 [==============================] - 1s 161us/step - loss: 0.0274 - acc: 0.9931 - val_loss: 1.0794 - val_acc: 0.8767\n",
            "Epoch 9/20\n",
            "7100/7100 [==============================] - 1s 160us/step - loss: 0.0238 - acc: 0.9932 - val_loss: 1.0924 - val_acc: 0.8778\n",
            "Epoch 10/20\n",
            "7100/7100 [==============================] - 1s 161us/step - loss: 0.0230 - acc: 0.9921 - val_loss: 1.1141 - val_acc: 0.8776\n",
            "Epoch 11/20\n",
            "7100/7100 [==============================] - 1s 164us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 1.1108 - val_acc: 0.8765\n",
            "Epoch 12/20\n",
            "7100/7100 [==============================] - 1s 162us/step - loss: 0.0259 - acc: 0.9911 - val_loss: 1.1801 - val_acc: 0.8721\n",
            "Epoch 13/20\n",
            "7100/7100 [==============================] - 1s 168us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 1.1873 - val_acc: 0.8750\n",
            "Epoch 14/20\n",
            "7100/7100 [==============================] - 1s 162us/step - loss: 0.0238 - acc: 0.9932 - val_loss: 1.0427 - val_acc: 0.8750\n",
            "Epoch 15/20\n",
            "7100/7100 [==============================] - 1s 160us/step - loss: 0.0178 - acc: 0.9951 - val_loss: 1.2328 - val_acc: 0.8723\n",
            "Epoch 16/20\n",
            "7100/7100 [==============================] - 1s 160us/step - loss: 0.0170 - acc: 0.9948 - val_loss: 1.0469 - val_acc: 0.8795\n",
            "Epoch 17/20\n",
            "7100/7100 [==============================] - 1s 159us/step - loss: 0.0252 - acc: 0.9915 - val_loss: 1.0954 - val_acc: 0.8784\n",
            "Epoch 18/20\n",
            "7100/7100 [==============================] - 1s 160us/step - loss: 0.0185 - acc: 0.9935 - val_loss: 1.1101 - val_acc: 0.8670\n",
            "Epoch 19/20\n",
            "7100/7100 [==============================] - 1s 161us/step - loss: 0.0206 - acc: 0.9932 - val_loss: 1.0334 - val_acc: 0.8743\n",
            "Epoch 20/20\n",
            "7100/7100 [==============================] - 1s 161us/step - loss: 0.0148 - acc: 0.9948 - val_loss: 1.1157 - val_acc: 0.8740\n",
            "Test loss: 1.1156937133951301\n",
            "Test accuracy: 0.874\n",
            "Train loss: 0.0005789420358765506\n",
            "Train accuracy: 0.999718309926315\n",
            "x label: (7200, 28, 28)\n",
            "y label: (7200,)\n",
            "x unlabel: (52800, 28, 28)\n",
            "y unlabel: (52800,)\n",
            "71\n",
            "Train on 7200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7200/7200 [==============================] - 1s 168us/step - loss: 0.0487 - acc: 0.9893 - val_loss: 1.0075 - val_acc: 0.8800\n",
            "Epoch 2/20\n",
            "7200/7200 [==============================] - 1s 161us/step - loss: 0.0382 - acc: 0.9910 - val_loss: 1.1169 - val_acc: 0.8763\n",
            "Epoch 3/20\n",
            "7200/7200 [==============================] - 1s 164us/step - loss: 0.0380 - acc: 0.9890 - val_loss: 0.9773 - val_acc: 0.8757\n",
            "Epoch 4/20\n",
            "7200/7200 [==============================] - 1s 165us/step - loss: 0.0441 - acc: 0.9890 - val_loss: 1.0171 - val_acc: 0.8790\n",
            "Epoch 5/20\n",
            "7200/7200 [==============================] - 1s 164us/step - loss: 0.0363 - acc: 0.9907 - val_loss: 0.9642 - val_acc: 0.8802\n",
            "Epoch 6/20\n",
            "7200/7200 [==============================] - 1s 164us/step - loss: 0.0347 - acc: 0.9894 - val_loss: 1.0687 - val_acc: 0.8810\n",
            "Epoch 7/20\n",
            "7200/7200 [==============================] - 1s 165us/step - loss: 0.0362 - acc: 0.9921 - val_loss: 1.0637 - val_acc: 0.8778\n",
            "Epoch 8/20\n",
            "7200/7200 [==============================] - 1s 167us/step - loss: 0.0312 - acc: 0.9906 - val_loss: 0.9988 - val_acc: 0.8773\n",
            "Epoch 9/20\n",
            "7200/7200 [==============================] - 1s 162us/step - loss: 0.0261 - acc: 0.9921 - val_loss: 1.0126 - val_acc: 0.8835\n",
            "Epoch 10/20\n",
            "7200/7200 [==============================] - 1s 165us/step - loss: 0.0333 - acc: 0.9907 - val_loss: 1.0969 - val_acc: 0.8770\n",
            "Epoch 11/20\n",
            "7200/7200 [==============================] - 1s 165us/step - loss: 0.0302 - acc: 0.9919 - val_loss: 1.0124 - val_acc: 0.8831\n",
            "Epoch 12/20\n",
            "7200/7200 [==============================] - 1s 162us/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.9871 - val_acc: 0.8800\n",
            "Epoch 13/20\n",
            "7200/7200 [==============================] - 1s 162us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 1.0574 - val_acc: 0.8741\n",
            "Epoch 14/20\n",
            "7200/7200 [==============================] - 1s 163us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.9506 - val_acc: 0.8755\n",
            "Epoch 15/20\n",
            "7200/7200 [==============================] - 1s 161us/step - loss: 0.0235 - acc: 0.9926 - val_loss: 1.1563 - val_acc: 0.8731\n",
            "Epoch 16/20\n",
            "7200/7200 [==============================] - 1s 162us/step - loss: 0.0249 - acc: 0.9932 - val_loss: 1.0790 - val_acc: 0.8788\n",
            "Epoch 17/20\n",
            "7200/7200 [==============================] - 1s 165us/step - loss: 0.0217 - acc: 0.9936 - val_loss: 1.0786 - val_acc: 0.8808\n",
            "Epoch 18/20\n",
            "7200/7200 [==============================] - 1s 161us/step - loss: 0.0225 - acc: 0.9939 - val_loss: 1.1330 - val_acc: 0.8788\n",
            "Epoch 19/20\n",
            "7200/7200 [==============================] - 1s 157us/step - loss: 0.0225 - acc: 0.9940 - val_loss: 1.1740 - val_acc: 0.8738\n",
            "Epoch 20/20\n",
            "7200/7200 [==============================] - 1s 159us/step - loss: 0.0237 - acc: 0.9935 - val_loss: 1.1137 - val_acc: 0.8781\n",
            "Test loss: 1.1137302026733633\n",
            "Test accuracy: 0.8781\n",
            "Train loss: 0.0023433813371605714\n",
            "Train accuracy: 0.9998611111111111\n",
            "x label: (7300, 28, 28)\n",
            "y label: (7300,)\n",
            "x unlabel: (52700, 28, 28)\n",
            "y unlabel: (52700,)\n",
            "72\n",
            "Train on 7300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7300/7300 [==============================] - 1s 165us/step - loss: 0.0438 - acc: 0.9873 - val_loss: 1.0233 - val_acc: 0.8740\n",
            "Epoch 2/20\n",
            "7300/7300 [==============================] - 1s 159us/step - loss: 0.0395 - acc: 0.9896 - val_loss: 0.9898 - val_acc: 0.8841\n",
            "Epoch 3/20\n",
            "7300/7300 [==============================] - 1s 165us/step - loss: 0.0315 - acc: 0.9918 - val_loss: 0.9594 - val_acc: 0.8803\n",
            "Epoch 4/20\n",
            "7300/7300 [==============================] - 1s 162us/step - loss: 0.0330 - acc: 0.9905 - val_loss: 0.9785 - val_acc: 0.8808\n",
            "Epoch 5/20\n",
            "7300/7300 [==============================] - 1s 162us/step - loss: 0.0395 - acc: 0.9886 - val_loss: 0.9758 - val_acc: 0.8781\n",
            "Epoch 6/20\n",
            "7300/7300 [==============================] - 1s 162us/step - loss: 0.0311 - acc: 0.9918 - val_loss: 1.0523 - val_acc: 0.8795\n",
            "Epoch 7/20\n",
            "7300/7300 [==============================] - 1s 161us/step - loss: 0.0214 - acc: 0.9932 - val_loss: 1.0683 - val_acc: 0.8742\n",
            "Epoch 8/20\n",
            "7300/7300 [==============================] - 1s 161us/step - loss: 0.0298 - acc: 0.9926 - val_loss: 1.0003 - val_acc: 0.8831\n",
            "Epoch 9/20\n",
            "7300/7300 [==============================] - 1s 164us/step - loss: 0.0293 - acc: 0.9930 - val_loss: 0.9907 - val_acc: 0.8864\n",
            "Epoch 10/20\n",
            "7300/7300 [==============================] - 1s 162us/step - loss: 0.0222 - acc: 0.9937 - val_loss: 1.0029 - val_acc: 0.8823\n",
            "Epoch 11/20\n",
            "7300/7300 [==============================] - 1s 164us/step - loss: 0.0271 - acc: 0.9934 - val_loss: 1.0129 - val_acc: 0.8785\n",
            "Epoch 12/20\n",
            "7300/7300 [==============================] - 1s 163us/step - loss: 0.0235 - acc: 0.9932 - val_loss: 1.0147 - val_acc: 0.8813\n",
            "Epoch 13/20\n",
            "7300/7300 [==============================] - 1s 163us/step - loss: 0.0273 - acc: 0.9923 - val_loss: 1.0657 - val_acc: 0.8794\n",
            "Epoch 14/20\n",
            "7300/7300 [==============================] - 1s 160us/step - loss: 0.0275 - acc: 0.9918 - val_loss: 1.0194 - val_acc: 0.8755\n",
            "Epoch 15/20\n",
            "7300/7300 [==============================] - 1s 162us/step - loss: 0.0260 - acc: 0.9929 - val_loss: 1.0452 - val_acc: 0.8747\n",
            "Epoch 16/20\n",
            "7300/7300 [==============================] - 1s 159us/step - loss: 0.0202 - acc: 0.9934 - val_loss: 1.1163 - val_acc: 0.8781\n",
            "Epoch 17/20\n",
            "7300/7300 [==============================] - 1s 158us/step - loss: 0.0229 - acc: 0.9934 - val_loss: 1.0827 - val_acc: 0.8786\n",
            "Epoch 18/20\n",
            "7300/7300 [==============================] - 1s 163us/step - loss: 0.0219 - acc: 0.9942 - val_loss: 1.0659 - val_acc: 0.8741\n",
            "Epoch 19/20\n",
            "7300/7300 [==============================] - 1s 160us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 1.0704 - val_acc: 0.8799\n",
            "Epoch 20/20\n",
            "7300/7300 [==============================] - 1s 163us/step - loss: 0.0217 - acc: 0.9942 - val_loss: 1.1309 - val_acc: 0.8775\n",
            "Test loss: 1.130872099792306\n",
            "Test accuracy: 0.8775\n",
            "Train loss: 0.002276279413105516\n",
            "Train accuracy: 0.9998630136986302\n",
            "x label: (7400, 28, 28)\n",
            "y label: (7400,)\n",
            "x unlabel: (52600, 28, 28)\n",
            "y unlabel: (52600,)\n",
            "73\n",
            "Train on 7400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7400/7400 [==============================] - 1s 165us/step - loss: 0.0562 - acc: 0.9849 - val_loss: 1.0248 - val_acc: 0.8811\n",
            "Epoch 2/20\n",
            "7400/7400 [==============================] - 1s 162us/step - loss: 0.0485 - acc: 0.9877 - val_loss: 0.9147 - val_acc: 0.8856\n",
            "Epoch 3/20\n",
            "7400/7400 [==============================] - 1s 161us/step - loss: 0.0345 - acc: 0.9919 - val_loss: 0.9325 - val_acc: 0.8806\n",
            "Epoch 4/20\n",
            "7400/7400 [==============================] - 1s 158us/step - loss: 0.0400 - acc: 0.9900 - val_loss: 1.0060 - val_acc: 0.8805\n",
            "Epoch 5/20\n",
            "7400/7400 [==============================] - 1s 165us/step - loss: 0.0358 - acc: 0.9914 - val_loss: 1.0094 - val_acc: 0.8827\n",
            "Epoch 6/20\n",
            "7400/7400 [==============================] - 1s 160us/step - loss: 0.0357 - acc: 0.9907 - val_loss: 0.9561 - val_acc: 0.8806\n",
            "Epoch 7/20\n",
            "7400/7400 [==============================] - 1s 160us/step - loss: 0.0375 - acc: 0.9899 - val_loss: 1.0062 - val_acc: 0.8812\n",
            "Epoch 8/20\n",
            "7400/7400 [==============================] - 1s 159us/step - loss: 0.0355 - acc: 0.9916 - val_loss: 1.0886 - val_acc: 0.8821\n",
            "Epoch 9/20\n",
            "7400/7400 [==============================] - 1s 161us/step - loss: 0.0287 - acc: 0.9930 - val_loss: 1.0403 - val_acc: 0.8782\n",
            "Epoch 10/20\n",
            "7400/7400 [==============================] - 1s 159us/step - loss: 0.0334 - acc: 0.9924 - val_loss: 1.1399 - val_acc: 0.8780\n",
            "Epoch 11/20\n",
            "7400/7400 [==============================] - 1s 161us/step - loss: 0.0340 - acc: 0.9914 - val_loss: 0.9529 - val_acc: 0.8797\n",
            "Epoch 12/20\n",
            "7400/7400 [==============================] - 1s 158us/step - loss: 0.0292 - acc: 0.9911 - val_loss: 1.0705 - val_acc: 0.8796\n",
            "Epoch 13/20\n",
            "7400/7400 [==============================] - 1s 159us/step - loss: 0.0242 - acc: 0.9928 - val_loss: 1.1155 - val_acc: 0.8808\n",
            "Epoch 14/20\n",
            "7400/7400 [==============================] - 1s 157us/step - loss: 0.0272 - acc: 0.9938 - val_loss: 1.1217 - val_acc: 0.8791\n",
            "Epoch 15/20\n",
            "7400/7400 [==============================] - 1s 163us/step - loss: 0.0335 - acc: 0.9930 - val_loss: 1.1174 - val_acc: 0.8751\n",
            "Epoch 16/20\n",
            "7400/7400 [==============================] - 1s 158us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 1.1263 - val_acc: 0.8831\n",
            "Epoch 17/20\n",
            "7400/7400 [==============================] - 1s 163us/step - loss: 0.0247 - acc: 0.9942 - val_loss: 1.0053 - val_acc: 0.8832\n",
            "Epoch 18/20\n",
            "7400/7400 [==============================] - 1s 162us/step - loss: 0.0285 - acc: 0.9930 - val_loss: 1.1017 - val_acc: 0.8794\n",
            "Epoch 19/20\n",
            "7400/7400 [==============================] - 1s 159us/step - loss: 0.0258 - acc: 0.9943 - val_loss: 1.0585 - val_acc: 0.8789\n",
            "Epoch 20/20\n",
            "7400/7400 [==============================] - 1s 162us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 1.0584 - val_acc: 0.8811\n",
            "Test loss: 1.0583639275697063\n",
            "Test accuracy: 0.8811\n",
            "Train loss: 0.006968594915350312\n",
            "Train accuracy: 0.9993243243243243\n",
            "x label: (7500, 28, 28)\n",
            "y label: (7500,)\n",
            "x unlabel: (52500, 28, 28)\n",
            "y unlabel: (52500,)\n",
            "74\n",
            "Train on 7500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7500/7500 [==============================] - 1s 165us/step - loss: 0.0507 - acc: 0.9892 - val_loss: 1.0104 - val_acc: 0.8834\n",
            "Epoch 2/20\n",
            "7500/7500 [==============================] - 1s 161us/step - loss: 0.0398 - acc: 0.9901 - val_loss: 1.0272 - val_acc: 0.8797\n",
            "Epoch 3/20\n",
            "7500/7500 [==============================] - 1s 163us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 1.0103 - val_acc: 0.8824\n",
            "Epoch 4/20\n",
            "7500/7500 [==============================] - 1s 158us/step - loss: 0.0390 - acc: 0.9901 - val_loss: 0.9932 - val_acc: 0.8811\n",
            "Epoch 5/20\n",
            "7500/7500 [==============================] - 1s 164us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 1.0131 - val_acc: 0.8820\n",
            "Epoch 6/20\n",
            "7500/7500 [==============================] - 1s 160us/step - loss: 0.0359 - acc: 0.9900 - val_loss: 1.0076 - val_acc: 0.8807\n",
            "Epoch 7/20\n",
            "7500/7500 [==============================] - 1s 164us/step - loss: 0.0320 - acc: 0.9924 - val_loss: 0.9804 - val_acc: 0.8893\n",
            "Epoch 8/20\n",
            "7500/7500 [==============================] - 1s 162us/step - loss: 0.0310 - acc: 0.9927 - val_loss: 1.1169 - val_acc: 0.8822\n",
            "Epoch 9/20\n",
            "7500/7500 [==============================] - 1s 165us/step - loss: 0.0313 - acc: 0.9939 - val_loss: 1.0591 - val_acc: 0.8813\n",
            "Epoch 10/20\n",
            "7500/7500 [==============================] - 1s 163us/step - loss: 0.0337 - acc: 0.9905 - val_loss: 1.0371 - val_acc: 0.8845\n",
            "Epoch 11/20\n",
            "7500/7500 [==============================] - 1s 158us/step - loss: 0.0273 - acc: 0.9923 - val_loss: 1.0100 - val_acc: 0.8772\n",
            "Epoch 12/20\n",
            "7500/7500 [==============================] - 1s 157us/step - loss: 0.0251 - acc: 0.9931 - val_loss: 1.0208 - val_acc: 0.8811\n",
            "Epoch 13/20\n",
            "7500/7500 [==============================] - 1s 163us/step - loss: 0.0281 - acc: 0.9925 - val_loss: 1.1015 - val_acc: 0.8737\n",
            "Epoch 14/20\n",
            "7500/7500 [==============================] - 1s 161us/step - loss: 0.0283 - acc: 0.9920 - val_loss: 1.0863 - val_acc: 0.8821\n",
            "Epoch 15/20\n",
            "7500/7500 [==============================] - 1s 162us/step - loss: 0.0289 - acc: 0.9931 - val_loss: 1.0931 - val_acc: 0.8814\n",
            "Epoch 16/20\n",
            "7500/7500 [==============================] - 1s 161us/step - loss: 0.0269 - acc: 0.9924 - val_loss: 1.0279 - val_acc: 0.8833\n",
            "Epoch 17/20\n",
            "7500/7500 [==============================] - 1s 161us/step - loss: 0.0250 - acc: 0.9940 - val_loss: 1.1014 - val_acc: 0.8771\n",
            "Epoch 18/20\n",
            "7500/7500 [==============================] - 1s 162us/step - loss: 0.0261 - acc: 0.9932 - val_loss: 0.9994 - val_acc: 0.8803\n",
            "Epoch 19/20\n",
            "7500/7500 [==============================] - 1s 159us/step - loss: 0.0199 - acc: 0.9955 - val_loss: 1.0394 - val_acc: 0.8835\n",
            "Epoch 20/20\n",
            "7500/7500 [==============================] - 1s 162us/step - loss: 0.0304 - acc: 0.9924 - val_loss: 1.1564 - val_acc: 0.8751\n",
            "Test loss: 1.156385757863703\n",
            "Test accuracy: 0.8751\n",
            "Train loss: 0.0065547812894720964\n",
            "Train accuracy: 0.9996\n",
            "x label: (7600, 28, 28)\n",
            "y label: (7600,)\n",
            "x unlabel: (52400, 28, 28)\n",
            "y unlabel: (52400,)\n",
            "75\n",
            "Train on 7600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7600/7600 [==============================] - 1s 157us/step - loss: 0.0521 - acc: 0.9891 - val_loss: 0.9771 - val_acc: 0.8851\n",
            "Epoch 2/20\n",
            "7600/7600 [==============================] - 1s 157us/step - loss: 0.0409 - acc: 0.9897 - val_loss: 1.0489 - val_acc: 0.8826\n",
            "Epoch 3/20\n",
            "7600/7600 [==============================] - 1s 162us/step - loss: 0.0466 - acc: 0.9882 - val_loss: 0.9641 - val_acc: 0.8789\n",
            "Epoch 4/20\n",
            "7600/7600 [==============================] - 1s 160us/step - loss: 0.0436 - acc: 0.9895 - val_loss: 1.0467 - val_acc: 0.8808\n",
            "Epoch 5/20\n",
            "7600/7600 [==============================] - 1s 159us/step - loss: 0.0347 - acc: 0.9914 - val_loss: 1.0141 - val_acc: 0.8759\n",
            "Epoch 6/20\n",
            "7600/7600 [==============================] - 1s 158us/step - loss: 0.0301 - acc: 0.9921 - val_loss: 1.1408 - val_acc: 0.8807\n",
            "Epoch 7/20\n",
            "7600/7600 [==============================] - 1s 156us/step - loss: 0.0286 - acc: 0.9930 - val_loss: 1.0696 - val_acc: 0.8839\n",
            "Epoch 8/20\n",
            "7600/7600 [==============================] - 1s 161us/step - loss: 0.0309 - acc: 0.9926 - val_loss: 1.1137 - val_acc: 0.8763\n",
            "Epoch 9/20\n",
            "7600/7600 [==============================] - 1s 160us/step - loss: 0.0294 - acc: 0.9938 - val_loss: 1.1360 - val_acc: 0.8805\n",
            "Epoch 10/20\n",
            "7600/7600 [==============================] - 1s 158us/step - loss: 0.0288 - acc: 0.9928 - val_loss: 1.0554 - val_acc: 0.8834\n",
            "Epoch 11/20\n",
            "7600/7600 [==============================] - 1s 159us/step - loss: 0.0341 - acc: 0.9922 - val_loss: 0.9949 - val_acc: 0.8826\n",
            "Epoch 12/20\n",
            "7600/7600 [==============================] - 1s 155us/step - loss: 0.0280 - acc: 0.9924 - val_loss: 1.1561 - val_acc: 0.8755\n",
            "Epoch 13/20\n",
            "7600/7600 [==============================] - 1s 159us/step - loss: 0.0274 - acc: 0.9929 - val_loss: 1.1558 - val_acc: 0.8771\n",
            "Epoch 14/20\n",
            "7600/7600 [==============================] - 1s 157us/step - loss: 0.0283 - acc: 0.9922 - val_loss: 1.0859 - val_acc: 0.8828\n",
            "Epoch 15/20\n",
            "7600/7600 [==============================] - 1s 159us/step - loss: 0.0277 - acc: 0.9936 - val_loss: 1.0834 - val_acc: 0.8786\n",
            "Epoch 16/20\n",
            "7600/7600 [==============================] - 1s 157us/step - loss: 0.0284 - acc: 0.9928 - val_loss: 1.0672 - val_acc: 0.8845\n",
            "Epoch 17/20\n",
            "7600/7600 [==============================] - 1s 158us/step - loss: 0.0243 - acc: 0.9932 - val_loss: 0.9633 - val_acc: 0.8799\n",
            "Epoch 18/20\n",
            "7600/7600 [==============================] - 1s 156us/step - loss: 0.0306 - acc: 0.9936 - val_loss: 0.9915 - val_acc: 0.8838\n",
            "Epoch 19/20\n",
            "7600/7600 [==============================] - 1s 156us/step - loss: 0.0241 - acc: 0.9939 - val_loss: 1.0746 - val_acc: 0.8811\n",
            "Epoch 20/20\n",
            "7600/7600 [==============================] - 1s 158us/step - loss: 0.0281 - acc: 0.9933 - val_loss: 1.0778 - val_acc: 0.8785\n",
            "Test loss: 1.0777756154606377\n",
            "Test accuracy: 0.8785\n",
            "Train loss: 0.006462344533388025\n",
            "Train accuracy: 0.9996052631578948\n",
            "x label: (7700, 28, 28)\n",
            "y label: (7700,)\n",
            "x unlabel: (52300, 28, 28)\n",
            "y unlabel: (52300,)\n",
            "76\n",
            "Train on 7700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7700/7700 [==============================] - 1s 164us/step - loss: 0.0545 - acc: 0.9871 - val_loss: 0.8768 - val_acc: 0.8800\n",
            "Epoch 2/20\n",
            "7700/7700 [==============================] - 1s 161us/step - loss: 0.0402 - acc: 0.9913 - val_loss: 0.9240 - val_acc: 0.8811\n",
            "Epoch 3/20\n",
            "7700/7700 [==============================] - 1s 156us/step - loss: 0.0505 - acc: 0.9875 - val_loss: 0.9673 - val_acc: 0.8812\n",
            "Epoch 4/20\n",
            "7700/7700 [==============================] - 1s 156us/step - loss: 0.0343 - acc: 0.9905 - val_loss: 1.0439 - val_acc: 0.8817\n",
            "Epoch 5/20\n",
            "7700/7700 [==============================] - 1s 156us/step - loss: 0.0373 - acc: 0.9905 - val_loss: 1.0820 - val_acc: 0.8797\n",
            "Epoch 6/20\n",
            "7700/7700 [==============================] - 1s 158us/step - loss: 0.0370 - acc: 0.9906 - val_loss: 0.9893 - val_acc: 0.8814\n",
            "Epoch 7/20\n",
            "7700/7700 [==============================] - 1s 155us/step - loss: 0.0336 - acc: 0.9931 - val_loss: 0.9366 - val_acc: 0.8828\n",
            "Epoch 8/20\n",
            "7700/7700 [==============================] - 1s 160us/step - loss: 0.0312 - acc: 0.9918 - val_loss: 1.1688 - val_acc: 0.8759\n",
            "Epoch 9/20\n",
            "7700/7700 [==============================] - 1s 156us/step - loss: 0.0316 - acc: 0.9914 - val_loss: 1.0204 - val_acc: 0.8770\n",
            "Epoch 10/20\n",
            "7700/7700 [==============================] - 1s 162us/step - loss: 0.0253 - acc: 0.9942 - val_loss: 0.9776 - val_acc: 0.8869\n",
            "Epoch 11/20\n",
            "7700/7700 [==============================] - 1s 159us/step - loss: 0.0398 - acc: 0.9900 - val_loss: 0.9373 - val_acc: 0.8820\n",
            "Epoch 12/20\n",
            "7700/7700 [==============================] - 1s 155us/step - loss: 0.0325 - acc: 0.9913 - val_loss: 1.0076 - val_acc: 0.8800\n",
            "Epoch 13/20\n",
            "7700/7700 [==============================] - 1s 157us/step - loss: 0.0363 - acc: 0.9904 - val_loss: 1.0190 - val_acc: 0.8798\n",
            "Epoch 14/20\n",
            "7700/7700 [==============================] - 1s 155us/step - loss: 0.0248 - acc: 0.9940 - val_loss: 1.0433 - val_acc: 0.8840\n",
            "Epoch 15/20\n",
            "7700/7700 [==============================] - 1s 158us/step - loss: 0.0226 - acc: 0.9943 - val_loss: 1.1323 - val_acc: 0.8818\n",
            "Epoch 16/20\n",
            "7700/7700 [==============================] - 1s 160us/step - loss: 0.0258 - acc: 0.9934 - val_loss: 1.0909 - val_acc: 0.8831\n",
            "Epoch 17/20\n",
            "7700/7700 [==============================] - 1s 157us/step - loss: 0.0271 - acc: 0.9932 - val_loss: 1.0302 - val_acc: 0.8802\n",
            "Epoch 18/20\n",
            "7700/7700 [==============================] - 1s 162us/step - loss: 0.0264 - acc: 0.9930 - val_loss: 1.0826 - val_acc: 0.8834\n",
            "Epoch 19/20\n",
            "7700/7700 [==============================] - 1s 158us/step - loss: 0.0315 - acc: 0.9927 - val_loss: 1.0100 - val_acc: 0.8814\n",
            "Epoch 20/20\n",
            "7700/7700 [==============================] - 1s 157us/step - loss: 0.0256 - acc: 0.9935 - val_loss: 1.0687 - val_acc: 0.8821\n",
            "Test loss: 1.0687274862849971\n",
            "Test accuracy: 0.8821\n",
            "Train loss: 0.006345700183405471\n",
            "Train accuracy: 0.9996103896103896\n",
            "x label: (7800, 28, 28)\n",
            "y label: (7800,)\n",
            "x unlabel: (52200, 28, 28)\n",
            "y unlabel: (52200,)\n",
            "77\n",
            "Train on 7800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7800/7800 [==============================] - 1s 164us/step - loss: 0.0492 - acc: 0.9895 - val_loss: 0.9811 - val_acc: 0.8824\n",
            "Epoch 2/20\n",
            "7800/7800 [==============================] - 1s 161us/step - loss: 0.0437 - acc: 0.9921 - val_loss: 0.9420 - val_acc: 0.8813\n",
            "Epoch 3/20\n",
            "7800/7800 [==============================] - 1s 159us/step - loss: 0.0364 - acc: 0.9906 - val_loss: 0.9958 - val_acc: 0.8865\n",
            "Epoch 4/20\n",
            "7800/7800 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.9881 - val_loss: 0.9987 - val_acc: 0.8795\n",
            "Epoch 5/20\n",
            "7800/7800 [==============================] - 1s 158us/step - loss: 0.0368 - acc: 0.9904 - val_loss: 0.9572 - val_acc: 0.8789\n",
            "Epoch 6/20\n",
            "7800/7800 [==============================] - 1s 156us/step - loss: 0.0363 - acc: 0.9921 - val_loss: 1.0028 - val_acc: 0.8848\n",
            "Epoch 7/20\n",
            "7800/7800 [==============================] - 1s 155us/step - loss: 0.0339 - acc: 0.9917 - val_loss: 0.9964 - val_acc: 0.8825\n",
            "Epoch 8/20\n",
            "7800/7800 [==============================] - 1s 154us/step - loss: 0.0282 - acc: 0.9915 - val_loss: 1.1025 - val_acc: 0.8802\n",
            "Epoch 9/20\n",
            "7800/7800 [==============================] - 1s 158us/step - loss: 0.0355 - acc: 0.9917 - val_loss: 1.0169 - val_acc: 0.8792\n",
            "Epoch 10/20\n",
            "7800/7800 [==============================] - 1s 157us/step - loss: 0.0292 - acc: 0.9932 - val_loss: 1.0733 - val_acc: 0.8820\n",
            "Epoch 11/20\n",
            "7800/7800 [==============================] - 1s 156us/step - loss: 0.0281 - acc: 0.9922 - val_loss: 1.0446 - val_acc: 0.8790\n",
            "Epoch 12/20\n",
            "7800/7800 [==============================] - 1s 161us/step - loss: 0.0299 - acc: 0.9923 - val_loss: 1.0708 - val_acc: 0.8821\n",
            "Epoch 13/20\n",
            "7800/7800 [==============================] - 1s 158us/step - loss: 0.0295 - acc: 0.9922 - val_loss: 1.0738 - val_acc: 0.8777\n",
            "Epoch 14/20\n",
            "7800/7800 [==============================] - 1s 162us/step - loss: 0.0266 - acc: 0.9933 - val_loss: 1.0991 - val_acc: 0.8858\n",
            "Epoch 15/20\n",
            "7800/7800 [==============================] - 1s 155us/step - loss: 0.0295 - acc: 0.9923 - val_loss: 1.0408 - val_acc: 0.8854\n",
            "Epoch 16/20\n",
            "7800/7800 [==============================] - 1s 156us/step - loss: 0.0218 - acc: 0.9950 - val_loss: 1.0536 - val_acc: 0.8819\n",
            "Epoch 17/20\n",
            "7800/7800 [==============================] - 1s 161us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 0.9896 - val_acc: 0.8855\n",
            "Epoch 18/20\n",
            "7800/7800 [==============================] - 1s 159us/step - loss: 0.0264 - acc: 0.9932 - val_loss: 0.9895 - val_acc: 0.8849\n",
            "Epoch 19/20\n",
            "7800/7800 [==============================] - 1s 157us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 1.0991 - val_acc: 0.8826\n",
            "Epoch 20/20\n",
            "7800/7800 [==============================] - 1s 157us/step - loss: 0.0247 - acc: 0.9937 - val_loss: 1.0632 - val_acc: 0.8840\n",
            "Test loss: 1.0631609092954553\n",
            "Test accuracy: 0.884\n",
            "Train loss: 0.006298439322193551\n",
            "Train accuracy: 0.9996153846153846\n",
            "x label: (7900, 28, 28)\n",
            "y label: (7900,)\n",
            "x unlabel: (52100, 28, 28)\n",
            "y unlabel: (52100,)\n",
            "78\n",
            "Train on 7900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "7900/7900 [==============================] - 1s 160us/step - loss: 0.0478 - acc: 0.9895 - val_loss: 1.0320 - val_acc: 0.8848\n",
            "Epoch 2/20\n",
            "7900/7900 [==============================] - 1s 158us/step - loss: 0.0393 - acc: 0.9908 - val_loss: 0.8872 - val_acc: 0.8884\n",
            "Epoch 3/20\n",
            "7900/7900 [==============================] - 1s 160us/step - loss: 0.0409 - acc: 0.9882 - val_loss: 0.9417 - val_acc: 0.8862\n",
            "Epoch 4/20\n",
            "7900/7900 [==============================] - 1s 156us/step - loss: 0.0402 - acc: 0.9903 - val_loss: 0.9555 - val_acc: 0.8810\n",
            "Epoch 5/20\n",
            "7900/7900 [==============================] - 1s 156us/step - loss: 0.0359 - acc: 0.9911 - val_loss: 0.9717 - val_acc: 0.8851\n",
            "Epoch 6/20\n",
            "7900/7900 [==============================] - 1s 159us/step - loss: 0.0397 - acc: 0.9903 - val_loss: 0.9872 - val_acc: 0.8863\n",
            "Epoch 7/20\n",
            "7900/7900 [==============================] - 1s 156us/step - loss: 0.0320 - acc: 0.9922 - val_loss: 0.9190 - val_acc: 0.8882\n",
            "Epoch 8/20\n",
            "7900/7900 [==============================] - 1s 154us/step - loss: 0.0361 - acc: 0.9908 - val_loss: 0.9278 - val_acc: 0.8837\n",
            "Epoch 9/20\n",
            "7900/7900 [==============================] - 1s 155us/step - loss: 0.0293 - acc: 0.9934 - val_loss: 1.1054 - val_acc: 0.8817\n",
            "Epoch 10/20\n",
            "7900/7900 [==============================] - 1s 155us/step - loss: 0.0331 - acc: 0.9906 - val_loss: 1.0052 - val_acc: 0.8807\n",
            "Epoch 11/20\n",
            "7900/7900 [==============================] - 1s 160us/step - loss: 0.0266 - acc: 0.9924 - val_loss: 1.0463 - val_acc: 0.8780\n",
            "Epoch 12/20\n",
            "7900/7900 [==============================] - 1s 156us/step - loss: 0.0300 - acc: 0.9911 - val_loss: 1.0085 - val_acc: 0.8772\n",
            "Epoch 13/20\n",
            "7900/7900 [==============================] - 1s 160us/step - loss: 0.0331 - acc: 0.9927 - val_loss: 0.9061 - val_acc: 0.8872\n",
            "Epoch 14/20\n",
            "7900/7900 [==============================] - 1s 154us/step - loss: 0.0242 - acc: 0.9946 - val_loss: 1.0307 - val_acc: 0.8874\n",
            "Epoch 15/20\n",
            "7900/7900 [==============================] - 1s 154us/step - loss: 0.0408 - acc: 0.9913 - val_loss: 0.9720 - val_acc: 0.8824\n",
            "Epoch 16/20\n",
            "7900/7900 [==============================] - 1s 153us/step - loss: 0.0274 - acc: 0.9927 - val_loss: 0.9230 - val_acc: 0.8863\n",
            "Epoch 17/20\n",
            "7900/7900 [==============================] - 1s 159us/step - loss: 0.0255 - acc: 0.9929 - val_loss: 1.0295 - val_acc: 0.8748\n",
            "Epoch 18/20\n",
            "7900/7900 [==============================] - 1s 161us/step - loss: 0.0258 - acc: 0.9933 - val_loss: 1.0220 - val_acc: 0.8806\n",
            "Epoch 19/20\n",
            "7900/7900 [==============================] - 1s 161us/step - loss: 0.0270 - acc: 0.9928 - val_loss: 0.9569 - val_acc: 0.8861\n",
            "Epoch 20/20\n",
            "7900/7900 [==============================] - 1s 165us/step - loss: 0.0243 - acc: 0.9943 - val_loss: 1.0140 - val_acc: 0.8842\n",
            "Test loss: 1.0139787909628475\n",
            "Test accuracy: 0.8842\n",
            "Train loss: 0.005443919784198051\n",
            "Train accuracy: 0.9996202531645569\n",
            "x label: (8000, 28, 28)\n",
            "y label: (8000,)\n",
            "x unlabel: (52000, 28, 28)\n",
            "y unlabel: (52000,)\n",
            "79\n",
            "Train on 8000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8000/8000 [==============================] - 1s 165us/step - loss: 0.0540 - acc: 0.9878 - val_loss: 0.9357 - val_acc: 0.8878\n",
            "Epoch 2/20\n",
            "8000/8000 [==============================] - 1s 162us/step - loss: 0.0483 - acc: 0.9876 - val_loss: 0.9284 - val_acc: 0.8814\n",
            "Epoch 3/20\n",
            "8000/8000 [==============================] - 1s 163us/step - loss: 0.0406 - acc: 0.9909 - val_loss: 0.9550 - val_acc: 0.8781\n",
            "Epoch 4/20\n",
            "8000/8000 [==============================] - 1s 167us/step - loss: 0.0389 - acc: 0.9914 - val_loss: 0.8302 - val_acc: 0.8918\n",
            "Epoch 5/20\n",
            "8000/8000 [==============================] - 1s 165us/step - loss: 0.0417 - acc: 0.9899 - val_loss: 1.0172 - val_acc: 0.8821\n",
            "Epoch 6/20\n",
            "8000/8000 [==============================] - 1s 163us/step - loss: 0.0341 - acc: 0.9910 - val_loss: 0.9766 - val_acc: 0.8838\n",
            "Epoch 7/20\n",
            "8000/8000 [==============================] - 1s 161us/step - loss: 0.0386 - acc: 0.9903 - val_loss: 0.9357 - val_acc: 0.8871\n",
            "Epoch 8/20\n",
            "8000/8000 [==============================] - 1s 162us/step - loss: 0.0326 - acc: 0.9925 - val_loss: 0.9477 - val_acc: 0.8895\n",
            "Epoch 9/20\n",
            "8000/8000 [==============================] - 1s 160us/step - loss: 0.0302 - acc: 0.9918 - val_loss: 0.9264 - val_acc: 0.8895\n",
            "Epoch 10/20\n",
            "8000/8000 [==============================] - 1s 155us/step - loss: 0.0314 - acc: 0.9925 - val_loss: 0.9754 - val_acc: 0.8889\n",
            "Epoch 11/20\n",
            "8000/8000 [==============================] - 1s 158us/step - loss: 0.0300 - acc: 0.9933 - val_loss: 0.9518 - val_acc: 0.8858\n",
            "Epoch 12/20\n",
            "8000/8000 [==============================] - 1s 161us/step - loss: 0.0265 - acc: 0.9915 - val_loss: 0.9806 - val_acc: 0.8901\n",
            "Epoch 13/20\n",
            "8000/8000 [==============================] - 1s 157us/step - loss: 0.0282 - acc: 0.9924 - val_loss: 0.9685 - val_acc: 0.8848\n",
            "Epoch 14/20\n",
            "8000/8000 [==============================] - 1s 162us/step - loss: 0.0311 - acc: 0.9930 - val_loss: 0.9608 - val_acc: 0.8886\n",
            "Epoch 15/20\n",
            "8000/8000 [==============================] - 1s 158us/step - loss: 0.0327 - acc: 0.9915 - val_loss: 0.8802 - val_acc: 0.8891\n",
            "Epoch 16/20\n",
            "8000/8000 [==============================] - 1s 160us/step - loss: 0.0258 - acc: 0.9919 - val_loss: 1.0509 - val_acc: 0.8831\n",
            "Epoch 17/20\n",
            "8000/8000 [==============================] - 1s 160us/step - loss: 0.0255 - acc: 0.9935 - val_loss: 1.0337 - val_acc: 0.8871\n",
            "Epoch 18/20\n",
            "8000/8000 [==============================] - 1s 159us/step - loss: 0.0294 - acc: 0.9929 - val_loss: 1.0251 - val_acc: 0.8869\n",
            "Epoch 19/20\n",
            "8000/8000 [==============================] - 1s 154us/step - loss: 0.0246 - acc: 0.9934 - val_loss: 1.0063 - val_acc: 0.8846\n",
            "Epoch 20/20\n",
            "8000/8000 [==============================] - 1s 159us/step - loss: 0.0232 - acc: 0.9929 - val_loss: 1.0759 - val_acc: 0.8859\n",
            "Test loss: 1.0758807992373605\n",
            "Test accuracy: 0.8859\n",
            "Train loss: 0.004060075081685227\n",
            "Train accuracy: 0.99975\n",
            "x label: (8100, 28, 28)\n",
            "y label: (8100,)\n",
            "x unlabel: (51900, 28, 28)\n",
            "y unlabel: (51900,)\n",
            "80\n",
            "Train on 8100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8100/8100 [==============================] - 1s 157us/step - loss: 0.0543 - acc: 0.9885 - val_loss: 1.0233 - val_acc: 0.8859\n",
            "Epoch 2/20\n",
            "8100/8100 [==============================] - 1s 157us/step - loss: 0.0471 - acc: 0.9885 - val_loss: 0.8681 - val_acc: 0.8815\n",
            "Epoch 3/20\n",
            "8100/8100 [==============================] - 1s 156us/step - loss: 0.0429 - acc: 0.9901 - val_loss: 0.8856 - val_acc: 0.8863\n",
            "Epoch 4/20\n",
            "8100/8100 [==============================] - 1s 156us/step - loss: 0.0380 - acc: 0.9902 - val_loss: 0.9465 - val_acc: 0.8848\n",
            "Epoch 5/20\n",
            "8100/8100 [==============================] - 1s 158us/step - loss: 0.0420 - acc: 0.9901 - val_loss: 1.0117 - val_acc: 0.8812\n",
            "Epoch 6/20\n",
            "8100/8100 [==============================] - 1s 153us/step - loss: 0.0374 - acc: 0.9912 - val_loss: 0.8731 - val_acc: 0.8904\n",
            "Epoch 7/20\n",
            "8100/8100 [==============================] - 1s 162us/step - loss: 0.0306 - acc: 0.9921 - val_loss: 0.9047 - val_acc: 0.8901\n",
            "Epoch 8/20\n",
            "8100/8100 [==============================] - 1s 156us/step - loss: 0.0295 - acc: 0.9909 - val_loss: 1.0005 - val_acc: 0.8855\n",
            "Epoch 9/20\n",
            "8100/8100 [==============================] - 1s 153us/step - loss: 0.0316 - acc: 0.9917 - val_loss: 0.9898 - val_acc: 0.8859\n",
            "Epoch 10/20\n",
            "8100/8100 [==============================] - 1s 159us/step - loss: 0.0235 - acc: 0.9941 - val_loss: 1.0763 - val_acc: 0.8864\n",
            "Epoch 11/20\n",
            "8100/8100 [==============================] - 1s 157us/step - loss: 0.0300 - acc: 0.9926 - val_loss: 1.0739 - val_acc: 0.8874\n",
            "Epoch 12/20\n",
            "8100/8100 [==============================] - 1s 159us/step - loss: 0.0286 - acc: 0.9930 - val_loss: 1.0661 - val_acc: 0.8883\n",
            "Epoch 13/20\n",
            "8100/8100 [==============================] - 1s 155us/step - loss: 0.0292 - acc: 0.9916 - val_loss: 1.0794 - val_acc: 0.8844\n",
            "Epoch 14/20\n",
            "8100/8100 [==============================] - 1s 155us/step - loss: 0.0321 - acc: 0.9915 - val_loss: 0.9856 - val_acc: 0.8889\n",
            "Epoch 15/20\n",
            "8100/8100 [==============================] - 1s 163us/step - loss: 0.0291 - acc: 0.9921 - val_loss: 0.9277 - val_acc: 0.8898\n",
            "Epoch 16/20\n",
            "8100/8100 [==============================] - 1s 154us/step - loss: 0.0330 - acc: 0.9917 - val_loss: 0.9568 - val_acc: 0.8880\n",
            "Epoch 17/20\n",
            "8100/8100 [==============================] - 1s 157us/step - loss: 0.0349 - acc: 0.9912 - val_loss: 1.0387 - val_acc: 0.8849\n",
            "Epoch 18/20\n",
            "8100/8100 [==============================] - 1s 155us/step - loss: 0.0214 - acc: 0.9944 - val_loss: 1.0163 - val_acc: 0.8873\n",
            "Epoch 19/20\n",
            "8100/8100 [==============================] - 1s 158us/step - loss: 0.0292 - acc: 0.9931 - val_loss: 1.0679 - val_acc: 0.8844\n",
            "Epoch 20/20\n",
            "8100/8100 [==============================] - 1s 156us/step - loss: 0.0304 - acc: 0.9930 - val_loss: 0.9679 - val_acc: 0.8895\n",
            "Test loss: 0.9678788138920449\n",
            "Test accuracy: 0.8895\n",
            "Train loss: 0.006010631851747494\n",
            "Train accuracy: 0.9996296296296296\n",
            "x label: (8200, 28, 28)\n",
            "y label: (8200,)\n",
            "x unlabel: (51800, 28, 28)\n",
            "y unlabel: (51800,)\n",
            "81\n",
            "Train on 8200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8200/8200 [==============================] - 1s 156us/step - loss: 0.0522 - acc: 0.9896 - val_loss: 0.8927 - val_acc: 0.8859\n",
            "Epoch 2/20\n",
            "8200/8200 [==============================] - 1s 157us/step - loss: 0.0446 - acc: 0.9895 - val_loss: 0.9662 - val_acc: 0.8898\n",
            "Epoch 3/20\n",
            "8200/8200 [==============================] - 1s 154us/step - loss: 0.0392 - acc: 0.9904 - val_loss: 0.9509 - val_acc: 0.8889\n",
            "Epoch 4/20\n",
            "8200/8200 [==============================] - 1s 159us/step - loss: 0.0499 - acc: 0.9885 - val_loss: 0.8520 - val_acc: 0.8856\n",
            "Epoch 5/20\n",
            "8200/8200 [==============================] - 1s 157us/step - loss: 0.0375 - acc: 0.9917 - val_loss: 0.9327 - val_acc: 0.8867\n",
            "Epoch 6/20\n",
            "8200/8200 [==============================] - 1s 160us/step - loss: 0.0323 - acc: 0.9932 - val_loss: 1.0043 - val_acc: 0.8893\n",
            "Epoch 7/20\n",
            "8200/8200 [==============================] - 1s 156us/step - loss: 0.0320 - acc: 0.9915 - val_loss: 0.9276 - val_acc: 0.8893\n",
            "Epoch 8/20\n",
            "8200/8200 [==============================] - 1s 158us/step - loss: 0.0379 - acc: 0.9904 - val_loss: 0.9199 - val_acc: 0.8935\n",
            "Epoch 9/20\n",
            "8200/8200 [==============================] - 1s 155us/step - loss: 0.0359 - acc: 0.9929 - val_loss: 0.9398 - val_acc: 0.8894\n",
            "Epoch 10/20\n",
            "8200/8200 [==============================] - 1s 153us/step - loss: 0.0271 - acc: 0.9928 - val_loss: 1.0589 - val_acc: 0.8862\n",
            "Epoch 11/20\n",
            "8200/8200 [==============================] - 1s 158us/step - loss: 0.0313 - acc: 0.9922 - val_loss: 0.8624 - val_acc: 0.8895\n",
            "Epoch 12/20\n",
            "8200/8200 [==============================] - 1s 156us/step - loss: 0.0345 - acc: 0.9912 - val_loss: 0.9676 - val_acc: 0.8836\n",
            "Epoch 13/20\n",
            "8200/8200 [==============================] - 1s 155us/step - loss: 0.0295 - acc: 0.9916 - val_loss: 0.9306 - val_acc: 0.8873\n",
            "Epoch 14/20\n",
            "8200/8200 [==============================] - 1s 154us/step - loss: 0.0300 - acc: 0.9930 - val_loss: 0.8994 - val_acc: 0.8885\n",
            "Epoch 15/20\n",
            "8200/8200 [==============================] - 1s 159us/step - loss: 0.0250 - acc: 0.9933 - val_loss: 1.1174 - val_acc: 0.8865\n",
            "Epoch 16/20\n",
            "8200/8200 [==============================] - 1s 159us/step - loss: 0.0176 - acc: 0.9955 - val_loss: 1.0731 - val_acc: 0.8880\n",
            "Epoch 17/20\n",
            "8200/8200 [==============================] - 1s 154us/step - loss: 0.0245 - acc: 0.9934 - val_loss: 0.9601 - val_acc: 0.8888\n",
            "Epoch 18/20\n",
            "8200/8200 [==============================] - 1s 154us/step - loss: 0.0202 - acc: 0.9948 - val_loss: 0.9973 - val_acc: 0.8891\n",
            "Epoch 19/20\n",
            "8200/8200 [==============================] - 1s 150us/step - loss: 0.0256 - acc: 0.9929 - val_loss: 0.9979 - val_acc: 0.8879\n",
            "Epoch 20/20\n",
            "8200/8200 [==============================] - 1s 151us/step - loss: 0.0236 - acc: 0.9939 - val_loss: 0.9878 - val_acc: 0.8891\n",
            "Test loss: 0.9878421858337264\n",
            "Test accuracy: 0.8891\n",
            "Train loss: 0.004038910088143095\n",
            "Train accuracy: 0.9997560975609756\n",
            "x label: (8300, 28, 28)\n",
            "y label: (8300,)\n",
            "x unlabel: (51700, 28, 28)\n",
            "y unlabel: (51700,)\n",
            "82\n",
            "Train on 8300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8300/8300 [==============================] - 1s 155us/step - loss: 0.0402 - acc: 0.9894 - val_loss: 0.9894 - val_acc: 0.8870\n",
            "Epoch 2/20\n",
            "8300/8300 [==============================] - 1s 157us/step - loss: 0.0444 - acc: 0.9886 - val_loss: 0.9614 - val_acc: 0.8876\n",
            "Epoch 3/20\n",
            "8300/8300 [==============================] - 1s 153us/step - loss: 0.0433 - acc: 0.9882 - val_loss: 0.9205 - val_acc: 0.8886\n",
            "Epoch 4/20\n",
            "8300/8300 [==============================] - 1s 156us/step - loss: 0.0444 - acc: 0.9889 - val_loss: 0.8331 - val_acc: 0.8904\n",
            "Epoch 5/20\n",
            "8300/8300 [==============================] - 1s 154us/step - loss: 0.0342 - acc: 0.9925 - val_loss: 0.8912 - val_acc: 0.8900\n",
            "Epoch 6/20\n",
            "8300/8300 [==============================] - 1s 161us/step - loss: 0.0329 - acc: 0.9920 - val_loss: 0.8940 - val_acc: 0.8914\n",
            "Epoch 7/20\n",
            "8300/8300 [==============================] - 1s 155us/step - loss: 0.0370 - acc: 0.9910 - val_loss: 0.9072 - val_acc: 0.8919\n",
            "Epoch 8/20\n",
            "8300/8300 [==============================] - 1s 154us/step - loss: 0.0298 - acc: 0.9922 - val_loss: 0.9706 - val_acc: 0.8841\n",
            "Epoch 9/20\n",
            "8300/8300 [==============================] - 1s 152us/step - loss: 0.0332 - acc: 0.9914 - val_loss: 0.9433 - val_acc: 0.8916\n",
            "Epoch 10/20\n",
            "8300/8300 [==============================] - 1s 156us/step - loss: 0.0336 - acc: 0.9916 - val_loss: 0.8878 - val_acc: 0.8918\n",
            "Epoch 11/20\n",
            "8300/8300 [==============================] - 1s 157us/step - loss: 0.0283 - acc: 0.9930 - val_loss: 0.9379 - val_acc: 0.8918\n",
            "Epoch 12/20\n",
            "8300/8300 [==============================] - 1s 153us/step - loss: 0.0270 - acc: 0.9920 - val_loss: 1.0380 - val_acc: 0.8847\n",
            "Epoch 13/20\n",
            "8300/8300 [==============================] - 1s 160us/step - loss: 0.0276 - acc: 0.9935 - val_loss: 1.0365 - val_acc: 0.8854\n",
            "Epoch 14/20\n",
            "8300/8300 [==============================] - 1s 152us/step - loss: 0.0255 - acc: 0.9933 - val_loss: 0.9304 - val_acc: 0.8912\n",
            "Epoch 15/20\n",
            "8300/8300 [==============================] - 1s 161us/step - loss: 0.0307 - acc: 0.9930 - val_loss: 0.9923 - val_acc: 0.8886\n",
            "Epoch 16/20\n",
            "8300/8300 [==============================] - 1s 160us/step - loss: 0.0218 - acc: 0.9933 - val_loss: 1.0304 - val_acc: 0.8888\n",
            "Epoch 17/20\n",
            "8300/8300 [==============================] - 1s 157us/step - loss: 0.0311 - acc: 0.9920 - val_loss: 0.9639 - val_acc: 0.8899\n",
            "Epoch 18/20\n",
            "8300/8300 [==============================] - 1s 158us/step - loss: 0.0251 - acc: 0.9934 - val_loss: 1.0617 - val_acc: 0.8885\n",
            "Epoch 19/20\n",
            "8300/8300 [==============================] - 1s 153us/step - loss: 0.0300 - acc: 0.9936 - val_loss: 1.0889 - val_acc: 0.8847\n",
            "Epoch 20/20\n",
            "8300/8300 [==============================] - 1s 154us/step - loss: 0.0278 - acc: 0.9928 - val_loss: 1.0789 - val_acc: 0.8852\n",
            "Test loss: 1.0788916129438935\n",
            "Test accuracy: 0.8852\n",
            "Train loss: 0.005920736958621738\n",
            "Train accuracy: 0.9996385542168674\n",
            "x label: (8400, 28, 28)\n",
            "y label: (8400,)\n",
            "x unlabel: (51600, 28, 28)\n",
            "y unlabel: (51600,)\n",
            "83\n",
            "Train on 8400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0477 - acc: 0.9889 - val_loss: 0.8954 - val_acc: 0.8923\n",
            "Epoch 2/20\n",
            "8400/8400 [==============================] - 1s 155us/step - loss: 0.0390 - acc: 0.9890 - val_loss: 0.8921 - val_acc: 0.8916\n",
            "Epoch 3/20\n",
            "8400/8400 [==============================] - 1s 151us/step - loss: 0.0402 - acc: 0.9920 - val_loss: 1.0035 - val_acc: 0.8881\n",
            "Epoch 4/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0382 - acc: 0.9905 - val_loss: 0.9161 - val_acc: 0.8953\n",
            "Epoch 5/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0385 - acc: 0.9905 - val_loss: 0.8493 - val_acc: 0.8903\n",
            "Epoch 6/20\n",
            "8400/8400 [==============================] - 1s 158us/step - loss: 0.0363 - acc: 0.9913 - val_loss: 0.9202 - val_acc: 0.8905\n",
            "Epoch 7/20\n",
            "8400/8400 [==============================] - 1s 153us/step - loss: 0.0358 - acc: 0.9925 - val_loss: 0.9091 - val_acc: 0.8897\n",
            "Epoch 8/20\n",
            "8400/8400 [==============================] - 1s 151us/step - loss: 0.0369 - acc: 0.9917 - val_loss: 0.8746 - val_acc: 0.8850\n",
            "Epoch 9/20\n",
            "8400/8400 [==============================] - 1s 153us/step - loss: 0.0267 - acc: 0.9926 - val_loss: 0.9499 - val_acc: 0.8902\n",
            "Epoch 10/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0358 - acc: 0.9907 - val_loss: 0.8796 - val_acc: 0.8897\n",
            "Epoch 11/20\n",
            "8400/8400 [==============================] - 1s 156us/step - loss: 0.0379 - acc: 0.9906 - val_loss: 0.9199 - val_acc: 0.8888\n",
            "Epoch 12/20\n",
            "8400/8400 [==============================] - 1s 158us/step - loss: 0.0297 - acc: 0.9931 - val_loss: 0.9441 - val_acc: 0.8901\n",
            "Epoch 13/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0315 - acc: 0.9923 - val_loss: 0.9780 - val_acc: 0.8945\n",
            "Epoch 14/20\n",
            "8400/8400 [==============================] - 1s 154us/step - loss: 0.0283 - acc: 0.9932 - val_loss: 0.9962 - val_acc: 0.8904\n",
            "Epoch 15/20\n",
            "8400/8400 [==============================] - 1s 152us/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.9184 - val_acc: 0.8900\n",
            "Epoch 16/20\n",
            "8400/8400 [==============================] - 1s 155us/step - loss: 0.0316 - acc: 0.9925 - val_loss: 0.9185 - val_acc: 0.8894\n",
            "Epoch 17/20\n",
            "8400/8400 [==============================] - 1s 153us/step - loss: 0.0303 - acc: 0.9921 - val_loss: 0.9487 - val_acc: 0.8909\n",
            "Epoch 18/20\n",
            "8400/8400 [==============================] - 1s 152us/step - loss: 0.0331 - acc: 0.9926 - val_loss: 0.9100 - val_acc: 0.8883\n",
            "Epoch 19/20\n",
            "8400/8400 [==============================] - 1s 153us/step - loss: 0.0275 - acc: 0.9926 - val_loss: 1.0461 - val_acc: 0.8842\n",
            "Epoch 20/20\n",
            "8400/8400 [==============================] - 1s 157us/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.9832 - val_acc: 0.8903\n",
            "Test loss: 0.9832428606905218\n",
            "Test accuracy: 0.8903\n",
            "Train loss: 0.005915577603458143\n",
            "Train accuracy: 0.9996428571428572\n",
            "x label: (8500, 28, 28)\n",
            "y label: (8500,)\n",
            "x unlabel: (51500, 28, 28)\n",
            "y unlabel: (51500,)\n",
            "84\n",
            "Train on 8500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8500/8500 [==============================] - 1s 153us/step - loss: 0.0524 - acc: 0.9900 - val_loss: 0.8285 - val_acc: 0.8911\n",
            "Epoch 2/20\n",
            "8500/8500 [==============================] - 1s 155us/step - loss: 0.0490 - acc: 0.9881 - val_loss: 0.9562 - val_acc: 0.8880\n",
            "Epoch 3/20\n",
            "8500/8500 [==============================] - 1s 149us/step - loss: 0.0350 - acc: 0.9915 - val_loss: 1.0580 - val_acc: 0.8874\n",
            "Epoch 4/20\n",
            "8500/8500 [==============================] - 1s 152us/step - loss: 0.0419 - acc: 0.9906 - val_loss: 0.8173 - val_acc: 0.8911\n",
            "Epoch 5/20\n",
            "8500/8500 [==============================] - 1s 155us/step - loss: 0.0353 - acc: 0.9912 - val_loss: 0.9417 - val_acc: 0.8882\n",
            "Epoch 6/20\n",
            "8500/8500 [==============================] - 1s 161us/step - loss: 0.0392 - acc: 0.9905 - val_loss: 0.9750 - val_acc: 0.8896\n",
            "Epoch 7/20\n",
            "8500/8500 [==============================] - 1s 155us/step - loss: 0.0328 - acc: 0.9916 - val_loss: 0.9377 - val_acc: 0.8836\n",
            "Epoch 8/20\n",
            "8500/8500 [==============================] - 1s 155us/step - loss: 0.0327 - acc: 0.9921 - val_loss: 0.9899 - val_acc: 0.8918\n",
            "Epoch 9/20\n",
            "8500/8500 [==============================] - 1s 157us/step - loss: 0.0339 - acc: 0.9909 - val_loss: 0.8503 - val_acc: 0.8950\n",
            "Epoch 10/20\n",
            "8500/8500 [==============================] - 1s 153us/step - loss: 0.0352 - acc: 0.9913 - val_loss: 0.9122 - val_acc: 0.8920\n",
            "Epoch 11/20\n",
            "8500/8500 [==============================] - 1s 154us/step - loss: 0.0287 - acc: 0.9921 - val_loss: 0.9362 - val_acc: 0.8890\n",
            "Epoch 12/20\n",
            "8500/8500 [==============================] - 1s 152us/step - loss: 0.0262 - acc: 0.9929 - val_loss: 0.8979 - val_acc: 0.8917\n",
            "Epoch 13/20\n",
            "8500/8500 [==============================] - 1s 156us/step - loss: 0.0321 - acc: 0.9911 - val_loss: 0.9168 - val_acc: 0.8902\n",
            "Epoch 14/20\n",
            "8500/8500 [==============================] - 1s 154us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 0.9177 - val_acc: 0.8879\n",
            "Epoch 15/20\n",
            "8500/8500 [==============================] - 1s 155us/step - loss: 0.0212 - acc: 0.9947 - val_loss: 1.0433 - val_acc: 0.8865\n",
            "Epoch 16/20\n",
            "8500/8500 [==============================] - 1s 154us/step - loss: 0.0282 - acc: 0.9935 - val_loss: 0.9270 - val_acc: 0.8874\n",
            "Epoch 17/20\n",
            "8500/8500 [==============================] - 1s 157us/step - loss: 0.0276 - acc: 0.9933 - val_loss: 0.9648 - val_acc: 0.8904\n",
            "Epoch 18/20\n",
            "8500/8500 [==============================] - 1s 153us/step - loss: 0.0251 - acc: 0.9931 - val_loss: 0.9177 - val_acc: 0.8823\n",
            "Epoch 19/20\n",
            "8500/8500 [==============================] - 1s 152us/step - loss: 0.0285 - acc: 0.9925 - val_loss: 0.9974 - val_acc: 0.8880\n",
            "Epoch 20/20\n",
            "8500/8500 [==============================] - 1s 156us/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.9442 - val_acc: 0.8908\n",
            "Test loss: 0.9442166992841825\n",
            "Test accuracy: 0.8908\n",
            "Train loss: 0.0039295024798363075\n",
            "Train accuracy: 0.9997647058823529\n",
            "x label: (8600, 28, 28)\n",
            "y label: (8600,)\n",
            "x unlabel: (51400, 28, 28)\n",
            "y unlabel: (51400,)\n",
            "85\n",
            "Train on 8600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8600/8600 [==============================] - 1s 155us/step - loss: 0.0360 - acc: 0.9905 - val_loss: 0.9491 - val_acc: 0.8928\n",
            "Epoch 2/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0420 - acc: 0.9886 - val_loss: 0.9254 - val_acc: 0.8913\n",
            "Epoch 3/20\n",
            "8600/8600 [==============================] - 1s 154us/step - loss: 0.0391 - acc: 0.9895 - val_loss: 0.9347 - val_acc: 0.8884\n",
            "Epoch 4/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0413 - acc: 0.9898 - val_loss: 1.0026 - val_acc: 0.8851\n",
            "Epoch 5/20\n",
            "8600/8600 [==============================] - 1s 153us/step - loss: 0.0387 - acc: 0.9903 - val_loss: 0.7492 - val_acc: 0.8924\n",
            "Epoch 6/20\n",
            "8600/8600 [==============================] - 1s 158us/step - loss: 0.0338 - acc: 0.9907 - val_loss: 0.9017 - val_acc: 0.8930\n",
            "Epoch 7/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0341 - acc: 0.9912 - val_loss: 0.8519 - val_acc: 0.8935\n",
            "Epoch 8/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0275 - acc: 0.9922 - val_loss: 0.9292 - val_acc: 0.8891\n",
            "Epoch 9/20\n",
            "8600/8600 [==============================] - 1s 157us/step - loss: 0.0292 - acc: 0.9917 - val_loss: 0.9903 - val_acc: 0.8938\n",
            "Epoch 10/20\n",
            "8600/8600 [==============================] - 1s 149us/step - loss: 0.0265 - acc: 0.9921 - val_loss: 0.9401 - val_acc: 0.8905\n",
            "Epoch 11/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0299 - acc: 0.9909 - val_loss: 0.9419 - val_acc: 0.8888\n",
            "Epoch 12/20\n",
            "8600/8600 [==============================] - 1s 155us/step - loss: 0.0333 - acc: 0.9906 - val_loss: 0.9676 - val_acc: 0.8893\n",
            "Epoch 13/20\n",
            "8600/8600 [==============================] - 1s 157us/step - loss: 0.0272 - acc: 0.9920 - val_loss: 0.9204 - val_acc: 0.8907\n",
            "Epoch 14/20\n",
            "8600/8600 [==============================] - 1s 157us/step - loss: 0.0231 - acc: 0.9928 - val_loss: 0.9105 - val_acc: 0.8903\n",
            "Epoch 15/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0277 - acc: 0.9926 - val_loss: 0.9167 - val_acc: 0.8899\n",
            "Epoch 16/20\n",
            "8600/8600 [==============================] - 1s 155us/step - loss: 0.0314 - acc: 0.9921 - val_loss: 0.9030 - val_acc: 0.8889\n",
            "Epoch 17/20\n",
            "8600/8600 [==============================] - 1s 156us/step - loss: 0.0259 - acc: 0.9922 - val_loss: 0.9481 - val_acc: 0.8851\n",
            "Epoch 18/20\n",
            "8600/8600 [==============================] - 1s 155us/step - loss: 0.0269 - acc: 0.9926 - val_loss: 0.9559 - val_acc: 0.8870\n",
            "Epoch 19/20\n",
            "8600/8600 [==============================] - 1s 152us/step - loss: 0.0278 - acc: 0.9936 - val_loss: 1.0152 - val_acc: 0.8918\n",
            "Epoch 20/20\n",
            "8600/8600 [==============================] - 1s 155us/step - loss: 0.0305 - acc: 0.9924 - val_loss: 0.8904 - val_acc: 0.8962\n",
            "Test loss: 0.8903712870386197\n",
            "Test accuracy: 0.8962\n",
            "Train loss: 0.0038552884638788465\n",
            "Train accuracy: 0.9997674418604651\n",
            "x label: (8700, 28, 28)\n",
            "y label: (8700,)\n",
            "x unlabel: (51300, 28, 28)\n",
            "y unlabel: (51300,)\n",
            "86\n",
            "Train on 8700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0496 - acc: 0.9887 - val_loss: 0.8707 - val_acc: 0.8876\n",
            "Epoch 2/20\n",
            "8700/8700 [==============================] - 1s 153us/step - loss: 0.0472 - acc: 0.9883 - val_loss: 0.9193 - val_acc: 0.8905\n",
            "Epoch 3/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0426 - acc: 0.9892 - val_loss: 0.9005 - val_acc: 0.8916\n",
            "Epoch 4/20\n",
            "8700/8700 [==============================] - 1s 152us/step - loss: 0.0414 - acc: 0.9897 - val_loss: 0.8745 - val_acc: 0.8872\n",
            "Epoch 5/20\n",
            "8700/8700 [==============================] - 1s 152us/step - loss: 0.0465 - acc: 0.9880 - val_loss: 0.8881 - val_acc: 0.8886\n",
            "Epoch 6/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0352 - acc: 0.9909 - val_loss: 0.8774 - val_acc: 0.8911\n",
            "Epoch 7/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0336 - acc: 0.9914 - val_loss: 0.9749 - val_acc: 0.8908\n",
            "Epoch 8/20\n",
            "8700/8700 [==============================] - 1s 156us/step - loss: 0.0319 - acc: 0.9900 - val_loss: 0.9970 - val_acc: 0.8850\n",
            "Epoch 9/20\n",
            "8700/8700 [==============================] - 1s 153us/step - loss: 0.0369 - acc: 0.9910 - val_loss: 0.9269 - val_acc: 0.8900\n",
            "Epoch 10/20\n",
            "8700/8700 [==============================] - 1s 152us/step - loss: 0.0329 - acc: 0.9907 - val_loss: 1.0167 - val_acc: 0.8894\n",
            "Epoch 11/20\n",
            "8700/8700 [==============================] - 1s 152us/step - loss: 0.0288 - acc: 0.9920 - val_loss: 0.9668 - val_acc: 0.8902\n",
            "Epoch 12/20\n",
            "8700/8700 [==============================] - 1s 150us/step - loss: 0.0314 - acc: 0.9914 - val_loss: 0.8726 - val_acc: 0.8946\n",
            "Epoch 13/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0355 - acc: 0.9908 - val_loss: 0.9725 - val_acc: 0.8949\n",
            "Epoch 14/20\n",
            "8700/8700 [==============================] - 1s 156us/step - loss: 0.0234 - acc: 0.9930 - val_loss: 0.9682 - val_acc: 0.8890\n",
            "Epoch 15/20\n",
            "8700/8700 [==============================] - 1s 154us/step - loss: 0.0286 - acc: 0.9922 - val_loss: 0.8634 - val_acc: 0.8905\n",
            "Epoch 16/20\n",
            "8700/8700 [==============================] - 1s 151us/step - loss: 0.0277 - acc: 0.9934 - val_loss: 1.0648 - val_acc: 0.8862\n",
            "Epoch 17/20\n",
            "8700/8700 [==============================] - 1s 150us/step - loss: 0.0300 - acc: 0.9925 - val_loss: 0.9802 - val_acc: 0.8883\n",
            "Epoch 18/20\n",
            "8700/8700 [==============================] - 1s 152us/step - loss: 0.0230 - acc: 0.9941 - val_loss: 0.9871 - val_acc: 0.8914\n",
            "Epoch 19/20\n",
            "8700/8700 [==============================] - 1s 150us/step - loss: 0.0275 - acc: 0.9922 - val_loss: 0.9628 - val_acc: 0.8898\n",
            "Epoch 20/20\n",
            "8700/8700 [==============================] - 1s 151us/step - loss: 0.0261 - acc: 0.9931 - val_loss: 0.9859 - val_acc: 0.8915\n",
            "Test loss: 0.9859414678223042\n",
            "Test accuracy: 0.8915\n",
            "Train loss: 0.003755533382038485\n",
            "Train accuracy: 0.9997701149425288\n",
            "x label: (8800, 28, 28)\n",
            "y label: (8800,)\n",
            "x unlabel: (51200, 28, 28)\n",
            "y unlabel: (51200,)\n",
            "87\n",
            "Train on 8800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8800/8800 [==============================] - 1s 156us/step - loss: 0.0531 - acc: 0.9882 - val_loss: 0.8665 - val_acc: 0.8931\n",
            "Epoch 2/20\n",
            "8800/8800 [==============================] - 1s 149us/step - loss: 0.0499 - acc: 0.9889 - val_loss: 0.8795 - val_acc: 0.8854\n",
            "Epoch 3/20\n",
            "8800/8800 [==============================] - 1s 152us/step - loss: 0.0414 - acc: 0.9906 - val_loss: 0.8733 - val_acc: 0.8927\n",
            "Epoch 4/20\n",
            "8800/8800 [==============================] - 1s 154us/step - loss: 0.0416 - acc: 0.9901 - val_loss: 0.9269 - val_acc: 0.8917\n",
            "Epoch 5/20\n",
            "8800/8800 [==============================] - 1s 152us/step - loss: 0.0345 - acc: 0.9913 - val_loss: 0.8191 - val_acc: 0.8936\n",
            "Epoch 6/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0271 - acc: 0.9926 - val_loss: 0.8559 - val_acc: 0.8916\n",
            "Epoch 7/20\n",
            "8800/8800 [==============================] - 1s 149us/step - loss: 0.0374 - acc: 0.9909 - val_loss: 0.8374 - val_acc: 0.8909\n",
            "Epoch 8/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0355 - acc: 0.9902 - val_loss: 0.8937 - val_acc: 0.8933\n",
            "Epoch 9/20\n",
            "8800/8800 [==============================] - 1s 151us/step - loss: 0.0324 - acc: 0.9924 - val_loss: 0.9191 - val_acc: 0.8919\n",
            "Epoch 10/20\n",
            "8800/8800 [==============================] - 1s 147us/step - loss: 0.0315 - acc: 0.9924 - val_loss: 0.9124 - val_acc: 0.8902\n",
            "Epoch 11/20\n",
            "8800/8800 [==============================] - 1s 151us/step - loss: 0.0306 - acc: 0.9922 - val_loss: 0.9096 - val_acc: 0.8936\n",
            "Epoch 12/20\n",
            "8800/8800 [==============================] - 1s 151us/step - loss: 0.0405 - acc: 0.9906 - val_loss: 0.9335 - val_acc: 0.8941\n",
            "Epoch 13/20\n",
            "8800/8800 [==============================] - 1s 153us/step - loss: 0.0364 - acc: 0.9899 - val_loss: 1.0161 - val_acc: 0.8899\n",
            "Epoch 14/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0288 - acc: 0.9928 - val_loss: 0.8970 - val_acc: 0.8934\n",
            "Epoch 15/20\n",
            "8800/8800 [==============================] - 1s 149us/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.9042 - val_acc: 0.8852\n",
            "Epoch 16/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0327 - acc: 0.9928 - val_loss: 0.9332 - val_acc: 0.8957\n",
            "Epoch 17/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0294 - acc: 0.9917 - val_loss: 0.9501 - val_acc: 0.8921\n",
            "Epoch 18/20\n",
            "8800/8800 [==============================] - 1s 150us/step - loss: 0.0238 - acc: 0.9943 - val_loss: 1.0244 - val_acc: 0.8920\n",
            "Epoch 19/20\n",
            "8800/8800 [==============================] - 1s 155us/step - loss: 0.0278 - acc: 0.9925 - val_loss: 0.8532 - val_acc: 0.8941\n",
            "Epoch 20/20\n",
            "8800/8800 [==============================] - 1s 153us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 1.0014 - val_acc: 0.8915\n",
            "Test loss: 1.0013828422530626\n",
            "Test accuracy: 0.8915\n",
            "Train loss: 0.005547062713374192\n",
            "Train accuracy: 0.9996590909090909\n",
            "x label: (8900, 28, 28)\n",
            "y label: (8900,)\n",
            "x unlabel: (51100, 28, 28)\n",
            "y unlabel: (51100,)\n",
            "88\n",
            "Train on 8900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "8900/8900 [==============================] - 1s 153us/step - loss: 0.0572 - acc: 0.9867 - val_loss: 0.9468 - val_acc: 0.8923\n",
            "Epoch 2/20\n",
            "8900/8900 [==============================] - 1s 149us/step - loss: 0.0458 - acc: 0.9883 - val_loss: 0.9704 - val_acc: 0.8905\n",
            "Epoch 3/20\n",
            "8900/8900 [==============================] - 1s 150us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 0.7714 - val_acc: 0.8896\n",
            "Epoch 4/20\n",
            "8900/8900 [==============================] - 1s 147us/step - loss: 0.0432 - acc: 0.9892 - val_loss: 1.0286 - val_acc: 0.8889\n",
            "Epoch 5/20\n",
            "8900/8900 [==============================] - 1s 152us/step - loss: 0.0362 - acc: 0.9900 - val_loss: 0.8354 - val_acc: 0.8941\n",
            "Epoch 6/20\n",
            "8900/8900 [==============================] - 1s 150us/step - loss: 0.0395 - acc: 0.9903 - val_loss: 0.8883 - val_acc: 0.8878\n",
            "Epoch 7/20\n",
            "8900/8900 [==============================] - 1s 146us/step - loss: 0.0331 - acc: 0.9904 - val_loss: 0.9273 - val_acc: 0.8872\n",
            "Epoch 8/20\n",
            "8900/8900 [==============================] - 1s 146us/step - loss: 0.0359 - acc: 0.9906 - val_loss: 0.9394 - val_acc: 0.8924\n",
            "Epoch 9/20\n",
            "8900/8900 [==============================] - 1s 146us/step - loss: 0.0302 - acc: 0.9928 - val_loss: 0.9618 - val_acc: 0.8916\n",
            "Epoch 10/20\n",
            "8900/8900 [==============================] - 1s 151us/step - loss: 0.0273 - acc: 0.9926 - val_loss: 0.9241 - val_acc: 0.8914\n",
            "Epoch 11/20\n",
            "8900/8900 [==============================] - 1s 151us/step - loss: 0.0300 - acc: 0.9913 - val_loss: 0.9647 - val_acc: 0.8905\n",
            "Epoch 12/20\n",
            "8900/8900 [==============================] - 1s 152us/step - loss: 0.0289 - acc: 0.9926 - val_loss: 0.9010 - val_acc: 0.8920\n",
            "Epoch 13/20\n",
            "8900/8900 [==============================] - 1s 149us/step - loss: 0.0341 - acc: 0.9911 - val_loss: 0.9368 - val_acc: 0.8900\n",
            "Epoch 14/20\n",
            "8900/8900 [==============================] - 1s 151us/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.8397 - val_acc: 0.8946\n",
            "Epoch 15/20\n",
            "8900/8900 [==============================] - 1s 149us/step - loss: 0.0247 - acc: 0.9937 - val_loss: 0.9115 - val_acc: 0.8936\n",
            "Epoch 16/20\n",
            "8900/8900 [==============================] - 1s 147us/step - loss: 0.0323 - acc: 0.9913 - val_loss: 0.8103 - val_acc: 0.8947\n",
            "Epoch 17/20\n",
            "8900/8900 [==============================] - 1s 150us/step - loss: 0.0283 - acc: 0.9911 - val_loss: 0.9882 - val_acc: 0.8916\n",
            "Epoch 18/20\n",
            "8900/8900 [==============================] - 1s 145us/step - loss: 0.0276 - acc: 0.9922 - val_loss: 1.0857 - val_acc: 0.8859\n",
            "Epoch 19/20\n",
            "8900/8900 [==============================] - 1s 145us/step - loss: 0.0255 - acc: 0.9930 - val_loss: 0.9664 - val_acc: 0.8909\n",
            "Epoch 20/20\n",
            "8900/8900 [==============================] - 1s 151us/step - loss: 0.0287 - acc: 0.9924 - val_loss: 0.9328 - val_acc: 0.8898\n",
            "Test loss: 0.9328210311895586\n",
            "Test accuracy: 0.8898\n",
            "Train loss: 0.00435214804252814\n",
            "Train accuracy: 0.9996629213483146\n",
            "x label: (9000, 28, 28)\n",
            "y label: (9000,)\n",
            "x unlabel: (51000, 28, 28)\n",
            "y unlabel: (51000,)\n",
            "89\n",
            "Train on 9000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9000/9000 [==============================] - 1s 153us/step - loss: 0.0482 - acc: 0.9891 - val_loss: 0.9188 - val_acc: 0.8945\n",
            "Epoch 2/20\n",
            "9000/9000 [==============================] - 1s 149us/step - loss: 0.0440 - acc: 0.9898 - val_loss: 0.9484 - val_acc: 0.8922\n",
            "Epoch 3/20\n",
            "9000/9000 [==============================] - 1s 145us/step - loss: 0.0460 - acc: 0.9897 - val_loss: 0.9297 - val_acc: 0.8936\n",
            "Epoch 4/20\n",
            "9000/9000 [==============================] - 1s 145us/step - loss: 0.0500 - acc: 0.9881 - val_loss: 0.8509 - val_acc: 0.8935\n",
            "Epoch 5/20\n",
            "9000/9000 [==============================] - 1s 147us/step - loss: 0.0379 - acc: 0.9904 - val_loss: 0.9952 - val_acc: 0.8913\n",
            "Epoch 6/20\n",
            "9000/9000 [==============================] - 1s 144us/step - loss: 0.0382 - acc: 0.9897 - val_loss: 0.9378 - val_acc: 0.8956\n",
            "Epoch 7/20\n",
            "9000/9000 [==============================] - 1s 148us/step - loss: 0.0296 - acc: 0.9917 - val_loss: 0.9442 - val_acc: 0.8938\n",
            "Epoch 8/20\n",
            "9000/9000 [==============================] - 1s 149us/step - loss: 0.0360 - acc: 0.9913 - val_loss: 0.9986 - val_acc: 0.8881\n",
            "Epoch 9/20\n",
            "9000/9000 [==============================] - 1s 148us/step - loss: 0.0343 - acc: 0.9923 - val_loss: 0.8614 - val_acc: 0.8939\n",
            "Epoch 10/20\n",
            "9000/9000 [==============================] - 1s 158us/step - loss: 0.0316 - acc: 0.9919 - val_loss: 0.9027 - val_acc: 0.8928\n",
            "Epoch 11/20\n",
            "9000/9000 [==============================] - 1s 156us/step - loss: 0.0318 - acc: 0.9912 - val_loss: 0.9125 - val_acc: 0.8905\n",
            "Epoch 12/20\n",
            "9000/9000 [==============================] - 1s 158us/step - loss: 0.0376 - acc: 0.9912 - val_loss: 0.8943 - val_acc: 0.8917\n",
            "Epoch 13/20\n",
            "9000/9000 [==============================] - 1s 154us/step - loss: 0.0341 - acc: 0.9914 - val_loss: 0.9753 - val_acc: 0.8903\n",
            "Epoch 14/20\n",
            "9000/9000 [==============================] - 1s 156us/step - loss: 0.0308 - acc: 0.9918 - val_loss: 0.9213 - val_acc: 0.8891\n",
            "Epoch 15/20\n",
            "9000/9000 [==============================] - 1s 154us/step - loss: 0.0282 - acc: 0.9929 - val_loss: 0.9184 - val_acc: 0.8913\n",
            "Epoch 16/20\n",
            "9000/9000 [==============================] - 1s 153us/step - loss: 0.0274 - acc: 0.9924 - val_loss: 0.9937 - val_acc: 0.8900\n",
            "Epoch 17/20\n",
            "9000/9000 [==============================] - 1s 153us/step - loss: 0.0295 - acc: 0.9923 - val_loss: 0.8704 - val_acc: 0.8973\n",
            "Epoch 18/20\n",
            "9000/9000 [==============================] - 1s 148us/step - loss: 0.0320 - acc: 0.9919 - val_loss: 0.8807 - val_acc: 0.8927\n",
            "Epoch 19/20\n",
            "9000/9000 [==============================] - 1s 149us/step - loss: 0.0323 - acc: 0.9911 - val_loss: 0.9594 - val_acc: 0.8919\n",
            "Epoch 20/20\n",
            "9000/9000 [==============================] - 1s 149us/step - loss: 0.0230 - acc: 0.9942 - val_loss: 0.9650 - val_acc: 0.8925\n",
            "Test loss: 0.9650279218631215\n",
            "Test accuracy: 0.8925\n",
            "Train loss: 0.005786327921706591\n",
            "Train accuracy: 0.9996666666666667\n",
            "x label: (9100, 28, 28)\n",
            "y label: (9100,)\n",
            "x unlabel: (50900, 28, 28)\n",
            "y unlabel: (50900,)\n",
            "90\n",
            "Train on 9100 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9100/9100 [==============================] - 1s 147us/step - loss: 0.0462 - acc: 0.9905 - val_loss: 0.9297 - val_acc: 0.8965\n",
            "Epoch 2/20\n",
            "9100/9100 [==============================] - 1s 146us/step - loss: 0.0453 - acc: 0.9892 - val_loss: 0.8345 - val_acc: 0.8956\n",
            "Epoch 3/20\n",
            "9100/9100 [==============================] - 1s 146us/step - loss: 0.0434 - acc: 0.9891 - val_loss: 0.8805 - val_acc: 0.8967\n",
            "Epoch 4/20\n",
            "9100/9100 [==============================] - 1s 150us/step - loss: 0.0470 - acc: 0.9887 - val_loss: 0.8883 - val_acc: 0.8941\n",
            "Epoch 5/20\n",
            "9100/9100 [==============================] - 1s 147us/step - loss: 0.0370 - acc: 0.9913 - val_loss: 0.9650 - val_acc: 0.8948\n",
            "Epoch 6/20\n",
            "9100/9100 [==============================] - 1s 147us/step - loss: 0.0272 - acc: 0.9919 - val_loss: 0.7872 - val_acc: 0.8955\n",
            "Epoch 7/20\n",
            "9100/9100 [==============================] - 1s 148us/step - loss: 0.0302 - acc: 0.9923 - val_loss: 0.8461 - val_acc: 0.8943\n",
            "Epoch 8/20\n",
            "9100/9100 [==============================] - 1s 145us/step - loss: 0.0370 - acc: 0.9914 - val_loss: 0.9380 - val_acc: 0.8932\n",
            "Epoch 9/20\n",
            "9100/9100 [==============================] - 1s 150us/step - loss: 0.0362 - acc: 0.9907 - val_loss: 0.8551 - val_acc: 0.8924\n",
            "Epoch 10/20\n",
            "9100/9100 [==============================] - 1s 150us/step - loss: 0.0351 - acc: 0.9907 - val_loss: 0.8602 - val_acc: 0.8941\n",
            "Epoch 11/20\n",
            "9100/9100 [==============================] - 1s 150us/step - loss: 0.0307 - acc: 0.9918 - val_loss: 0.8611 - val_acc: 0.8954\n",
            "Epoch 12/20\n",
            "9100/9100 [==============================] - 1s 148us/step - loss: 0.0334 - acc: 0.9912 - val_loss: 1.0045 - val_acc: 0.8909\n",
            "Epoch 13/20\n",
            "9100/9100 [==============================] - 1s 154us/step - loss: 0.0291 - acc: 0.9926 - val_loss: 0.9679 - val_acc: 0.8920\n",
            "Epoch 14/20\n",
            "9100/9100 [==============================] - 1s 151us/step - loss: 0.0362 - acc: 0.9902 - val_loss: 0.8384 - val_acc: 0.8966\n",
            "Epoch 15/20\n",
            "9100/9100 [==============================] - 1s 145us/step - loss: 0.0353 - acc: 0.9914 - val_loss: 0.8311 - val_acc: 0.8959\n",
            "Epoch 16/20\n",
            "9100/9100 [==============================] - 1s 150us/step - loss: 0.0298 - acc: 0.9918 - val_loss: 0.9528 - val_acc: 0.8908\n",
            "Epoch 17/20\n",
            "9100/9100 [==============================] - 1s 149us/step - loss: 0.0300 - acc: 0.9927 - val_loss: 0.9375 - val_acc: 0.8966\n",
            "Epoch 18/20\n",
            "9100/9100 [==============================] - 1s 146us/step - loss: 0.0274 - acc: 0.9927 - val_loss: 0.9516 - val_acc: 0.8945\n",
            "Epoch 19/20\n",
            "9100/9100 [==============================] - 1s 147us/step - loss: 0.0287 - acc: 0.9930 - val_loss: 0.9443 - val_acc: 0.8984\n",
            "Epoch 20/20\n",
            "9100/9100 [==============================] - 1s 149us/step - loss: 0.0302 - acc: 0.9919 - val_loss: 0.8788 - val_acc: 0.8933\n",
            "Test loss: 0.8788175372174301\n",
            "Test accuracy: 0.8933\n",
            "Train loss: 0.005870983942292276\n",
            "Train accuracy: 0.9996703296703296\n",
            "x label: (9200, 28, 28)\n",
            "y label: (9200,)\n",
            "x unlabel: (50800, 28, 28)\n",
            "y unlabel: (50800,)\n",
            "91\n",
            "Train on 9200 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9200/9200 [==============================] - 1s 147us/step - loss: 0.0540 - acc: 0.9895 - val_loss: 0.9044 - val_acc: 0.8914\n",
            "Epoch 2/20\n",
            "9200/9200 [==============================] - 1s 149us/step - loss: 0.0494 - acc: 0.9888 - val_loss: 0.8351 - val_acc: 0.8983\n",
            "Epoch 3/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0438 - acc: 0.9913 - val_loss: 0.8626 - val_acc: 0.8918\n",
            "Epoch 4/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0426 - acc: 0.9900 - val_loss: 0.8573 - val_acc: 0.8930\n",
            "Epoch 5/20\n",
            "9200/9200 [==============================] - 1s 145us/step - loss: 0.0382 - acc: 0.9900 - val_loss: 0.9276 - val_acc: 0.8950\n",
            "Epoch 6/20\n",
            "9200/9200 [==============================] - 1s 143us/step - loss: 0.0399 - acc: 0.9904 - val_loss: 0.9242 - val_acc: 0.8916\n",
            "Epoch 7/20\n",
            "9200/9200 [==============================] - 1s 145us/step - loss: 0.0372 - acc: 0.9905 - val_loss: 0.9292 - val_acc: 0.8919\n",
            "Epoch 8/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0429 - acc: 0.9915 - val_loss: 0.9286 - val_acc: 0.8961\n",
            "Epoch 9/20\n",
            "9200/9200 [==============================] - 1s 144us/step - loss: 0.0323 - acc: 0.9922 - val_loss: 0.8417 - val_acc: 0.8870\n",
            "Epoch 10/20\n",
            "9200/9200 [==============================] - 1s 147us/step - loss: 0.0356 - acc: 0.9917 - val_loss: 0.9882 - val_acc: 0.8941\n",
            "Epoch 11/20\n",
            "9200/9200 [==============================] - 1s 145us/step - loss: 0.0325 - acc: 0.9922 - val_loss: 0.9109 - val_acc: 0.8938\n",
            "Epoch 12/20\n",
            "9200/9200 [==============================] - 1s 147us/step - loss: 0.0334 - acc: 0.9915 - val_loss: 0.8968 - val_acc: 0.8911\n",
            "Epoch 13/20\n",
            "9200/9200 [==============================] - 1s 150us/step - loss: 0.0276 - acc: 0.9941 - val_loss: 0.9117 - val_acc: 0.8895\n",
            "Epoch 14/20\n",
            "9200/9200 [==============================] - 1s 148us/step - loss: 0.0358 - acc: 0.9899 - val_loss: 0.8704 - val_acc: 0.8924\n",
            "Epoch 15/20\n",
            "9200/9200 [==============================] - 1s 145us/step - loss: 0.0377 - acc: 0.9917 - val_loss: 0.7749 - val_acc: 0.8901\n",
            "Epoch 16/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0370 - acc: 0.9898 - val_loss: 0.8681 - val_acc: 0.8906\n",
            "Epoch 17/20\n",
            "9200/9200 [==============================] - 1s 147us/step - loss: 0.0356 - acc: 0.9923 - val_loss: 0.9408 - val_acc: 0.8870\n",
            "Epoch 18/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0319 - acc: 0.9929 - val_loss: 0.8934 - val_acc: 0.8981\n",
            "Epoch 19/20\n",
            "9200/9200 [==============================] - 1s 146us/step - loss: 0.0335 - acc: 0.9922 - val_loss: 0.9010 - val_acc: 0.8936\n",
            "Epoch 20/20\n",
            "9200/9200 [==============================] - 1s 148us/step - loss: 0.0308 - acc: 0.9923 - val_loss: 0.8072 - val_acc: 0.8965\n",
            "Test loss: 0.8072343734785449\n",
            "Test accuracy: 0.8965\n",
            "Train loss: 0.008021574279018008\n",
            "Train accuracy: 0.9994565217391305\n",
            "x label: (9300, 28, 28)\n",
            "y label: (9300,)\n",
            "x unlabel: (50700, 28, 28)\n",
            "y unlabel: (50700,)\n",
            "92\n",
            "Train on 9300 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9300/9300 [==============================] - 1s 149us/step - loss: 0.0564 - acc: 0.9869 - val_loss: 0.8785 - val_acc: 0.8951\n",
            "Epoch 2/20\n",
            "9300/9300 [==============================] - 1s 147us/step - loss: 0.0571 - acc: 0.9863 - val_loss: 0.8784 - val_acc: 0.8949\n",
            "Epoch 3/20\n",
            "9300/9300 [==============================] - 1s 148us/step - loss: 0.0358 - acc: 0.9904 - val_loss: 0.9207 - val_acc: 0.8943\n",
            "Epoch 4/20\n",
            "9300/9300 [==============================] - 1s 144us/step - loss: 0.0427 - acc: 0.9894 - val_loss: 0.8887 - val_acc: 0.8939\n",
            "Epoch 5/20\n",
            "9300/9300 [==============================] - 1s 146us/step - loss: 0.0441 - acc: 0.9904 - val_loss: 0.9375 - val_acc: 0.8895\n",
            "Epoch 6/20\n",
            "9300/9300 [==============================] - 1s 146us/step - loss: 0.0428 - acc: 0.9901 - val_loss: 0.9190 - val_acc: 0.8924\n",
            "Epoch 7/20\n",
            "9300/9300 [==============================] - 1s 145us/step - loss: 0.0401 - acc: 0.9903 - val_loss: 0.8349 - val_acc: 0.8973\n",
            "Epoch 8/20\n",
            "9300/9300 [==============================] - 1s 148us/step - loss: 0.0370 - acc: 0.9900 - val_loss: 0.8980 - val_acc: 0.8964\n",
            "Epoch 9/20\n",
            "9300/9300 [==============================] - 1s 146us/step - loss: 0.0276 - acc: 0.9925 - val_loss: 0.8044 - val_acc: 0.8970\n",
            "Epoch 10/20\n",
            "9300/9300 [==============================] - 1s 147us/step - loss: 0.0281 - acc: 0.9937 - val_loss: 0.8043 - val_acc: 0.8935\n",
            "Epoch 11/20\n",
            "9300/9300 [==============================] - 1s 148us/step - loss: 0.0400 - acc: 0.9900 - val_loss: 0.9124 - val_acc: 0.8986\n",
            "Epoch 12/20\n",
            "9300/9300 [==============================] - 1s 150us/step - loss: 0.0380 - acc: 0.9915 - val_loss: 0.8849 - val_acc: 0.8990\n",
            "Epoch 13/20\n",
            "9300/9300 [==============================] - 1s 147us/step - loss: 0.0323 - acc: 0.9920 - val_loss: 0.8427 - val_acc: 0.8936\n",
            "Epoch 14/20\n",
            "9300/9300 [==============================] - 1s 148us/step - loss: 0.0336 - acc: 0.9911 - val_loss: 0.8175 - val_acc: 0.8938\n",
            "Epoch 15/20\n",
            "9300/9300 [==============================] - 1s 147us/step - loss: 0.0320 - acc: 0.9918 - val_loss: 0.9782 - val_acc: 0.8919\n",
            "Epoch 16/20\n",
            "9300/9300 [==============================] - 1s 146us/step - loss: 0.0379 - acc: 0.9905 - val_loss: 0.8818 - val_acc: 0.8920\n",
            "Epoch 17/20\n",
            "9300/9300 [==============================] - 1s 146us/step - loss: 0.0367 - acc: 0.9901 - val_loss: 0.7920 - val_acc: 0.8928\n",
            "Epoch 18/20\n",
            "9300/9300 [==============================] - 1s 147us/step - loss: 0.0275 - acc: 0.9920 - val_loss: 1.0054 - val_acc: 0.8939\n",
            "Epoch 19/20\n",
            "9300/9300 [==============================] - 1s 143us/step - loss: 0.0271 - acc: 0.9923 - val_loss: 0.9109 - val_acc: 0.8978\n",
            "Epoch 20/20\n",
            "9300/9300 [==============================] - 1s 145us/step - loss: 0.0296 - acc: 0.9927 - val_loss: 0.8955 - val_acc: 0.8928\n",
            "Test loss: 0.8954988448403077\n",
            "Test accuracy: 0.8928\n",
            "Train loss: 0.007464329104843654\n",
            "Train accuracy: 0.999354838684041\n",
            "x label: (9400, 28, 28)\n",
            "y label: (9400,)\n",
            "x unlabel: (50600, 28, 28)\n",
            "y unlabel: (50600,)\n",
            "93\n",
            "Train on 9400 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0528 - acc: 0.9872 - val_loss: 0.7829 - val_acc: 0.8993\n",
            "Epoch 2/20\n",
            "9400/9400 [==============================] - 1s 150us/step - loss: 0.0485 - acc: 0.9886 - val_loss: 0.9124 - val_acc: 0.8939\n",
            "Epoch 3/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0436 - acc: 0.9893 - val_loss: 0.8846 - val_acc: 0.8922\n",
            "Epoch 4/20\n",
            "9400/9400 [==============================] - 1s 148us/step - loss: 0.0456 - acc: 0.9894 - val_loss: 0.9511 - val_acc: 0.8937\n",
            "Epoch 5/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0452 - acc: 0.9880 - val_loss: 0.9492 - val_acc: 0.8904\n",
            "Epoch 6/20\n",
            "9400/9400 [==============================] - 1s 145us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 0.8427 - val_acc: 0.8918\n",
            "Epoch 7/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0342 - acc: 0.9917 - val_loss: 0.8511 - val_acc: 0.8932\n",
            "Epoch 8/20\n",
            "9400/9400 [==============================] - 1s 147us/step - loss: 0.0364 - acc: 0.9915 - val_loss: 0.7578 - val_acc: 0.8945\n",
            "Epoch 9/20\n",
            "9400/9400 [==============================] - 1s 145us/step - loss: 0.0374 - acc: 0.9911 - val_loss: 0.8916 - val_acc: 0.8928\n",
            "Epoch 10/20\n",
            "9400/9400 [==============================] - 1s 145us/step - loss: 0.0364 - acc: 0.9902 - val_loss: 0.8847 - val_acc: 0.8929\n",
            "Epoch 11/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0367 - acc: 0.9909 - val_loss: 0.9965 - val_acc: 0.8926\n",
            "Epoch 12/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0346 - acc: 0.9906 - val_loss: 0.8591 - val_acc: 0.8923\n",
            "Epoch 13/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0333 - acc: 0.9929 - val_loss: 0.8953 - val_acc: 0.8933\n",
            "Epoch 14/20\n",
            "9400/9400 [==============================] - 1s 148us/step - loss: 0.0316 - acc: 0.9919 - val_loss: 0.9055 - val_acc: 0.8937\n",
            "Epoch 15/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0309 - acc: 0.9924 - val_loss: 0.9374 - val_acc: 0.8909\n",
            "Epoch 16/20\n",
            "9400/9400 [==============================] - 1s 145us/step - loss: 0.0384 - acc: 0.9899 - val_loss: 0.9081 - val_acc: 0.8930\n",
            "Epoch 17/20\n",
            "9400/9400 [==============================] - 1s 145us/step - loss: 0.0297 - acc: 0.9933 - val_loss: 0.8533 - val_acc: 0.8970\n",
            "Epoch 18/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.8871 - val_acc: 0.8891\n",
            "Epoch 19/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0330 - acc: 0.9919 - val_loss: 0.8891 - val_acc: 0.8995\n",
            "Epoch 20/20\n",
            "9400/9400 [==============================] - 1s 146us/step - loss: 0.0357 - acc: 0.9927 - val_loss: 0.9106 - val_acc: 0.8918\n",
            "Test loss: 0.9105815857713198\n",
            "Test accuracy: 0.8918\n",
            "Train loss: 0.006030175022112533\n",
            "Train accuracy: 0.9995744680851064\n",
            "x label: (9500, 28, 28)\n",
            "y label: (9500,)\n",
            "x unlabel: (50500, 28, 28)\n",
            "y unlabel: (50500,)\n",
            "94\n",
            "Train on 9500 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9500/9500 [==============================] - 1s 150us/step - loss: 0.0502 - acc: 0.9872 - val_loss: 0.8326 - val_acc: 0.8945\n",
            "Epoch 2/20\n",
            "9500/9500 [==============================] - 1s 144us/step - loss: 0.0511 - acc: 0.9865 - val_loss: 0.7792 - val_acc: 0.8958\n",
            "Epoch 3/20\n",
            "9500/9500 [==============================] - 1s 146us/step - loss: 0.0456 - acc: 0.9884 - val_loss: 0.8265 - val_acc: 0.8943\n",
            "Epoch 4/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0424 - acc: 0.9883 - val_loss: 0.9155 - val_acc: 0.8969\n",
            "Epoch 5/20\n",
            "9500/9500 [==============================] - 1s 144us/step - loss: 0.0287 - acc: 0.9923 - val_loss: 0.8084 - val_acc: 0.8959\n",
            "Epoch 6/20\n",
            "9500/9500 [==============================] - 1s 149us/step - loss: 0.0342 - acc: 0.9909 - val_loss: 0.8508 - val_acc: 0.8946\n",
            "Epoch 7/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0350 - acc: 0.9900 - val_loss: 0.8940 - val_acc: 0.8936\n",
            "Epoch 8/20\n",
            "9500/9500 [==============================] - 1s 146us/step - loss: 0.0334 - acc: 0.9923 - val_loss: 0.8103 - val_acc: 0.8955\n",
            "Epoch 9/20\n",
            "9500/9500 [==============================] - 1s 147us/step - loss: 0.0338 - acc: 0.9900 - val_loss: 0.8841 - val_acc: 0.8960\n",
            "Epoch 10/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0319 - acc: 0.9907 - val_loss: 0.8550 - val_acc: 0.8922\n",
            "Epoch 11/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0383 - acc: 0.9908 - val_loss: 0.8305 - val_acc: 0.8919\n",
            "Epoch 12/20\n",
            "9500/9500 [==============================] - 1s 146us/step - loss: 0.0280 - acc: 0.9925 - val_loss: 0.8665 - val_acc: 0.8989\n",
            "Epoch 13/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0342 - acc: 0.9902 - val_loss: 0.8314 - val_acc: 0.8961\n",
            "Epoch 14/20\n",
            "9500/9500 [==============================] - 1s 145us/step - loss: 0.0338 - acc: 0.9914 - val_loss: 0.9600 - val_acc: 0.8935\n",
            "Epoch 15/20\n",
            "9500/9500 [==============================] - 1s 147us/step - loss: 0.0354 - acc: 0.9908 - val_loss: 0.8252 - val_acc: 0.8952\n",
            "Epoch 16/20\n",
            "9500/9500 [==============================] - 1s 144us/step - loss: 0.0268 - acc: 0.9940 - val_loss: 0.9026 - val_acc: 0.8923\n",
            "Epoch 17/20\n",
            "9500/9500 [==============================] - 1s 143us/step - loss: 0.0278 - acc: 0.9921 - val_loss: 0.8388 - val_acc: 0.8920\n",
            "Epoch 18/20\n",
            "9500/9500 [==============================] - 1s 144us/step - loss: 0.0320 - acc: 0.9922 - val_loss: 0.8809 - val_acc: 0.8949\n",
            "Epoch 19/20\n",
            "9500/9500 [==============================] - 1s 143us/step - loss: 0.0283 - acc: 0.9923 - val_loss: 0.9117 - val_acc: 0.8921\n",
            "Epoch 20/20\n",
            "9500/9500 [==============================] - 1s 144us/step - loss: 0.0352 - acc: 0.9892 - val_loss: 0.8505 - val_acc: 0.8924\n",
            "Test loss: 0.8505054242739075\n",
            "Test accuracy: 0.8924\n",
            "Train loss: 0.004856054643059332\n",
            "Train accuracy: 0.9996842105263158\n",
            "x label: (9600, 28, 28)\n",
            "y label: (9600,)\n",
            "x unlabel: (50400, 28, 28)\n",
            "y unlabel: (50400,)\n",
            "95\n",
            "Train on 9600 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9600/9600 [==============================] - 1s 143us/step - loss: 0.0446 - acc: 0.9889 - val_loss: 0.8859 - val_acc: 0.8926\n",
            "Epoch 2/20\n",
            "9600/9600 [==============================] - 1s 142us/step - loss: 0.0428 - acc: 0.9890 - val_loss: 0.8381 - val_acc: 0.8943\n",
            "Epoch 3/20\n",
            "9600/9600 [==============================] - 1s 144us/step - loss: 0.0403 - acc: 0.9900 - val_loss: 0.8539 - val_acc: 0.8946\n",
            "Epoch 4/20\n",
            "9600/9600 [==============================] - 1s 145us/step - loss: 0.0357 - acc: 0.9906 - val_loss: 0.8594 - val_acc: 0.8907\n",
            "Epoch 5/20\n",
            "9600/9600 [==============================] - 1s 144us/step - loss: 0.0371 - acc: 0.9899 - val_loss: 0.8394 - val_acc: 0.8988\n",
            "Epoch 6/20\n",
            "9600/9600 [==============================] - 1s 144us/step - loss: 0.0397 - acc: 0.9901 - val_loss: 0.8063 - val_acc: 0.8986\n",
            "Epoch 7/20\n",
            "9600/9600 [==============================] - 1s 147us/step - loss: 0.0345 - acc: 0.9906 - val_loss: 0.7808 - val_acc: 0.8949\n",
            "Epoch 8/20\n",
            "9600/9600 [==============================] - 1s 145us/step - loss: 0.0340 - acc: 0.9896 - val_loss: 0.8756 - val_acc: 0.8967\n",
            "Epoch 9/20\n",
            "9600/9600 [==============================] - 1s 145us/step - loss: 0.0378 - acc: 0.9904 - val_loss: 0.8316 - val_acc: 0.8976\n",
            "Epoch 10/20\n",
            "9600/9600 [==============================] - 1s 148us/step - loss: 0.0377 - acc: 0.9904 - val_loss: 0.8958 - val_acc: 0.8943\n",
            "Epoch 11/20\n",
            "9600/9600 [==============================] - 1s 145us/step - loss: 0.0291 - acc: 0.9918 - val_loss: 0.8078 - val_acc: 0.8925\n",
            "Epoch 12/20\n",
            "9600/9600 [==============================] - 1s 143us/step - loss: 0.0307 - acc: 0.9918 - val_loss: 0.7861 - val_acc: 0.8961\n",
            "Epoch 13/20\n",
            "9600/9600 [==============================] - 1s 148us/step - loss: 0.0323 - acc: 0.9911 - val_loss: 0.9335 - val_acc: 0.8938\n",
            "Epoch 14/20\n",
            "9600/9600 [==============================] - 1s 147us/step - loss: 0.0407 - acc: 0.9885 - val_loss: 0.7907 - val_acc: 0.8993\n",
            "Epoch 15/20\n",
            "9600/9600 [==============================] - 1s 147us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.8393 - val_acc: 0.8913\n",
            "Epoch 16/20\n",
            "9600/9600 [==============================] - 1s 145us/step - loss: 0.0261 - acc: 0.9927 - val_loss: 0.9083 - val_acc: 0.9001\n",
            "Epoch 17/20\n",
            "9600/9600 [==============================] - 1s 147us/step - loss: 0.0338 - acc: 0.9914 - val_loss: 0.8360 - val_acc: 0.8923\n",
            "Epoch 18/20\n",
            "9600/9600 [==============================] - 1s 148us/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.8567 - val_acc: 0.8938\n",
            "Epoch 19/20\n",
            "9600/9600 [==============================] - 1s 144us/step - loss: 0.0325 - acc: 0.9916 - val_loss: 0.8960 - val_acc: 0.8961\n",
            "Epoch 20/20\n",
            "9600/9600 [==============================] - 1s 144us/step - loss: 0.0244 - acc: 0.9927 - val_loss: 0.9176 - val_acc: 0.8908\n",
            "Test loss: 0.9175559169896776\n",
            "Test accuracy: 0.8908\n",
            "Train loss: 0.0033689653687874947\n",
            "Train accuracy: 0.9996875\n",
            "x label: (9700, 28, 28)\n",
            "y label: (9700,)\n",
            "x unlabel: (50300, 28, 28)\n",
            "y unlabel: (50300,)\n",
            "96\n",
            "Train on 9700 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9700/9700 [==============================] - 1s 144us/step - loss: 0.0486 - acc: 0.9884 - val_loss: 0.8018 - val_acc: 0.8978\n",
            "Epoch 2/20\n",
            "9700/9700 [==============================] - 1s 144us/step - loss: 0.0412 - acc: 0.9887 - val_loss: 0.8716 - val_acc: 0.8968\n",
            "Epoch 3/20\n",
            "9700/9700 [==============================] - 1s 141us/step - loss: 0.0354 - acc: 0.9899 - val_loss: 0.8326 - val_acc: 0.8956\n",
            "Epoch 4/20\n",
            "9700/9700 [==============================] - 1s 146us/step - loss: 0.0384 - acc: 0.9907 - val_loss: 0.7678 - val_acc: 0.8971\n",
            "Epoch 5/20\n",
            "9700/9700 [==============================] - 1s 146us/step - loss: 0.0339 - acc: 0.9903 - val_loss: 0.8764 - val_acc: 0.8985\n",
            "Epoch 6/20\n",
            "9700/9700 [==============================] - 1s 144us/step - loss: 0.0374 - acc: 0.9905 - val_loss: 0.8911 - val_acc: 0.8964\n",
            "Epoch 7/20\n",
            "9700/9700 [==============================] - 1s 147us/step - loss: 0.0322 - acc: 0.9902 - val_loss: 0.7964 - val_acc: 0.8962\n",
            "Epoch 8/20\n",
            "9700/9700 [==============================] - 1s 146us/step - loss: 0.0365 - acc: 0.9895 - val_loss: 0.8425 - val_acc: 0.8928\n",
            "Epoch 9/20\n",
            "9700/9700 [==============================] - 1s 143us/step - loss: 0.0373 - acc: 0.9903 - val_loss: 0.7500 - val_acc: 0.8946\n",
            "Epoch 10/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0352 - acc: 0.9891 - val_loss: 0.8425 - val_acc: 0.8941\n",
            "Epoch 11/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0391 - acc: 0.9888 - val_loss: 0.8952 - val_acc: 0.8935\n",
            "Epoch 12/20\n",
            "9700/9700 [==============================] - 1s 142us/step - loss: 0.0344 - acc: 0.9907 - val_loss: 0.8234 - val_acc: 0.8948\n",
            "Epoch 13/20\n",
            "9700/9700 [==============================] - 1s 146us/step - loss: 0.0325 - acc: 0.9912 - val_loss: 0.8147 - val_acc: 0.8944\n",
            "Epoch 14/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0270 - acc: 0.9919 - val_loss: 0.8759 - val_acc: 0.8980\n",
            "Epoch 15/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.9234 - val_acc: 0.8900\n",
            "Epoch 16/20\n",
            "9700/9700 [==============================] - 1s 143us/step - loss: 0.0262 - acc: 0.9921 - val_loss: 0.9132 - val_acc: 0.8954\n",
            "Epoch 17/20\n",
            "9700/9700 [==============================] - 1s 143us/step - loss: 0.0242 - acc: 0.9943 - val_loss: 0.9207 - val_acc: 0.8950\n",
            "Epoch 18/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0271 - acc: 0.9919 - val_loss: 0.9433 - val_acc: 0.8964\n",
            "Epoch 19/20\n",
            "9700/9700 [==============================] - 1s 143us/step - loss: 0.0302 - acc: 0.9909 - val_loss: 0.8214 - val_acc: 0.8994\n",
            "Epoch 20/20\n",
            "9700/9700 [==============================] - 1s 145us/step - loss: 0.0201 - acc: 0.9935 - val_loss: 0.8629 - val_acc: 0.8987\n",
            "Test loss: 0.862925343349648\n",
            "Test accuracy: 0.8987\n",
            "Train loss: 0.0017716020490514584\n",
            "Train accuracy: 0.9998969072164948\n",
            "x label: (9800, 28, 28)\n",
            "y label: (9800,)\n",
            "x unlabel: (50200, 28, 28)\n",
            "y unlabel: (50200,)\n",
            "97\n",
            "Train on 9800 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9800/9800 [==============================] - 1s 146us/step - loss: 0.0534 - acc: 0.9872 - val_loss: 0.8705 - val_acc: 0.8924\n",
            "Epoch 2/20\n",
            "9800/9800 [==============================] - 1s 147us/step - loss: 0.0402 - acc: 0.9885 - val_loss: 0.8638 - val_acc: 0.8951\n",
            "Epoch 3/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0337 - acc: 0.9915 - val_loss: 0.8412 - val_acc: 0.8926\n",
            "Epoch 4/20\n",
            "9800/9800 [==============================] - 1s 145us/step - loss: 0.0436 - acc: 0.9892 - val_loss: 0.9198 - val_acc: 0.8943\n",
            "Epoch 5/20\n",
            "9800/9800 [==============================] - 1s 143us/step - loss: 0.0422 - acc: 0.9894 - val_loss: 0.7503 - val_acc: 0.9007\n",
            "Epoch 6/20\n",
            "9800/9800 [==============================] - 1s 147us/step - loss: 0.0476 - acc: 0.9871 - val_loss: 0.8643 - val_acc: 0.8938\n",
            "Epoch 7/20\n",
            "9800/9800 [==============================] - 1s 145us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.9334 - val_acc: 0.8925\n",
            "Epoch 8/20\n",
            "9800/9800 [==============================] - 1s 143us/step - loss: 0.0354 - acc: 0.9904 - val_loss: 0.8337 - val_acc: 0.8921\n",
            "Epoch 9/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0326 - acc: 0.9903 - val_loss: 0.8717 - val_acc: 0.8967\n",
            "Epoch 10/20\n",
            "9800/9800 [==============================] - 1s 141us/step - loss: 0.0311 - acc: 0.9914 - val_loss: 0.8696 - val_acc: 0.8959\n",
            "Epoch 11/20\n",
            "9800/9800 [==============================] - 1s 143us/step - loss: 0.0297 - acc: 0.9927 - val_loss: 0.8273 - val_acc: 0.8998\n",
            "Epoch 12/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0279 - acc: 0.9914 - val_loss: 0.8510 - val_acc: 0.9021\n",
            "Epoch 13/20\n",
            "9800/9800 [==============================] - 1s 145us/step - loss: 0.0304 - acc: 0.9915 - val_loss: 0.9303 - val_acc: 0.8929\n",
            "Epoch 14/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0281 - acc: 0.9928 - val_loss: 0.7376 - val_acc: 0.8955\n",
            "Epoch 15/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0288 - acc: 0.9923 - val_loss: 0.9002 - val_acc: 0.8937\n",
            "Epoch 16/20\n",
            "9800/9800 [==============================] - 1s 143us/step - loss: 0.0297 - acc: 0.9911 - val_loss: 0.8035 - val_acc: 0.8969\n",
            "Epoch 17/20\n",
            "9800/9800 [==============================] - 1s 147us/step - loss: 0.0260 - acc: 0.9932 - val_loss: 0.9129 - val_acc: 0.8996\n",
            "Epoch 18/20\n",
            "9800/9800 [==============================] - 1s 143us/step - loss: 0.0347 - acc: 0.9914 - val_loss: 0.8587 - val_acc: 0.8953\n",
            "Epoch 19/20\n",
            "9800/9800 [==============================] - 1s 144us/step - loss: 0.0282 - acc: 0.9932 - val_loss: 0.9082 - val_acc: 0.8932\n",
            "Epoch 20/20\n",
            "9800/9800 [==============================] - 1s 146us/step - loss: 0.0269 - acc: 0.9942 - val_loss: 0.8347 - val_acc: 0.8962\n",
            "Test loss: 0.8347445048911264\n",
            "Test accuracy: 0.8962\n",
            "Train loss: 0.0035392802866865206\n",
            "Train accuracy: 0.9997959183673469\n",
            "x label: (9900, 28, 28)\n",
            "y label: (9900,)\n",
            "x unlabel: (50100, 28, 28)\n",
            "y unlabel: (50100,)\n",
            "98\n",
            "Train on 9900 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "9900/9900 [==============================] - 1s 144us/step - loss: 0.0472 - acc: 0.9887 - val_loss: 0.8536 - val_acc: 0.8967\n",
            "Epoch 2/20\n",
            "9900/9900 [==============================] - 1s 147us/step - loss: 0.0469 - acc: 0.9874 - val_loss: 0.8405 - val_acc: 0.8934\n",
            "Epoch 3/20\n",
            "9900/9900 [==============================] - 1s 143us/step - loss: 0.0414 - acc: 0.9887 - val_loss: 0.8962 - val_acc: 0.8958\n",
            "Epoch 4/20\n",
            "9900/9900 [==============================] - 1s 148us/step - loss: 0.0431 - acc: 0.9888 - val_loss: 0.8556 - val_acc: 0.8971\n",
            "Epoch 5/20\n",
            "9900/9900 [==============================] - 1s 144us/step - loss: 0.0370 - acc: 0.9911 - val_loss: 0.8579 - val_acc: 0.8934\n",
            "Epoch 6/20\n",
            "9900/9900 [==============================] - 1s 142us/step - loss: 0.0343 - acc: 0.9914 - val_loss: 0.8424 - val_acc: 0.8994\n",
            "Epoch 7/20\n",
            "9900/9900 [==============================] - 1s 144us/step - loss: 0.0369 - acc: 0.9896 - val_loss: 0.8379 - val_acc: 0.8964\n",
            "Epoch 8/20\n",
            "9900/9900 [==============================] - 1s 143us/step - loss: 0.0266 - acc: 0.9919 - val_loss: 0.9126 - val_acc: 0.8961\n",
            "Epoch 9/20\n",
            "9900/9900 [==============================] - 1s 143us/step - loss: 0.0316 - acc: 0.9912 - val_loss: 0.8116 - val_acc: 0.8980\n",
            "Epoch 10/20\n",
            "9900/9900 [==============================] - 1s 145us/step - loss: 0.0319 - acc: 0.9903 - val_loss: 0.8324 - val_acc: 0.8914\n",
            "Epoch 11/20\n",
            "9900/9900 [==============================] - 1s 143us/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.8418 - val_acc: 0.8993\n",
            "Epoch 12/20\n",
            "9900/9900 [==============================] - 1s 141us/step - loss: 0.0302 - acc: 0.9926 - val_loss: 0.9202 - val_acc: 0.8956\n",
            "Epoch 13/20\n",
            "9900/9900 [==============================] - 1s 146us/step - loss: 0.0437 - acc: 0.9879 - val_loss: 0.8047 - val_acc: 0.8999\n",
            "Epoch 14/20\n",
            "9900/9900 [==============================] - 1s 141us/step - loss: 0.0328 - acc: 0.9909 - val_loss: 0.9013 - val_acc: 0.8939\n",
            "Epoch 15/20\n",
            "9900/9900 [==============================] - 1s 150us/step - loss: 0.0277 - acc: 0.9927 - val_loss: 0.8712 - val_acc: 0.8960\n",
            "Epoch 16/20\n",
            "9900/9900 [==============================] - 1s 149us/step - loss: 0.0335 - acc: 0.9909 - val_loss: 0.8575 - val_acc: 0.8957\n",
            "Epoch 17/20\n",
            "9900/9900 [==============================] - 1s 148us/step - loss: 0.0342 - acc: 0.9913 - val_loss: 0.8351 - val_acc: 0.8974\n",
            "Epoch 18/20\n",
            "9900/9900 [==============================] - 1s 147us/step - loss: 0.0267 - acc: 0.9920 - val_loss: 0.7470 - val_acc: 0.8958\n",
            "Epoch 19/20\n",
            "9900/9900 [==============================] - 1s 141us/step - loss: 0.0283 - acc: 0.9923 - val_loss: 0.9742 - val_acc: 0.8924\n",
            "Epoch 20/20\n",
            "9900/9900 [==============================] - 1s 142us/step - loss: 0.0333 - acc: 0.9921 - val_loss: 0.7452 - val_acc: 0.8944\n",
            "Test loss: 0.7452489804301892\n",
            "Test accuracy: 0.8944\n",
            "Train loss: 0.006291287668091632\n",
            "Train accuracy: 0.9996969696969698\n",
            "x label: (10000, 28, 28)\n",
            "y label: (10000,)\n",
            "x unlabel: (50000, 28, 28)\n",
            "y unlabel: (50000,)\n",
            "99\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.0451 - acc: 0.9905 - val_loss: 0.8342 - val_acc: 0.8959\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.0412 - acc: 0.9894 - val_loss: 0.8927 - val_acc: 0.8952\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.0451 - acc: 0.9894 - val_loss: 0.7521 - val_acc: 0.8919\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 1s 146us/step - loss: 0.0364 - acc: 0.9908 - val_loss: 0.8176 - val_acc: 0.8944\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.0430 - acc: 0.9888 - val_loss: 0.8747 - val_acc: 0.8953\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 1s 150us/step - loss: 0.0377 - acc: 0.9905 - val_loss: 0.8663 - val_acc: 0.8975\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 1s 148us/step - loss: 0.0324 - acc: 0.9914 - val_loss: 0.8656 - val_acc: 0.8962\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 2s 151us/step - loss: 0.0371 - acc: 0.9905 - val_loss: 0.8853 - val_acc: 0.8956\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 1s 148us/step - loss: 0.0376 - acc: 0.9908 - val_loss: 0.8108 - val_acc: 0.8941\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 1s 148us/step - loss: 0.0320 - acc: 0.9897 - val_loss: 0.8477 - val_acc: 0.8976\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 1s 148us/step - loss: 0.0371 - acc: 0.9896 - val_loss: 0.8539 - val_acc: 0.8949\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 2s 151us/step - loss: 0.0362 - acc: 0.9900 - val_loss: 0.8524 - val_acc: 0.8969\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 1s 145us/step - loss: 0.0359 - acc: 0.9903 - val_loss: 0.8523 - val_acc: 0.8941\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.0247 - acc: 0.9924 - val_loss: 0.8355 - val_acc: 0.8986\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.0315 - acc: 0.9921 - val_loss: 0.7740 - val_acc: 0.8984\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.0261 - acc: 0.9920 - val_loss: 0.9224 - val_acc: 0.8951\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 1s 144us/step - loss: 0.0321 - acc: 0.9923 - val_loss: 0.8798 - val_acc: 0.8899\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 1s 140us/step - loss: 0.0361 - acc: 0.9902 - val_loss: 0.9741 - val_acc: 0.8938\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 1s 143us/step - loss: 0.0316 - acc: 0.9914 - val_loss: 0.8006 - val_acc: 0.8969\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 1s 141us/step - loss: 0.0267 - acc: 0.9928 - val_loss: 0.8141 - val_acc: 0.8990\n",
            "Test loss: 0.8140993600918711\n",
            "Test accuracy: 0.899\n",
            "Train loss: 0.0035279625045105604\n",
            "Train accuracy: 0.9998\n",
            "x label: (10100, 28, 28)\n",
            "y label: (10100,)\n",
            "x unlabel: (49900, 28, 28)\n",
            "y unlabel: (49900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-D-Ehn-uiwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pseudo_label)\n",
        "print(actual_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YbZwptmEtm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(num_labels[:100], test_acc[:100], label=\"Testset\")\n",
        "plt.plot(num_labels[:100], train_acc[:100], label=\"Trainset\")\n",
        "plt.xlabel('Number of labelled images')\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.axis(ymin=0, xmin=0, ymax=1.01, xmax=20000)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNs65v0L5fc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MyFile=open('AL_50_acc.txt','w')\n",
        "\n",
        "for element in test_acc:\n",
        "     MyFile.write(str(element))\n",
        "     MyFile.write('\\n')\n",
        "MyFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vcva_-__Yrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png',show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJqMaSmj_tnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history = model.fit(x_label_cnn, y_label_cnn,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU943KZhAHMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot training & validation accuracy values\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('Model accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot training & validation loss values\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Model loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Test'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}